\documentclass{report}

\usepackage[latin1]{inputenc} % un package
\usepackage[T1]{fontenc}      % un second package
\usepackage[francais]{babel}  % un troisième package
\usepackage{graphicx}
\usepackage{float}
\usepackage{ccicons}
\usepackage{url}
\usepackage{amsmath,amsfonts,amssymb}
\DeclareMathOperator{\asinh}{asinh}
\usepackage{pgfplots}
\usepackage{wasysym}
\usepackage{braket}
\usepackage{sagetex}
\usepackage{pdfpages}
\usepackage{hyperref}
\usetikzlibrary{math}

\usepackage[europeanvoltages, europeancurrents, europeanresistors, americaninductors]{circuitikz}

\usepackage[left,modulo]{lineno}
%\linenumbers

\def\releaseversion{1} %Mettre à 1 pour afficher les graphiques

\begin{document}
\title{Notes de cours}
\author{Baptiste LEGOUIX}
%\date{\today\\[30pt] \huge \ccbynd}
\date{\today}

\maketitle

\tableofcontents

\part{Mathématique}

\chapter{Fondements des mathématiques}
\begin{it}- La mathématique est l'ensemble des vérités pouvant êtres établies par le raisonnement seul (sans rapport à l'expérience, au réel).

- La logique est la branche de la mathématique qui étudie les relations entre propositions.

- Une proposition est une affirmation pouvant être vraie ou fausse mais forcément vraie si elle est énoncée seule.\end{it}

\section{Logique}

\subsection{Connecteurs logiques}

Un \emph{prédicat} est une proposition incomplète, qui dépend de \emph{variables} qui l'entourent.

On appelle \emph{connecteur logique} un prédicat dont les variables sont des propositions.

On peut décrire le comportement des connecteurs logiques grâce à des \emph{tables de vérité}. On peut par ailleurs considérer celles-ci comme des \emph{définitions} des connecteurs logiques. 

\subsubsection{L'implication}
L'implication est le connecteur logique de base de la démonstration : $P\Rightarrow Q$ signifie que sous l'hypothèse $P$ on peut démontrer $Q$ :

\begin{center}\begin{tabular}{c|c||c}$P$ & $Q$ & $P\Rightarrow Q$ \\\hline Faux & Faux & Vrai \\\hline Faux & Vrai & Vrai \\\hline Vrai & Faux & Faux \\\hline Vrai & Vrai & Vrai\end{tabular}\end{center}

On dit alors que $P$ est une \emph{condition suffisante} à $Q$ ou bien que $Q$ est une \emph{condition nécessaire} à $P$ (dans le sens où si $Q$ est fausse alors $P$ ne peut pas être vraie)

Nous y reviendrons très bientôt, mais les tables de vérités sont en fait des énumérations d'implications (à partir des données de gauches on obtient les données de droites et pas l'inverse).

\subsubsection{La conjonction (ET)}
La conjonction notée $\wedge$ mais lue ``et'' satisfait la table de vérité suivante :

\begin{center}\begin{tabular}{c|c||c}$P$ & $Q$ & $P\wedge Q$ \\\hline Faux & Faux & Faux \\\hline Faux & Vrai & Faux \\\hline Vrai & Faux & Faux \\\hline Vrai & Vrai & Vrai\end{tabular}\end{center}

\subsubsection{L'équivalence}
L'équivalence est en quelque sorte l'\emph{égalité} de la logique. $P\Leftrightarrow Q$ signifie que $P$ et $Q$ sont en fait deux façons de dire la même chose, soit que $(P\Rightarrow Q)\wedge(Q\Rightarrow P)$ :

\begin{center}\begin{tabular}{c|c||c}$P$ & $Q$ & $P\Leftrightarrow Q$ \\\hline Faux & Faux & Vrai \\\hline Faux & Vrai & Faux \\\hline Vrai & Faux & Faux \\\hline Vrai & Vrai & Vrai\end{tabular}\end{center}

On lit $P\Leftrightarrow Q$, "$P$ si et seulement si $Q$". On parle de \emph{condition nécessaire et suffisante}. 

\subsubsection{La disjonction (OU)}
La disjonction notée $\vee$ mais lue ``ou'' satisfait la table de vérité suivante : 

\begin{center}\begin{tabular}{c|c||c}$P$ & $Q$ & $P\vee Q$ \\\hline Faux & Faux & Faux \\\hline Faux & Vrai & Vrai \\\hline Vrai & Faux & Vrai \\\hline Vrai & Vrai & Vrai\end{tabular}\end{center}

On remarque que la disjonction diffère du \emph{ou} du sens commun par la dernière ligne.

\subsubsection{La négation (NON)}
$\neg P$, lue ``non P'', indique que $P$ n'est pas vraie :

\begin{center}\begin{tabular}{c|c}$P$ & $\neg P$ \\\hline Faux & Vrai \\\hline Vrai & Faux\end{tabular}\end{center}

\subsection{Propriétés des connecteurs logiques}
Il s'agit maintenant d'étudier les comportements des connecteurs logiques avec eux-mêmes et entre eux.

\subsubsection{Commutativité}
La commutativité est une propriété que peuvent satisfaire les connecteurs logiques à deux variables (pas la négation donc). Elle s'énonce comme suit pour un connecteur logique $\bullet$ quelles que soient les propositions $P$ et $Q$ :

\[\boxed{P\bullet Q\Leftrightarrow Q\bullet P}\]

Pour des raisons de symétrie dans la table de vérité de $\bullet$, on a commutativité si et seulement si $P\bullet \neg P \Leftrightarrow \neg P\bullet P$, que satisfont tous les connecteurs logiques précédemment cités à l'exception de l'implication.

\subsubsection{Associativité}
L'associativité est également une propriété propre concernant les connecteurs logiques à deux variables. Elle s'énonce comme suit quelles que soient les propositions $P$, $Q$, et $R$ :

\[\boxed{(P\bullet Q)\bullet R\Leftrightarrow P\bullet (Q\bullet R)}\]

Le parenthèses intérieurs définissent les connections logiques prioritaires, c'est à dire celles à traiter en premier.

L'associativité est notamment satisfaite par tous les connecteurs logiques précédemment cités à l'exception de l'implication. On peut alors se passer des parenthèses.

Par convention, une conjonction est prioritaire sur une disjonction ce qui permet d'oublier les parenthèses dans certains cas pour ne pas alourdir les notations. On a également priorité de tous les connecteurs sur l'implication et l'équivalence.

\subsubsection{Distributivité}

On remarque que la table de vérité de $P\vee Q\wedge R$ est identique à la table de vérité de $(P\vee Q)\wedge (P\vee R)$ tout comme celle de $P\wedge (Q\vee R)$ est identique à celle de $P\wedge Q\vee P\wedge R$ :

\begin{center}\begin{tabular}{c|c|c||c|c|c|c}$P$ & $Q$ & $R$ & $P\vee Q\wedge R$ & $(P\vee Q)\wedge (P\vee R)$ & $P\wedge (Q\vee R)$ & $P\wedge Q\vee P\wedge R$ \\\hline Faux & Faux & Faux & Faux & Faux & Faux & Faux \\\hline Faux & Faux & Vrai & Faux & Faux & Faux & Faux \\\hline Faux & Vrai & Faux & Faux & Faux & Faux & Faux \\\hline Faux & Vrai & Vrai & Vrai & Vrai & Faux & Faux \\\hline Vrai & Faux & Faux & Vrai & Vrai & Faux & Faux \\\hline Vrai & Faux & Vrai & Vrai & Vrai & Vrai & Vrai \\\hline Vrai & Vrai & Faux & Vrai & Vrai & Vrai & Vrai \\\hline Vrai & Vrai & Vrai & Vrai & Vrai & Vrai & Vrai\end{tabular}
\end{center}

Ainsi on a :

\[\boxed{
P\vee Q\wedge R\Leftrightarrow (P\vee Q)\wedge (P\vee R))
}\]

Et ($\wedge$) :

\[\boxed{
P\wedge (Q\vee R)\Leftrightarrow P\wedge Q\vee P\wedge R
}\]
\subsection{Tables de vérité}

Une table de vérité est l'énumération de toutes les implications propres à un connecteur logique. Par exemple :

\begin{center}\begin{tabular}{c|c|c||c}$P$ & $Q$ & $R$ & $P\vee Q\wedge R$ \\\hline Vrai & Faux & Vrai & Vrai \end{tabular}
\end{center}

Indique que $P\wedge \neg Q \wedge R \Rightarrow P\vee Q\wedge R$. Reprenons maintenant la table de vérité de l'implication :

\begin{center}\begin{tabular}{c|c||c}$P$ & $Q$ & $P\Rightarrow Q$ \\\hline Faux & Faux & Vrai \\\hline Faux & Vrai & Vrai \\\hline Vrai & Faux & Faux \\\hline Vrai & Vrai & Vrai\end{tabular}\end{center}

Elle signifie que $(\neg P\wedge \neg Q\vee \neg P\wedge Q\vee P\wedge Q \Rightarrow (P\Rightarrow Q))\wedge (P\wedge \neg Q \Rightarrow \neg (P\Rightarrow Q))$.

C'est une définition \emph{récursive} de l'implication (on use de l'implication pour définir l'implication). On définit récursivement de la même façon la conjonction, la disjonction et la négation.

On voit ainsi qu'on peut exprimer n'importe quelle table de vérité avec les quatres connecteurs $\Rightarrow$, $\wedge$, $\vee$ et $\neg$. On parle alors de système logique complet (ici un système dit \emph{à la Hilbert}) ce qui laisse entendre qu'on peut y démontrer toute vérité.

Les systèmes constitués de l'implication et de \emph{la disjonction réciproque} ou \emph{conjonction réciproque} sont également complets (ces deux connecteurs sont dits \emph{universels}) :

\begin{center}\begin{tabular}{c|c||c|c}$P$ & $Q$ & $P\downarrow Q$ & $P\uparrow Q$ \\\hline Faux & Faux & Vrai & Vrai \\\hline Faux & Vrai & Faux & Vrai \\\hline Vrai & Faux & Faux & Vrai \\\hline Vrai & Vrai & Faux & Faux\end{tabular}
\end{center}

\subsection{Logique classique}

On postule que :
\[
\neg (P\wedge \neg P) \wedge (P\vee \neg P)
\]

Cette proposition n'est pas démontrable mais est la pierre angulaire de la logique dite \emph{classique}, la première découverte historiquement. On dit que c'est l'\emph{axiome} de la logique classique.

C'est une \emph{disjonction exclusive} (le ``ou'' du sens commun) :

\begin{center}\begin{tabular}{c|c||c}$P$ & $Q$ & $P\oplus Q$ \\\hline Faux & Faux & Faux \\\hline Faux & Vrai & Vrai \\\hline Vrai & Faux & Vrai \\\hline Vrai & Vrai & Faux\end{tabular} \end{center}

On peut donc l'écrire $\boxed{P\oplus \neg P}$ ; ce qui implique par ailleurs $\boxed{P\Leftrightarrow \neg \neg P}$.

\subsubsection{Tables de vérité en logique classique}

Les implications des tables de vérité deviennent alors des équivalences : pour rester sur l'exemple de l'implication, on a maintenant $\neg P\wedge \neg Q\vee \neg P\wedge Q\vee P\wedge Q \Leftrightarrow (P\Rightarrow Q)$.

Qu'on peut \emph{factoriser} (distribution inverse) : $\neg P\wedge (\neg Q\vee Q)\vee P\wedge Q \Leftrightarrow (P\Rightarrow Q)$.

Mais l'axiome de la logique classique nous dit que $\neg Q\vee Q$ donc $\neg P\vee P\wedge Q \Leftrightarrow (P\Rightarrow Q)$.

On distribue : $(\neg P\vee P)\wedge (\neg P\vee Q) \Leftrightarrow (P\Rightarrow Q)$.

Pour finir $\neg P\vee Q \Leftrightarrow (P\Rightarrow Q)$.

Ces simplifications peuvent être automatisées en faisant usage d'un \emph{tableau de Karnaugh} dont le fonctionnement ne sera pas étudié dans ce cours.

\subsubsection{Loi de deMorgan}

\begin{center}\begin{tabular}{c|c||c|c|c|c}$P$ & $Q$ & $\neg (P\wedge Q)$ & $\neg P\vee \neg Q$ & $\neg (P\vee Q)$ & $\neg P\wedge \neg Q$ \\\hline Faux & Faux & Vrai & Vrai & Vrai & Vrai \\\hline Faux & Vrai & Vrai & Vrai & Faux & Faux \\\hline Vrai & Faux & Vrai & Vrai & Faux & Faux \\\hline Vrai & Vrai & Faux & Faux & Faux & Faux\end{tabular}
\end{center}

On a donc :

\[\boxed{
\neg (P\wedge Q)\Leftrightarrow \neg P\vee \neg Q
}\]

Ainsi que :

\[\boxed{
\neg (P\vee Q)\Leftrightarrow \neg P\wedge \neg Q)
}\]

\subsubsection{Loi de Peirce}

La loi de Pierce se démontre par le biais du raisonnement $((A\Rightarrow B)\Rightarrow A)\Leftrightarrow ((\neg A\vee B)\Rightarrow A)\Leftrightarrow \neg (\neg A\vee B)\vee A\Leftrightarrow \neg \neg A\wedge \neg B\vee A\Leftrightarrow A\wedge \neg B\vee A\Leftrightarrow (A\vee A)\wedge (\neg B\vee A)\Rightarrow A$. \textcolor{red}{(équivalence à la dernière étape ?)}

De sorte qu'on puisse simplement écrire :

\[\boxed{
((A\Rightarrow B)\Rightarrow A)\Rightarrow A
}\]

En postulant la loi de Peirce on peut par ailleurs démontrer l'axiome de la logique classique (il y a équivalence).

\textcolor{red}{(trouver illustration)}

\subsubsection{Raisonnement par contraposé}

On démontre par la seule ligne $(A\Rightarrow B)\Leftrightarrow A\vee \neg B\Leftrightarrow \neg \neg A\vee \neg B\Leftrightarrow (\neg A\Leftarrow \neg B)$ que :

\[\boxed{
(A\Rightarrow B)\Leftrightarrow  (\neg A\Leftarrow \neg B)
}\]

Cette propriété est tellement intuitive qu'elle est plus un élément de vocabulaire qu'un résultat intéressant en soi.

\subsubsection{Raisonnement par l'absurde}

$(\neg P\Rightarrow (Q\wedge \neg Q))\Leftrightarrow (\neg \neg P\Leftarrow \neg(Q\wedge \neg Q))\Leftrightarrow P$. Et donc :

\[\boxed{
(\neg P\Rightarrow (Q\wedge \neg Q))\Leftrightarrow P
}\]

Ce type raisonnement est très utilisé dans les jeux de logique : Sudoku, Mastermind... L'idée est de supposer $\neg P$, d'arriver à une contradiction et donc d'en conclure $P$.

\section{Théorie naïve des ensembles}

\subsection{Quantificateurs}

\subsection{Relations}

\textcolor{red}{(évoquer la théorie des graphs)}

\subsection{Algèbre de Boole}

\subsection{Produit carthésien}

\section{Algèbre}

\textcolor{red}{(définition de $\mathbb{N}$ par le cardinal)}

\chapter{Arithmétique}

L'arithmétique est sans doute la moins utile des branches des mathématiques pour l'ingénierie. Elle est aussi la plus complexe de toutes. Beaucoup de questions la concernant restent encore aujourd'hui sans réponse, et même les conjectures les plus simples comme celle du \emph{dernier théorème de Fermat} peuvent avoir traversé les âges comme défi pour les mathématiciens ; pour finalement mener à des démonstrations de près sinon plus d'une centaine de page.

Les principales applications de l'arithmétique pour l'ingénieur ont trait au dénombrement et à la cryptographie ; bien que dans le dernier cas l'arithmétique n'est intéressante que parce-qu'elle produit des algorithmes que nous pensons asymétriques ; sans en être sûrs pour autant. Il faut savoir qu'aujourd'hui, toute la sécurité informatique du monde est basée sur une \emph{croyance mathématique} qui consiste à penser qu'il n'est pas possible d'aller aussi vite (sans faire usage du \emph{calcul quantique}) pour décomposer un nombre en ses facteurs premiers que pour multiplier ceux-ci ; mais aucune démonstration expertisée n'existe encore pour corrober cette conjecture.

Notez que la première conséquence sociétale de la construction d'ordinateurs quantiques serait d'ouvrir la voie à l'effondrement de la quasi-totalités des systèmes de sécurité en fonctionnement dans le monde, car l'\emph{algorithme de Shor} permet une décomposition en facteurs premiers rapide.

La seconde serait la nécessité de la mise en place d'un protocole de transmission de données critiques - fondamentalement inviolable soit dit en passant - comme \emph{BB84} pour contrer la première.

La dernière serait de permettre la simulation des systèmes quantiques avec autant d'efficacité qu'on en a aujourd'hui pour les systèmes classique, faisant peut-être entrer l'ingénierie dans une nouvelle ère. Mais ce n'est plus en rapport avec ce chapitre.

L'arithmétique est l'étude de l'ensemble $\mathbb{Z}$ tel qu'on l'a vu dans le chapitre précédent \textcolor{red}{(pas fait)}, bien que la plupart des résultats concernent simplement $\mathbb{N}$ voir des sous-ensembles finis de celui-ci.

\section{Démonstraton par récurrence}

Soit un prédicat prenant en entrée un entier naturel $n \in \mathbb{N}$, alors il est clair que :

\[\boxed{
\left(P_a \cap (\forall n\geq a, \; P_{n+1} \Rightarrow P_n)\right) \Rightarrow (\forall n\geq a, \; P_n)}
\]

Cette propriété est déduite du cinquième axiome de Peano \textcolor{red}{(pas fait)}.

Une \emph{démonstration par récurrence} se fera donc en deux temps :

\begin{itemize}
\item Initialisation : on démontre $P_a$.
\item Récurrence : on démontre $\forall n\geq a, \; P_{n+1} \Rightarrow P_n$.
\end{itemize}

Et la véracité de $P_n, \; \forall n\geq a$ est induite. Nous allons en voir des exemple dans la section suivante.

\section{Suite \& série}

\subsection{Suite}

Une \emph{suite} $u$ est un ensemble en bijection avec $\mathbb{N}$ ou bien avec une partie de $\mathbb{N}$. Intuitivement, c'est donc un \emph{ensemble ordonné et dénombrable}, puisqu'elle est attribuée d'une fonction successeur permettant de passer de l'un de ses éléments au suivant.

La suite finie est le seul objet pouvant-être mis en mémoire dans un ordinateur (encore qu'il faille aussi que les valeurs possibles que peuvent prendre les termes de la suite soient en nombre fini). C'est pourquoi on cherchera à approximer toute sorte d'entité mathématique par des suites finies dès que l'on aura pour projet d'employer le calcul numérique.

Une suite peut-être définie absolument ou récursivement. Dans le premier cas, on donne directement la fonction $f$ qui met l'ensemble $E$ considéré en bijection avec $\mathbb{N}$ :

\[\boxed{
f : i \rightarrow u_i
}\]

Dans le second, c'est la fonction successeur ainsi que le premier terme qui permettent de construire la suite :

\[\boxed{
s : u_i \rightarrow u_{i+1}
}\]

La première formulation est souvent plus élégante pour mener des démonstrations, la seconde plus adaptée au calcul algorithmique qu'on implémente sur ordinateur.

\subsubsection{Variation d'un suite}

On dit qu'une suite est \emph{croissante} sur un intervalle $[m;n]\cap \mathbb{N}$ ssi :

\[\boxed{
\forall i \in [m;n-1]\cap \mathbb{N}, \; u_{i+1} \geq u_i
}\]

De la même façon, une suite sera \emph{décroissante} ssi :

\[\boxed{
\forall i \in [m;n-1]\cap \mathbb{N}, \; u_{i+1} \leq u_i
}\]

Une suite croissante ou décroissante sur un intervalle est dite \emph{monotone} sur ce même intervalle.

\subsubsection{Convergence d'une suite}

Une \emph{suite infinie} comprend une infinité de termes, et est donc en bijection avec $\mathbb{N}$ tout entier.

Une classe de suites qui va être particulièrement intéressante est celle pour lesquelles :

\[\boxed{
\exists l \in \mathbb{N} : \; \forall \varepsilon \in \mathbb{R}, \; \exists \eta \in \mathbb{N} : \; \forall n>\eta, \; |u_n - l| < \varepsilon
}\]

\textcolor{red}{(pas défini le module avant)}

Où $l$ est qualifiée de \emph{limite} de $u$, et où $u$ est dite \emph{convergente}. L'idée étant de comprendre $l$ comme la valeur qu'on aurait envie d'affecter à $u_\infty$ si celle-ci existe, c'est-à-dire si la suite tend bien vers une constante.

Un mathématicien rigoureux prendra le soin de prouver la convergence d'une suite avant d'en calculer la limite, mais un ingénieur a souvent une vie bien trop remplie pour se permettre un tel luxe ; il veillera cependant à ne pas faire n'importe quoi et se convaincra mentalement et par des arguments simples qu'il y a bien convergence ou non.

Pour calculer la limite, on usera du fait qu'à l'infini la suite est constante et donc que $\boxed{u_{i+1} = u_i}$.

\subsubsection{Suite arithmético-géométrique}

Une \emph{suite arithmétique} est définie par son premier terme et sa relation de récurrence :

\[\boxed{
u_{i+1} = u_i + b
}\]

Nous allons montrer par récurrence que son expression absolue est :

\[\boxed{
u_i = u_0 + i\times b
}\]

Commençons :

\begin{itemize}
\item Initialisation : $u_0 = u_0 + 0\times b$, nécessairement vrai.
\item Récurrence : supposons $u_i = u_0 + i\times b$, alors $u_{i+1} = u_i + b = u_0 + i\times b + b = u_0 + (i+1)\times b$
\end{itemize}

Ces deux résultats suffisent à conclure à la véracité de la proposition $u_i = u_0 + i\times b$.

Une \emph{suite géométrique} s'écrit :

\[\boxed{
u_{i+1} = u_i \times a
}\]

D'expression absolue :

\[\boxed{
u_i = u_0 \times a^i
}\]

Parce-que :

\begin{itemize}
\item Initialisation : $u_0 = u_0 \times a^0$, nécessairement vrai.
\item Récurrence : supposons $u_i = u_0 \times a^i$, alors $u_{i+1} = u_0 \times a^i \times a = u_0 \times a^{i+1}$
\end{itemize}

Tout ceci semble évident, mais maintenant qu'en est-il de la \emph{suite arithmético-géométrique} ?

\[\boxed{
u_{i+1} = a u_i + b
}\]

(dorénavant je n'écrirai que très rarement le produit ``$\times$'', comme tout le monde le fait pour alléger notablement les notations.)

Alors le terme général s'écrira si $a\neq 1$ :

\[\boxed{
u_i = \left(u_0 - \frac{b}{1-a}\right) a^i + \frac{b}{1-a}
}\]

Ce résultat est beaucoup moins intuitif que les précédents, mais la démonstration s'effectue d'une façon tout à fait analogue :

\begin{itemize}
\item Initialisation : 

\[
u_0 = \left(u_0 - \frac{b}{1-a}\right) a^0 + \frac{b}{1-a}
\]

\item Récurrence :

\[
u_i = \left(u_0 - \frac{b}{1-a}\right) a^i + \frac{b}{1-a}
\]

\[
\Rightarrow u_{i+1} = a\left(\left(u_0 - \frac{b}{1-a}\right) a^i + \frac{b}{1-a}\right) + b
\]

\[
\Rightarrow u_{i+1} = \left(u_0 - \frac{b}{1-a}\right) a^{i+1} + \frac{ab}{1-a} + \frac{b(1-a)}{1-a}
\]

\[
\Rightarrow u_{i+1} = \left(u_0 - \frac{b}{1-a}\right) a^{i+1} + \frac{ab+b(1-a)}{1-a}
\]

\[
\Rightarrow u_{i+1} = \left(u_0 - \frac{b}{1-a}\right) a^{i+1} - \frac{b}{1-a}
\]

\end{itemize}

La suite arithmético-géométrique est relativement peu utile, et interviendra surtout en théorie des graphes ou pour modéliser des réflexions multiples en physique. Elle nous a permis néanmoins d'introduire l'arithmétique de façon douce, les démonstrations se compliquant notablement dans les prochaines sections.

La série arithmético-géometrique converge ssi $a<1$, auquel cas $a^i \rightarrow 0$ et sa limite vaut :

\[\boxed{
\lim u = \frac{b}{1-a}
}\]

Qui est bien la seule valeur possible de $u_i$ pour que :

\[
u_{i+1} = au_i+b
\]

\[
u_{i+1} = \frac{ab}{1-a}+b
\]

\[
u_{i+1} = \frac{ab+b(1-a)}{1-a}
\]

\[
u_{i+1} = \frac{b}{1-a}
\]

\[
u_{i+1} = u_i
\]

\subsection{Série}

Une série est une suite définie comme la somme des termes d'une autre suite :

\[\boxed{
\left\{\begin{matrix}
S_0 = u_0 \\
S_{n+1} = S_n + u_{n+1}
\end{matrix}\right.
}\]

Pour dire que $S$ est la série associée à $u$, on écrira :

\[\boxed{
S_n = \sum_{i=0}^n u_i
}\]

Le symbole $\Sigma$ désignant la sommation des termes d'une suite. On peut ainsi voir la multiplication \textcolor{red}{(pas parlé avant)} sur $\mathbb{Z}$ comme étant la série associée à une suite constante, au premier terme près :

\[
n\times x = \sum_{i=1}^n x
\]

Une autre façon de voir la chose est de considérer $u_n = n\times x$ comme une suite arithmétique initialisée à $0$.

J'en profite pour définir également le symbole $\Pi$ qui désigne un produit de termes :

\[\boxed{
\left\{\begin{matrix}
P_0 = u_0 \\
P_{n+1} = P_n \times u_{n+1}
\end{matrix}\right.
\Leftrightarrow
P_n = \prod_{i=0}^n u_i
}\]

Qui nous sera utile dès la prochaine section. Par analogie avec la série associée à la suite constante, la fonction puissance sur $\mathbb{Z}$ est le produit de constante :

\[
x^n = \prod_{i=1}^n x
\]

Qui n'est autre qu'une suite géométrique initialisée à $1$.

\subsubsection{Série arithmético-géométrique}

La série associée à une suite arithmético-géométrique est donc donnée par :

\[
S_n = \sum_{i=0}^n \left(\left(u_0 - \frac{b}{1-a}\right) a^i + \frac{b}{1-a} \right)
\]

\[
S_n =  \left(u_0 - \frac{b}{1-a}\right) \sum_{i=0}^n a^i + \frac{b}{1-a}(n+1)
\]

Montrons que :

\[\boxed{
\sum_{i=0}^n a^i = \frac{1-a^{n+1}}{1-a}
}\]

Par récurrence :

\begin{itemize}
\item Initialisation : 

\[
\sum_{i=0}^0 a^i = 1 = \frac{1-a^1}{1-a}
\]

\item Récurrence :

\[
\sum_{i=0}^n a^i = \frac{1-a^{n+1}}{1-a}
\]

\[
\Rightarrow \sum_{i=0}^{n+1} a^i = \frac{1-a^{n+1}}{1-a} + a^{n+1}
\]

\[
\Rightarrow \sum_{i=0}^{n+1} a^i = \frac{1-a^{n+1}}{1-a} + \frac{(1-a)a^{n+1}}{1-a}
\]

\[
\Rightarrow \sum_{i=0}^{n+1} a^i = \frac{1-a^{(n+1)+1}}{1-a}
\]
\end{itemize}

Ce qui nous donne pour la série :

\[\boxed{
S_n =  \left(u_0 - \frac{b}{1-a}\right) \frac{1-a^{n+1}}{1-a} + \frac{b}{1-a}(n+1)
}\]

La légende dit que le petit Gauss - celui qui trouva nombre de résultats que nous verrons dans ce chapitre - a à l'âge de onze ans calculé mentalement et très rapidement la somme des 100 premiers entiers naturels :

\[
S_{100} =   \sum_{i=0}^{100} i
\]

Qui est la série arithmétique d'ordre $1$, donc pour laquelle $u_0 = 0$, $a=1$ et $b=0$. Cependant, la formule précédente ne peut pas s'appliquer à un tel cas car $a=1$ est \emph{singulier}. Cependant, le raisonnement de Gauss l'aurait amené à penser que :

\[\boxed{
\sum_{i=0}^{n} i = \frac{n(n+1)}{2}
}\]

Ce qui se montre facilement par récurrence :

\begin{itemize}
\item Initialisation :

\[
\sum_{i=0}^{0} i = 0 = \frac{0(0+1)}{2}
\]

\item Hérédité :

\[
\sum_{i=0}^{n} i = \frac{n(n+1)}{2}
\]

\[
\Rightarrow \sum_{i=0}^{n+1} i = \frac{n(n+1)}{2}+ (n+1)
\]

\[
\Rightarrow \sum_{i=0}^{n+1} i = \frac{n(n+1)}{2}+ \frac{2(n+1)}{2}
\]

Qu'on développe en :

\[
\Rightarrow \sum_{i=0}^{n+1} i = \frac{n^2+3n+2}{2}
\]

Que je vous laisse vérifier être égale à :

\[
\Rightarrow \sum_{i=0}^{n+1} i = \frac{(n+1)(n+2)}{2}
\]

La factorisation d'un polynôme étant évoquée dans le chapitre suivant.

\end{itemize}

Et Gauss aurait trouvé $S_{100} = \frac{100\times101}{2} = 5050$.

Je tiens à souligner que la démonstration par récurrence est une méthode présupposant la connaissance du résultat, et qu'elle fait totalement écran à la démarche constructive qui a dû être menée pour ne serait-ce que l'intuiter. Par exemple, elle ne dit pas que Gauss a remarqué que $\forall i, \; u_i+u_{n-i} = i+(n-i) = n$ pour arriver rapidement à ce résultat.

La série arithmético-géométrique ne converge que dans le cas $a<1\wedge b=0$, auquel cas :

\[\boxed{
\lim S =  \frac{u_0}{1-a}
}\]

\subsubsection{Série de Riemann}

La série de Riemann est la version la plus archaïque d'un objet mathématique considéré par la communautée scientifique comme recelant le problême mathématique le plus tenace sur lequel on se soit jamais penché. En effet, la recherche de la démonstration de l'\emph{hypothèse de Riemann} conjecturée en 1859 figurait déjà dans la liste des problêmes posés par Hilbert comme priorité pour les universitaires à l'occasion du passage en l'an $1900$.

Un siècle plus tard, il est en tête de liste des sept problêmes posés pour l'an $2000$.

Comme tout ce que vous pouvez trouver dans ce chapitre, les séries de Riemann ne sont que très rarement utiles à l'ingénieur, mais elles permettent de mettre en évidence une profonde structure sous-jacente aux séries qui fait qu'on doit manipuler celles-ci avec beaucoup de précaution lorsqu'on s'intéresse à leur convergence.

On a :

\[
\forall s \in \mathbb{Z}, \; S(s) : n \rightarrow \sum_{i=0}^n i^{-s}
\]

En notant :

\[\boxed{
\forall s \in \mathbb{N}\setminus \{0;1\}, \; \zeta(s)=\lim S(s) =\sum_{i=0}^\infty i^{-s}
}\]

Qui peut-être prolongée (au sens de \emph{Laurent}) \textcolor{red}{(pas parlé après)} dans $\mathbb{C}\setminus {1}$, et c'est la recherche des points $s$ auxquels $\zeta(s)=\lim S(s)=0$ qui retourne les cerveaux depuis un siècle et demi.

Première remarque : $\forall s \in \mathbb{N}\setminus \{0\}, \; \lim i^{-s} = 0$. C'est une condition nécessaire pour que la série converge, mais pas suffisante ! Autrement dit, en toute généralité on a l'implication :

\[\boxed{
\sum u \; converge \Rightarrow \forall u, \lim u = 0
}\]

Mais pas l'équivalence. Et en effet, bien que $\lim i^{-1} = 0$, la série $S(1)$ ne converge pas, et $1$ est singulier pour $\zeta$.

Par contre, en $s=2$, on a bien convergence :

\[
\zeta(2) = \frac{\pi^2}{6}
\]

Que nous ne démontrerons nulle part dans ce cours, bien que tous les outils qui permettent de le faire y figurent. Je voudrais juste que vous preniez ce petit écart sur les séries de Riemann comme un premier avertissement sur les étrangetés qui peuvent survenir quand on regarde vers l'infini, et sur la délicatesse dont il faut faire preuve si on ne veut pas s'y perdre.  

\section{Dénombrement}

Le dénombrement a trait au rangement d'objets - éventuellement préalablement catégorisés - dans des tiroirs. Il n'intervient presque jamais directement en physique, mais les méthodes que nous allons voir peuvent figurer dans des démonstrations mathématiques menant à des résultats qui peuvent - eux - se révéler utiles en pratique.

Le cas le plus simple est celui où nous avons en notre possession $n$ objets et $n$ tiroirs, et où on s'essaye au comptage des dispositions possibles que peuvent prendre les objets sachant qu'un tiroir doit contenir un unique objet. On dit que le nombre de \emph{permutations} est donné par :

\[\boxed{
P_n = \prod_{i=1}^n i = n!
}\]

Ce nouvel outil noté par un point d'exclamation - qu'on prénomme la \emph{factorielle} - est l'élément de base du dénombrement.

L'idée est qu'en choisissant un premier tiroir pour le premier objet, on enlève un choix possible pour le second, et ainsi de suite jusqu'à temps qu'il n'en reste plus qu'un (le plus simple est donc de voir le produit général comme un produit descendant).

Dans un second temps, nous allons supposer qu'il n'est plus possible de ranger tous les objets, car nous n'avons à notre disposition plus que $k$ tiroirs. Le nombre de solutions à ce nouveau problême s'appelle le nombre d'\emph{arrangements} :

\[\boxed{
A_n^k = \prod_{i=k+1}^n i = \frac{n!}{(n-k)!}
}\]

Qui reste plus simple à intuiter en considérant le produit général comme décroissant.

Enfin - et c'est le réel outil de dénombrement qui nous sera utile dans les formules - imaginons qu'on n'ai plus à s'en faire des tiroirs, mais qu'il faille simplement séparer les objets en deux groupes, le premier étant de taille imposée $k$. Alors, on se convaincra que le nombre de \emph{combinaisons} est donné par :

\[\boxed{
C_n^k = \frac{A_n^k}{P_k} = \frac{\prod_{i=k+1}^n i}{\prod_{i=1}^k i} = \frac{n!}{k!(n-k)!}
}\]

Ainsi, dès que nous aurons à choisir $k$ éléments dans un ensemble comprenant $n$ éléments - sans considération aucune pour leur ordre - alors c'est la combinaison que nous emploierons.

L'exemple le plus frappant est celui du binôme de Newton, qui généralise l'une des identités remarquables vues au chapitre précédent \textcolor{red}{(pas fait)}. Nous allons montrer par récurrence que :

\[\boxed{
\forall (x,y) \in \mathbb{C}, \; (x+y)^n=\sum_{k=0}^n C_n^k x^{n-k} y^k,
}\]

Allons-y :

\begin{itemize}
\item Initialisation : Pour $n = 0$, parce-que $0! = 1$ (ça vous surprend ? Quel est l'élément neutre pour la multiplication ?), on trouve :

\[
(x+y)^0=C_0^0 x^0y^0
\]

\[
1 = 1
\]

Qui est à priori vrai.

\item Récurrence : Supposons la formule vraie à l'ordre $n$. Alors à l'ordre $n+1$ :

\[
(x+y)^{n+1}=(x+y)\sum_{k=0}^n C_n^k x^{n-k} y^k,
\]

\[
(x+y)^{n+1}=x^{n+1}+x\sum_{k=1}^n C_n^k x^{n-k} y^k 
+y\sum_{k=0}^{n-1} C_n^k x^{n-k} y^k
+ y^{n+1}
\]

En changeant de variable pour $k' = k+1$ dans la seconde somme :

\[
(x+y)^{n+1}=x^{n+1}+y^{n+1}+x\sum_{k=1}^n C_n^k x^{n-k} y^k 
+y\sum_{k'=1}^{n} C_n^{(k'-1)} x^{n-(k'-1)} y^{k'-1}
\]

On rentre $x$ et $y$ dans les sommes :

\[
(x+y)^{n+1}=x^{n+1}+ y^{n+1}+\sum_{k=1}^n C_n^k x^{n-k+1} y^k 
+\sum_{k'=1}^{n} C_n^{(k'-1)} x^{n-k'+1} y^{k'}
\]

Et en combinant celles-ci :

\[
(x+y)^{n+1}=x^{n+1}+ y^{n+1}+\sum_{k=1}^n \left(C_n^k + C_n^{(k-1)}\right) x^{n-k+1} y^k
\]

C'est alors qu'il nous faut démontrer un résultat intermédiaire sur les combinaisons :

\[
C_n^k + C_n^{(k-1)} = \frac{n!}{k!(n-k)!} + \frac{n!}{(k-1)!(n-k+1)!}
\]

\[
C_n^k + C_n^{(k-1)} = \frac{(n-k+1)n!}{k!(n-k+1)(n-k)!} + \frac{kn!}{k(k-1)!(n-k+1)!}
\]

\[
C_n^k + C_n^{(k-1)} = \frac{(n-k+1)n!}{k!(n-k+1)!} + \frac{kn!}{k!(n-k+1)!}
\]

\[
C_n^k + C_n^{(k-1)} = \frac{(n-k+1)n!+kn!}{k!(n-k+1)!}
\]

\[
C_n^k + C_n^{(k-1)} = \frac{(n+1)n!}{k!(n-k+1)!}
\]

\[
C_n^k + C_n^{(k-1)} = \frac{(n+1)!}{k!(n-k+1)!}
\]

\[
C_n^k + C_n^{(k-1)} = C_{n+1}^k
\]

Nous en sommes donc à :

\[
(x+y)^{n+1}=x^{n+1}+ y^{n+1}+\sum_{k=1}^n C_{n+1}^k x^{n-k+1} y^k
\]

Et parceque $\forall n, \; C_{n+1}^0 = 1$ :

\[
(x+y)^{n+1}=C_{n+1}^0 x^{n+1}y^0+ C_{n+1}^0 x^0 y^{n+1}+\sum_{k=1}^n C_{n+1}^k x^{n-k+1} y^k
\]

\[
(x+y)^{n+1}=\sum_{k=1}^{n+1} C_{n+1}^k x^{n-k+1} y^k
\]
\end{itemize}

Ce qui achève la récurrence. Cette démonstration étant la première un peu complexe qu'on ait eu à effectuer, j'ai été loin dans le détail, mais à l'avenir je serai moins généreux en terme d'étapes de calcul.

\section{Division euclidienne}

La division euclidienne est l'outil d'approximation par excellence. Evidemment, on l'utilise souvent sous une forme très détournée, mais nous verrons qu'elle est à la base des tentatives d'approximation locale des fonctions telle que la \emph{série de Taylor}.

Elle se trouve néanmoins appliquée de façon quasi-directe dans un domaine de l'ingénierie : quand il s'agit de calculer la transformée en $z$ inverse d'un filtre linéaire ou linéarisé.

Tout part d'un constat :

\[\boxed{
\forall (a,b) \in \mathbb{Z}^2, \; \exists ! (q,r) \in \mathbb{Z}^2 :
\left\{\begin{matrix}
a = bq+r \\
0\leq r<|b|
\end{matrix}\right.
}\]

$q$ est appellé \emph{quotient}, et $r$ est le \emph{reste}.

Si en outre $r = 0$, on dit que ``$b$ divise $a$'' noté $b|a$.

On pourra également écrire que :

\[
\frac{a}{b} = q + \frac{r}{b}
\]

Laissant penser que $q$ est l'approximation entière de $\frac{a}{b}$, qui est l'idée qu'il va falloir garder à l'esprit pour toute cette section.

Nous passerons sur la démonstration de ce ``constat'' étant donné qu'une preuve très simple peut-être formulée sur la base de l'algorithme de calcul qui sera présenté plus loin. En effet, si l'on est en mesure à partir de $a$ et de $b$ de construire $r$ et $q$, il ne nous reste plus qu'à prouver leur unicité ce qui se fait facilement en remarquant que tout $q$ plus grand ou plus petit engendrerait une violation de la relation $0\leq r<|b|$.

\subsection{Numération}

Avant de rentrer plus dans le détail de la division euclidienne - laquelle forme vraiment le coeur de l'arithmétique et des ennuis qu'il apporte - je souhaiterais dire un mot des systèmes de numération, en espérant que vous percevrez le lien entre les deux.

La numération est l'étude des techniques de comptage, notamment de celles qui existèrent historiquement.

Le premier mode de comptage qu'on pourrait envisager consisterait à représenter l'unité par un bâton, et de copier ce bâton autant de fois qu'il le faut. Mettons qu'on veuille écrire le nombre $a$, cette décomposition s'écrirai de façon moderne :

\[
a = \sum_{i=1}^x 1
\]

L'indice supérieur de la somme étant $a$, il nous faudra donc dessiner $a$ bâtons. Cela peut s'avérer fastidieux, et un bon système de numération implémentera un bon compromis entre nombre de caractères utilisables (plus il y a de caractères, plus c'est complexe) et nombre de caractères à écrire pour un nombre donné.

Les romains et égyptiens ont eu l'idée de regrouper des quantités sous différents signes. Par exemple, pour un ensemble de caractères disponibles ên bijection avec les poids $c_i$ qu'ils représentent :

\[
a = \sum_{i} a_i c_i
\]

Avec $a_i$ le nombre de fois que sont écrits les caractères de poids $c_i$. Il n'existe alors plus une unique façon d'écrire $x$. Cependant, si nous imposons que le nombre de caractères écrits:

\[
\sum_{i} a_i
\]

Doit-être choisi minimal, alors l'écriture devient unique. La structure émergente est dite d'\emph{algèbre}, et bien que le lien avec notre propos soit très fort il est encore trop tôt pour en parler.

Les systèmes de numération modernes sont plus malins, et reposent sur l'idée d'écrire directement la suite $a$. En outre, on utilise la décomposition suivante :

\[
a = \sum_{i=0}^\infty a_i \beta^i
\]

Et même en introduisant la virgule pour identifier la position de $a_0$ :

\[\boxed{
a = \sum_{i \in \mathbb{Z}} a_i \beta^i
}\]

Avec $\beta \in \mathbb{N}$ la \emph{base} de numération. Historiquement fûrent surtout utilisés la base $60$ et la base $10$. La base $2$ est le fameux \emph{binaire} dans lequel comptent les ordinateurs ; et la base $16$ dite \emph{hexadécimale} est souvent employée pour rendre plus compacte l'écriture du binaire.

Par exemple, l'écriture de $420$ en base $10$ est :

\[
420 = 4\times 10^2 + 2\times 10^1 + 0\times 10^0
\]

Si l'on travaille en base $\beta$ et qu'on a $\beta$ caractères à notre disposition, alors l'écriture est nécessairement unique.

\subsection{Changement de base}

Nous en sommes à :

\[
a = \sum_{i \in \mathbb{Z}} a_i \beta^i
\]

Pris tels que $\sum_{i \in \mathbb{Z}} a_i$ soit minimal.

Nous voyons alors que le calcul des $a_i$ sont définis par une récurrence descendante faisant intervenir une division euclidiennes \textcolor{red}{(démonstration)} :

\[\boxed{
\left\{\begin{matrix}
a_{i-1} = r_i / \beta^i \\
r_{i-1} =  r_i \% \beta^i
\end{matrix}\right.
}\]

Avec $r_i / \beta^i$ le quotient de $r_i$ par $\beta^i$ et $r_i \% \beta^i$ le reste associé (notations issues de l'informatique).

Pour écrire $420$ en base $2$ nous ferions donc :

\[
\left\{\begin{matrix}
a_{8} = 420 / 2^9 = 0\\
r_{8} =  420 \% 2^9 = 420
\end{matrix}\right.
\]

\[
\left\{\begin{matrix}
a_{7} = 420 / 2^8 = 1\\
r_{7} =  420 \% 2^8 = 420-2^8 = 164
\end{matrix}\right.
\]

\[
\left\{\begin{matrix}
a_{6} = 164 / 2^7 = 1\\
r_{6} =  164 \% 2^7 = 164-2^7 = 36
\end{matrix}\right.
\]

\[
\left\{\begin{matrix}
a_{5} = 36 / 2^6 = 0\\
r_{5} =  36 \% 2^6 = 36
\end{matrix}\right.
\]

\[
\left\{\begin{matrix}
a_{4} = 36 / 2^5 = 1\\
r_{4} =  36 \% 2^5 = 36-32=4
\end{matrix}\right.
\]

\[
\left\{\begin{matrix}
a_{3} = 4 / 2^4 = 0\\
r_{3} =  4 \% 2^4 = 4
\end{matrix}\right.
\]

\[
\left\{\begin{matrix}
a_{2} = 4 / 2^3 = 0\\
r_{2} =  4 \% 2^3 = 4
\end{matrix}\right.
\]

\[
\left\{\begin{matrix}
a_{1} = 4 / 2^2 = 1\\
r_{1} =  4 \% 2^2 = 0
\end{matrix}\right.
\]

Le reste étant nul, tous les chiffres suivants sont $0$. On a donc en base 2:

\[
420 = (110100100)_2
\]

\subsection{Calcul de la division euclidienne}

Nous voici enfin au stade où la compréhension de l'algorithme de division euclidienne appris dans l'enfance est à portée de main.

Soient deux nombres $a$ et $b$ écrits dans une même base $\beta$, alors l'algorithme de calcul de $a/ b$ et $a\% b$ est donné par :

\[\boxed{
\left\{\begin{matrix}
q_{i-1} = (r_i/ \beta^{i-1}) / b \\
r_{i-1} = r_i\% (b\beta^{i-1})
\end{matrix}\right.
}\]

\textcolor{red}{(démonstration)}

Vous laissant le soin de vérifier si c'est bien ce que vous faites quand vous divisez ; sous l'hypothèse que vous sachiez encore diviser. Par exemple pour $\frac{420}{13}$, en base $10$ :

\[
\left\{\begin{matrix}
q_{2} = (420/ 10^2) / 13 = 0 \\
r_{2} = 420\% (13\times 10^2) = 420
\end{matrix}\right.
\]

\[
\left\{\begin{matrix}
q_{1} = (420/ 10^1) / 13 = 3 \\
r_{1} = 420\% (13\times 10^1) = 420- 390 = 30
\end{matrix}\right.
\]

\[
\left\{\begin{matrix}
q_{0} = (30/ 10^0) / 13 = 2 \\
r_{0} = 30\% (13\times 10^0) = 30- 26 = 4
\end{matrix}\right.
\]

On a $r<b$, ce qui permet de s'arréter pour écrire :

\[
420 = 13 \times 32 + 4
\]

On pourrait néanmoins calculer $q_{-1}$, qui est le premier chiffre après la virgule, et ainsi de suite jusqu'au degré de précision que l'on veut.

C'est pour cette raison que la division euclidienne est la pierre angulaire des méthodes d'approximation locales des fonctions que nous verrons dans le prochain chapitre \textcolor{red}{(pas fait)}. Elle permet de trouver une valeur raisonnable à donner à un nombre, avec un reste aussi petit qu'on le désire.

Ce remarquable mode de décomposition est relié aux polynômes, que nous aborderons également dans le prochain chapitre.

\section{Primalité}

C'est ici que les ennuis commencent vraiment.

La lecture de cette section n'est vraiment pas indispensable, bien qu'elle puisse nous être utile par ses aspects les plus basiques pour trouver l'écriture la plus simple d'une fraction, et pour décomposer les fractions rationnelles. Cependant, la section suivante fournira une application très particulière mais directe aux théorèmes les plus avancés figurant dans la présente.

\subsection{Simplification des fractions}

Nous avons vu qu'un système de numération simple consiste à trouver les $c_i$ permettant d'écrire $a$ comme :

\[
a = \sum_{i=1}^\infty a_i c_i
\]

Une autre façon - plus efficace mais bien plus difficile à manier - consiste à assimiler le nombre à la suite $a'$, de sorte que :

\[
a = \prod_{i=1}^\infty {c'_i}^{a'_i}
\]

Je n'aurais pas la moindre idée de comment m'y prendre pour prouver que cette solution est la meilleure de toutes, mais en voici une qui dans les faits répond particulièrement bien à ce problême. Prendre $c'=p$ avec :

\[
\forall a \in \mathbb{N}\setminus\{1\}, \; a\in p \Leftrightarrow \left(\forall b \in \mathbb{N}\setminus \{0;1;a\}, \; \neg{(b|a)} \right) 
\]

Et la suite $p$ est celle des \emph{nombres premiers}. L'idée est donc de choisir les nombres $p_i$ ne pouvant pas être décomposés en produit de nombres plus petits ; et c'est bien ce que sont les nombres premiers.

Le passage de l'écriture sommatoire (la suite $a$) à l'écriture en un produit de puissances de nombres premiers (la suite $a'$) est ce qu'on appelle la \emph{factorisation en facteurs premiers}, et c'est parce-que nous n'avons pas connaissance d'algorithme non-quantique permettant d'effectuer cette opération rapidement (alors que les facteurs premiers étant connus, leur multiplication est facile même s'ils sont très grands) que nous sommes en mesure de concevoir des systèmes de chiffrement à clé publique robustes, comme nous en verrons un exemple dans la prochaine section.

Cette décomposition en produit de facteurs premiers trouve tout son intéret dans l'écriture fractionnaire :

\[\boxed{
\forall (a,b)\in \mathbb{N}^{*2}, \; \frac{a}{b} = \prod_{i=1}^\infty\frac{p_i^{a'_i}}{p_i^{b'_i}}
}\]

Parce-qu'en effet, si pour un certain $i$, $a_i$ et $b_i$ sont tous deux non-nuls, on pourra simplifier pour enlever un terme au numérateur ou au dénominateur, ce qui est plus économique en terme d'écriture.

Par exemple, la fraction suivante s'écrit en décomposant les deux nombres en facteurs premiers :

\[
\frac{120}{234} = \frac{17\times 3\times 2}{13\times 3^2\times 2}
\]

Qu'on simplifie en :

\[
\frac{120}{234} = \frac{17}{13\times 3}
\]

\[
\frac{120}{234} = \frac{17}{39}
\]

Qui est donc la forme la plus simple qu'on puisse lui donner.

L'étape délicate étant la décomposition en facteurs premiers. Une autre manière de calculer une simplification de fraction passe par le \emph{pgcd} et l'\emph{algorithme d'Euclide}. Mais vraiment, l'ingénieur moyen ne trouvera aucune utilité à l'apprendre ; si par miracle vous en auriez besoin, demandez à un collégien de vous faire un point dessus, l'éducation nationale persistant à continuer de l'enseigner sans que personne ne sache vraiment pourquoi à l'ère de l'informatique.

\subsection{Petit théorème de Fermat}

\textcolor{red}{(compléter)}

\section{Cryptographie}

\chapter{Analyse}

\section{Intégration}

\textcolor{red}{(+intégration par parties)}

\section{Continuité}

\textcolor{red}{(+théorème des valeurs intermédiaires)}

\section{Dérivation}

\textcolor{red}{(+théorème des accroissements finis)}

\section{Série de fonctions}

\section{Fonctions polynomiales}

\subsection{Polynômes}

Les polynômes sont un sujet délicat des mathématiques, à la croisée des chemins entre analyse, arithmétique et algèbre linéaire. Il en émanera la structure la plus élaborée que nous verrons dans ce cours, celle d'\emph{algèbre}.

Ils sont définis par une série de fonction, elle-même générée par une suite $a$ appellée \emph{coefficients} :

\[\boxed{
P : x \rightarrow \sum_{i=0}^\infty a_i x^i
}\]

Sa dérivée sera donc donnée par \textcolor{red}{(pas utile tout de suite)} :

\[\boxed{
P'(x) = \sum_{i=0}^\infty i a_i x^{i-1}
}\]

Si la suite $a$ s'annule à un certain rang, on appelle $\deg P$ le rang de sa dernière valeur non-nulle.

On remarquera que l'écriture d'un polynôme ressemble furieusement à la décomposition d'un nombre dans une base, en posant $x = \beta$. Et en effet, la théorie des polynômes est la façon abstraite de parler des bases.

Si les polynômes sont liés à l'arithmétique, c'est parce-qu'ils décrivent une structure d'algèbre \textcolor{red}{(pas défini)}, au sens où on a le produit :

\[
P\times Q(x) = \left(\sum_{i=0}^\infty a_i x^i\right) \times \left(\sum_{j=0}^\infty b_j x^j\right)
\]

Pouvant s'écrire :

\[\boxed{
P\times Q(x) = \sum_{i=0}^\infty \sum_{j=0}^\infty a_i b_j x^{i+j}
}\]

Cette double somme sur $x^{i+j}$ est néanmoins ennuyeuse, car différentes valeurs de $i$ et $j$ permettent d'obtenir une valeur unique de $i+j$. Pour résoudre ce problême, on effecture un \emph{changement de variable} \textcolor{red}{(pas parlé avant)} en posant $k=i+j$ :

\[
P\times Q(x) = \sum_{k=0}^\infty \sum_{i=0}^k a_i b_{k-i} x^k
\]

La seconde somme a alors le bon goût de devenir finie, étant donné que $j$ ne pouvait-être négatif.

La somme $\sum_{i=0}^k a_i b_{k-i}$ est le coefficient $c_k$ du produit polynomiale à l'ordre $k$, de sorte que :

\[
P\times Q(x) = \sum_{k=0}^\infty c_k x^k
\]

En toute généralité, on posera $\deg 0 = -\infty$ de sorte qu'on ai toujours $\deg P\times Q = \deg P + \deg Q$.

Parce-que l'ensemble des polynômes - notée $\mathbb{K}[x]$ - décrivent une algèbre, on peut faire de l'arithmétique sur eux. Commençons par le début : la division Euclidienne.

\[\boxed{
\forall (A,B) \in \mathbb{K}^2[x], \; \exists ! (Q,R) \in \mathbb{K}^2[x] :
\left\{\begin{matrix}
A = BQ+R \\
\deg R<\deg B
\end{matrix}\right.
}\]

Parce-que la meilleur façon de voir la division euclidienne est de la voir comme une approximation, on pourra écrire :

\[
\frac{A}{B} = Q + \frac{R}{B}
\]

En se permettant de diviser les polynômes, donnant ainsi lieu à des \emph{fractions rationnelles}, qui ne sont des polynômes que si $B|A$.

Les fractions rationnelles et les divisions euclidiennes qui permettent leur calcul approché nous seront très utiles quand nous voudrons calculer des transformées en $z$ inverses de systèmes linéaires.

On procède exactement de la même façon que pour la division euclidienne, c'est pourquoi je ne fournirai pas d'exemple ici. Cependant, vous trouverez dans le chapitre de traitement du signal un exemple appliqué \textcolor{red}{(pas fait)}.

\subsection{Factorisation d'un polynôme}

Un polynôme peut se factoriser sous la forme :

\[
P(x) = \prod_{z\in \mathbb{C}} (x-z)^{a'(z)}
\]

Qui est une écriture très exotique, celle du produit généralisée sur l'ensemble continu $\mathbb{C}$. Cependant, cette écriture doit s'annuler seulement là où l'autre écriture s'annule, et si on appelle $r$ l'ensemble des valeurs de $x$ pour lesquelles $P$ s'annule - les \emph{racines} de $P$) - il faut nécessairement que :

\[
\forall z \in r, \; a'(z)\geq 1
\]

En outre, par définition du degré et en regardant ce que donnerait le développement de cette forme factorisée, un tel produit ne peut contenir plus de $\deg P$ termes. Il y a donc au plus $\deg P$ racines, ce qui s'écrit $\#r < \deg P$. $r$ étant par conséquent finie et dénombrable, on aura :

\[\boxed{
P(x) = \prod_{i=1}^{\#r} (x-r_i)^{a'_i}
}\]

Au sens usuel du produit.

Le problême de la recherche des $a'$ par les $a$ étant homologue à la factorisation en facteurs premiers d'un nombre. On comprend alors que nombres premiers, polynômes et nombres complexes partagent un même liant très profond, au coeur - entre autres - de l'hypothèse de Riemann.

\textcolor{red}{(Factorisation des polynômes d'ordre 2)}

\textcolor{red}{(théorie de Galois)}

\subsection{Fractions rationnelles}

Une fraction rationnelle est le rapport de deux polynômes :

\[\boxed{
\frac{P}{Q}(x) = \frac{\sum_{i=0}^\infty a_i x^i}{\sum_{i=0}^\infty b_i x^i}
}\]

Tout comme nous pouvons simplifier les fractions ne gardant que les facteurs premiers, nous pouvons faire de même avec les fractions rationnelles :

\[\boxed{
\frac{P}{Q}(x) = \prod_{i=1}^\infty\frac{(x-r_i)^{a'_i}}{(x-p_i)^{b'_i}}
}\]

En supposant les suites $a$ et $b$ nulles à partir des rangs $\#r$ et $\#p$. Par exemple :

\[
\frac{x^3-10x^2+23x-14}{x^4-11x^3+38x^2-52x+24} = \frac{(x-7)(x-2)(x-1)}{(x-6)(x-2)^2(x-1)}
\]

Qu'on simplifie en :

\[
\frac{x^3-10x^2+23x-14}{x^4-11x^3+38x^2-52x+24} = \frac{x-7}{(x-6)(x-2)}
\]

\[
\frac{x^3-10x^2+23x-14}{x^4-11x^3+38x^2-52x+24} = \frac{x-7}{x^2-8x+12}
\]

Cet exemple est l'exact analogue de celui proposé pour les fractions simples, $17$ étant le septième nombre premier, $3$ le second, etc.

Il y a en fait injection \textcolor{red}{(pas parlé avant)} de $\mathbb{N}$ dans $\mathbb{K}[x]$, et ce même si $x^3-10x^2+23x-14$ semble au premier abord n'avoir pas grand chose à voir avec $120$.

Mais cette arithmétique des polynômes ne s'applique en l'état qu'a une classe extrêmement particulière de polynômes : ceux dont les racines sont des nombres premiers. Or, si l'on peut toujours factoriser un polynôme, comme nous le verrons dans bien des exemples ses racines pourront apparaître n'importe où dans $\mathbb{C}$.

\subsection{Décomposition en éléments simples}

Ce que nous allons aborder dans cette section n'a pas vraiment d'équivalent dans $\mathbb{N}$. On pourrait lui en trouver un en cherchant bien, mais il serait sans intéret (en tous cas je ne lui en vois pas).

Si $\deg P < \deg Q$ (sinon, une division euclidienne permet de faire apparaître un reste pour lequel c'est le cas), il existe une autre façon d'écrire :

\[
\frac{P}{Q}(x) = \prod_{i=1}^\infty\frac{(x-r_i)^{a'_i}}{(x-p_i)^{b'_i}}
\]

Qui est :

\[\boxed{
\frac{P}{Q}(x) = \sum_{i=1}^{\#s}  \left(\sum_{j=1}^{b_i} \frac{A_{ij}}{(x-p_i)^j}\right)
}\]

\textcolor{red}{(prouver l'unicité)}

Où les $A_{ij}$ constituent une suite bidimensionnelle unique pour un polynôme donné. La procédure permettant de les calculer (qui nous suffira comme preuve de l'existence de la décomposition) est la suivante :

\begin{itemize}
\item On choisi un pôle $p_k$ d'ordre de multiplicité $b'_k$.
\item On multiplie les deux formes par $(x-p_k)^{b'_k}$ :

\[
(x-p_k)^{b'_k} \prod_{i=1}^\infty\frac{(x-r_i)^{a'_i}}{(x-p_i)^{b'_i}}
= (x-p_k)^{b'_k} \sum_{i=1}^{\#s}  \left(\sum_{j=1}^{b_i} \frac{A_{ij}}{(x-p_i)^j}\right)
\]

A gauche, le pôle $p_k$ est \emph{éliminé}. A droite, Il est placé en facteur de tous les termes sauf de celui où le dénominateur est égal au terme multiplicatif :

\[
\frac{\prod_{i=1}^\infty (x-r_i)^{a'_i}}{\prod_{i\in \mathbb{N} \setminus \{k\}} (x-p_i)^{b'_i}}
= A_{kb'_k} + \sum_{i\neq k \wedge j \neq b'_k} \frac{A_{ij}}{(x-p_i)^j}(x-p_k)^{b'_k}
\]

\item Puisque cette expression est sensée être valable $\forall x \in \mathbb{C}$, on choisi $x=p_k$ de sorte que :

\[
\frac{\prod_{i=1}^\infty (p_k-r_i)^{a'_i}}{\prod_{i\in \mathbb{N} \setminus \{k\}} (p_k-p_i)^{b'_i}}
= A_{kb'_k} + \sum_{i\neq k \wedge j \neq b'_k} \frac{A_{ij}}{(p_k-p_i)^j}(p_k-p_k)^{b'_k}
\]

\[
\frac{\prod_{i=1}^\infty (p_k-r_i)^{a'_i}}{\prod_{i\in \mathbb{N} \setminus \{k\}} (p_k-p_i)^{b'_i}}
= A_{kb'_k}
\]

\item On recommence avec $\frac{P}{Q}(x)$ soustrait de l'élément simple qu'on vient de calculer (l'unicité de la décomposition assure que les $A_{ij}$ qui restent à calculer sont inchangés pour la nouvelle fraction rationnelle ainsi construite) :

\[
\prod_{i=1}^\infty\frac{(x-r_i)^{a'_i}}{(x-p_i)^{b'_i}}
- \frac{A_{kb'_k}}{(x-p_k)^{b'_k}}
= \sum_{i=1}^{\#s}  \left(\sum_{j=1}^{b_i} \frac{A_{ij}}{(x-p_i)^j}\right)
- \frac{A_{kb'_k}}{(x-p_k)^{b'_k}}
\]

\[
\frac{\prod_{i=1}^\infty(x-r_i)^{a'_i} - A_{kb'_k}\prod_{i\neq k}(x-p_i)^{b'_i}}{\prod_{i=1}^\infty(x-p_i)^{b'_i}}
= \sum_{i\neq k}  \left(\sum_{j=1}^{b_i} \frac{A_{ij}}{(x-p_i)^j}\right)
+ \sum_{j=1}^{b'_k-1} \frac{A_{kj}}{(x-p_k)^{j}}
\]

Qui a perdu un ordre de multiplicité sur son pôle $p_k$.

\end{itemize}

Vous trouverez un exemple de cette procédure appliquée à la transformée en $z$ dans le chapitre de traitement du signal \textcolor{red}{(pas fait)}

Pour une fraction rationnelle à valeurs dans $\mathbb{R}$, si $p_k$ est un pôle irréel simple ($p_k \in \mathbb{C}/\mathbb{R} \wedge b'_k = 1$) alors son conjugé $p_k^*$ l'est également de façon à ce qu'on puisse alors réunir ces pôles décomposés en un seul :

\[
\frac{A_{k1}}{x-p_k} + \frac{A_{k'1}}{x-p_k^*} 
= \frac{A_{k1}(x-p_k^*) + A_{k'1}(x-p_k)}{(x-p_k)(x-p_k^*)}
\]

\[
\frac{A_{k1}}{x-p_k} + \frac{A_{k'1}}{x-p_k^*} 
= \frac{(A_{k1}+A_{k'1})x-(A_{k1}p_k^* + A_{k'1}p_k)}{x^2+(2\Re p_k) x + |p_k|^2}
\]

On remarque à l'occasion que $A_{k1}$ et $A_{k'1}$ sont également conjugés pour que la fraction soit à valeur dans  $\mathbb{R}$.

La recherche du numérateur se fera donc en écrivant :

\[
\frac{A_{k1}}{x-p_k} + \frac{A_{k'1}}{x-p_k^*} 
= \frac{B_{k1}x-C_{k1}}{x^2+(2\Re p_k) x + |p_k|^2}
\]

Dès qu'une paire de pôles complexes se présentera, donnant lieu à une recherche de $B_{k1}$ et $C_{k1}$ sur la droite réelle selon la même procédure que précédemment.

\textcolor{red}{(quid de quand les pôle sont complexes mais pas simples ?)}

\section{Théorie de l'exponentielle}

\section{Fonctions trigonométriques}

\section{Analyse complexe}

\textcolor{red}{(+ théorème des résidus)}

\chapter{Algèbre linéaire}

\chapter{Espaces fonctionnels}

\section{Théorie des distributions}

\subsection{Structure algébrique}

Les \emph{espaces fonctionnels} visent à généraliser les notions d'algèbre linéaire aux ensembles \emph{de dimension en bijection avec $\mathbb{R}$}.

Plus simplement, les \emph{distributions} qui forment une base de $E$ sont en nombre infini non-dénombrable.

On défini le produit scalaire sur $E$ par :

\[\boxed{
\braket{f|g} = \int_\mathbb{R} f^*(x)g(x) dx
}\]

Et la norme associée :

\[\boxed{
||f||^2 = \braket{f|f} = \int_\mathbb{R} |f(x)|^2 dx
}\]

On dira que $f$ est normée si $||f|| = 1$.

Commençons par remarquer que :

\[\boxed{
\braket{f|g} = \braket{g|f}^* = \braket{g^*|f^*}
}\]

Les notations utilisées (dites \emph{Bra-Ket}, inventées par Dirac pour les besoins de la physique quantique) diffèrent de ce qui se fait en algèbre linéaire classique, mais je vous invite à chercher l'analogie à chaque formule pour voir que les concepts sont simplement transposés d'une théorie à l'autre.

On se place toujours dans le cas où $E$ est un $\mathbb{C}-$algèbre fonctionnel.

On notera que même si la notion de distribution semble correpondre à la notion de fonction, il n'en est rien car $f$ ou $g$ n'ont pas forcément besoin d'avoir une valeur en tout point pour que le produit scalaire puisse être défini. 

Par exemple, considérons la \emph{distribution de Dirac} $\delta$ nulle sur $\mathbb{R} / {0}$ et d'intégrale $\int_\mathbb{R} \delta(x) dx = 1$. Alors, on a rien dit sur sa valeur en $0$ et on sent bien que celle-ci doit-être infinie ce qui fait que $\delta$ ne peut-être une fonction au sens qu'on leur a donné \textcolor{red}{(pas fait)}. Pourtant :

\[
\braket{\delta|f} = \int_\mathbb{R} \delta^*(x)f(x) dx
\]

La conjugaison peut-être levée car $\delta$ est réelle :

\[
\braket{\delta|f} = \int_\mathbb{R} \delta(x)f(x) dx
\]

Le découpage naturel à effectuer est :

\[
\braket{\delta|f} = \int_{\mathbb{R}/0} \delta(x)f(x) dx + \delta(0)f(0) dx
\]

Par la définition de $\delta$, le premier terme est nul et $\delta(0) dx = \int_\mathbb{R} \delta(x) dx = 1$ :

\[
\braket{\delta|f} = f(0)
\]

On voit donc que $\delta$ n'a pas besoin d'être une fonction pour qu'on puisse affecter une valeur à $\braket{\delta|f}$. La théorie des distributions donne exactement les hypothèses à satisfaire sur $f$ et $g$ pour que $\braket{\delta|f}$ soit défini, mais je préfère laisser ces considérations aux mathématiciens.

L'ensemble des translations de la distribution de Dirac est le pendant distributoire de la base canonique. C'est à dire qu'en construisant une \emph{base fonctionnelle} :

\[\boxed{
\delta_\tau : (x,\xi) \rightarrow \delta(x-\xi)
}\]

On trouve que pour toute distribution f :

\[
\braket{\delta_\tau(\xi)|f} = f(\xi)
\]

Ou autrement écrit :

\[\boxed{
\braket{\delta(x-\xi)|f(x)} = f(\xi)
}\]

Autrement dit, la projection d'une distribution $f$ sur la base des $\delta_\tau(\xi)$ coïncide avec l'idée de \emph{valeur attribuée à une fonction}.

L'idée étant que connaître l'ensemble des produits scalaires d'une distribution $f : x \rightarrow y$ dans une base $\phi : (x,\xi) \rightarrow y$ revient à connaître $f$ elle-même. Ainsi, on n'a pas besoin de donner la valeur de $\delta(0)$ pour manipuler $\delta$.

Nous allons nous baser sur cette idée pour établir un certain nombre de résultats plus généraux. Par exemple si l'on connait les décompositions canoniques de $f$ et $g$, leur produit scalaire est donné par :

\[
\braket{f|g} = \int_\mathbb{R} f^*(\xi)g(\xi) d\xi = \int_\mathbb{R} \braket{f(x)|\delta(x-\xi)}\braket{\delta(x-\xi)|g(x)} d\xi
\]

De la même façon, on peut invoquer n'importe quelle base $\phi(\xi)$ et montrer que :

\[
\int_\mathbb{R} \braket{f|\phi(\xi)}\braket{\phi(\xi)|g} d\xi = \int_\mathbb{R} \left( \int_\mathbb{R} f^*(x)\phi(\xi,x) dx \int_\mathbb{R} \phi^*(\xi,y)g(y) \; dy \right) d\xi
\]

\[
\int_\mathbb{R} \braket{f|\phi(\xi)}\braket{\phi(\xi)|g} d\xi = \iiint_\mathbb{R^3} f^*(x)\phi(\xi,x) \phi^*(\xi,y)g(y) \;dx \; dy \; d\xi
\]

\[\boxed{
\braket{f|g} = \int_\mathbb{R} \braket{f|\phi(\xi)}\braket{\phi(\xi)|g} d\xi
}\]

Ce résultat qui peut sembler abstrait, est en fait ce qui fait en grande partie le mystère de la \emph{mécanique quantique}, où il prend le nom de \emph{principe de superposition}. Il est cependant tout à fait essentiel également en physique classique, comme nous allons le voir maintenant. Notamment, il permet de considérer des décompositions de distributions. Posons $g(x)=\delta(x-\rho)$ :

\[
\braket{f(x)|\delta(x-\rho)} = \int_\mathbb{R} \braket{f(x)|\phi(\xi,x)}\braket{\phi(\xi,x)|\delta(x-\rho)} d\xi
\]

\[
f^*(\rho) = \int_\mathbb{R} \braket{f(x)|\phi(\xi,x)}\phi^*(\xi,\rho) d\xi
\]

On pourra utiliser la notation :

\[\boxed{
\bra{f} = \int_\mathbb{R} \braket{f|\phi(\xi)}\bra{\phi(\xi)} d\xi
}\]

Et de la même façon :

\[\boxed{
\ket{g} = \int_\mathbb{R} \braket{\phi(\xi)|g}\ket{\phi(\xi)} d\xi
}\]

Qui donnent enfin une écriture permettant de perçevoir idéalement l'analogie avec l'algèbre linéaire développée précédemment \textcolor{red}{(pas fait)}. Sous cette forme, $f$ est décomposée dans la base $\phi$ de façon manifeste, comme peut l'être un vecteur $\vec{v}$ dans une base $\vec{e}$.

L'idée essentielle est donc que connaître l'ensemble des $\braket{f|\phi(\xi)}$ revient à connaître complètement $\bra{f}$, et de même pour $\braket{\phi(\xi)|g}$ et $\ket{g}$

\textcolor{red}{(quid de $\ket{\phi(\xi)}\bra{\phi(\xi)}$ ?)}

\subsection{Dérivation des distributions}

\textcolor{red}{(démonstration à partir de l'intégration par parties)}

On trouve :

\[\boxed{
\braket{\frac{d f}{d x}|g} = - \braket{f|\frac{d g}{d x}}
}\]

\subsection{Produit de convolution}

Le produit de convolution de deux distributions $f$ et $g$ est donné par :

\[\boxed{
f * g : x \rightarrow \int_\mathbb{R} f(x-t)g(t) dt
}\]

On se convaincra facilement que ce produit est commutatif.

\[\boxed{
\forall (f,g)\in \mathbb{L}^2, \; f * g = g* f
}\]

Par ailleurs, la distribution de Dirac est l'identité de ce produit :

\[
(f * \delta)(x) = \int_\mathbb{R} \delta(x-t)f(t) dt
\]

\[
(f * \delta)(x) = f(x) \delta(0) dx
\]

\[\boxed{
f * \delta = f
}\]

Nous aurons l'occasion d'aborder de multiples applications du produit de convolution.

Un autre résultat important est que le théorème de Fubini \textcolor{red}{(qu'est-ce ?)} implique :

\[\boxed{
\int_\mathbb{R} f * g = \int_\mathbb{R} f \times \int_\mathbb{R} g
}\]

Enfin, on montre trivialement que :

\[\boxed{
\frac{d (f * g)}{d x} = \frac{d f}{d x} * g = f * \frac{d g}{d x}
}\]

Qui n'est pas vraiment un comportement attendu pour un produit ; et pourtant c'est bien celui du produit de convolution.

\subsection{Inégalité de Cauchy-Schwarz}

De façon totalement équivoque à ce qui se fait en algèbre linéaire \textcolor{red}{(pas fait)}, on a :

\[\boxed{
\braket{f|g}\leq ||f||\times||g||
}\]

Qu'on explicitera en :

\[
\int_\mathbb{R} f\times g \leq \sqrt{\int_\mathbb{R} |f|^2 \times \int_\mathbb{R} |g|^2}
\]

Le fait est qu'en physique, l'\emph{énergie} est donnée par le produit scalaire de deux valeurs couplées. On s'interroge alors sur le sens à donner au produit des normes. Et bien par exemple, en électricité, la \emph{puissance moyenne} est :

\[
\braket{P} = \frac{\braket{U|I}}{\aleph_1}
\]

\[
\braket{P} = \frac{1}{\aleph_1}\int_\mathbb{R} U\times I
\]

Pour une tension et un courant cosinusoïdaux et déphasés d'un angle $\varphi$, la puissance moyenne s'écrira donc :

\[
\braket{P} = \frac{1}{2\pi}\int_0^{2\pi} U_0 \cos(\alpha)I_0\cos(\alpha+\varphi) \; d\alpha
\]

Les grandeurs $U$ et $I$ étant réelles. Par le théorème trigonométrique sur les produits de cosinus \textcolor{red}{(pas fait)} :

\[
\braket{P} = \frac{U_0 I_0}{4\pi} \int_0^{2\pi} \cos(\varphi) + \cos(2\alpha+\varphi) \; d\alpha
\]

\[
\braket{P} = \frac{U_0 I_0}{4\pi} \left(\int_0^{2\pi} \cos(\varphi) \; d\alpha + \int_0^{2\pi} \cos(2\alpha+\varphi) \; d\alpha \right)
\]

\[
\braket{P} = \frac{U_0 I_0}{4\pi} \left(2\pi \cos(\varphi) + \frac{\sin(4\pi+\varphi)}{2} - \frac{\sin(0+\varphi)}{2} \right)
\]

Et par la $2\pi-$périodicité du sinus :

\[
\braket{P} = \frac{U_0 I_0}{2} \cos(\varphi)
\]

Alors que le produit des normes s'écrit :

\[
S = ||U||\times||I||
\]
\[
S = \sqrt{\int_\mathbb{R} U \times \int_\mathbb{R} I}
\]

\[
S = \frac{1}{2\pi}\sqrt{\int_0^{2\pi} U_0^2 \cos^2(\alpha) \; d\alpha \times \int_0^{2\pi} I_0^2\cos^2(\alpha+\varphi) \; d\alpha}
\]

\[
S = \frac{U_0 I_0}{2\pi}\sqrt{\int_0^{2\pi}  \cos^2(\alpha) \; d\alpha \times \int_0^{2\pi} \cos^2(\alpha+\varphi) \; d\alpha}
\]

Par le même théorème trigonométrique :

\[
S = \frac{U_0 I_0}{2\pi}\sqrt{\int_0^{2\pi}  \frac{\cos(2\alpha)+1}{2} \; d\alpha \times \int_0^{2\pi} \frac{\cos(2(\alpha+\varphi))+1}{2} \; d\alpha}
\]

\[
S = \frac{U_0 I_0}{2\sqrt{4}\pi}\sqrt{\left(\int_0^{2\pi} \cos(2\alpha)\; d\alpha+\int_0^{2\pi} \; d\alpha \right)\times \left(\int_0^{2\pi} \cos(2(\alpha+\varphi))\; d\alpha+ \int_0^{2\pi} d\alpha\right)}
\]

Par la $2\pi-$périodicité on peut éluder les cosinus :

\[
S = \frac{U_0 I_0}{4\pi}\sqrt{\int_0^{2\pi} \; d\alpha \times \int_0^{2\pi} d\alpha}
\]

\[
S = \frac{U_0 I_0}{2}
\]

En fait, comme à aucun moment le produit central n'a été distribué et que le problême initial est symétrique, on a directement les normes :

\[
\frac{||U||}{\aleph_1} = \frac{U_0}{\sqrt{2}}, \; \frac{||I||}{\aleph_1} = \frac{I_0}{\sqrt{2}}
\]

Qui - comme nous le verrons - prennent le nom de \emph{valeurs efficaces}.

L'inégalité de Cauchy-Schwartz s'écrit :

\[
\braket{U|I}\leq ||U||\times||I||
\]

\[
\frac{U_0 I_0}{2} \cos(\varphi)\leq \frac{U_0 I_0}{2}
\]

Ce qui semble tout à fait raisonnable.

On peut se demander ce que cela représente physiquement, et nous aurons l'occasion d'en reparler à l'avenir. Pour le moment, contentons-nous juste de dire que si le produit scalaire est la \emph{puissance active} - qui est la puissance transitant continuement dans le système - le produit des norme est la \emph{puissance apparente} et le défaut au sens de Fresnel \textcolor{red}{(pas parlé avant)} est la \emph{puissance réactive} notée $Q$ :

\[
Q = \sqrt{S^2-\braket{P}^2} = \sqrt{\left(\frac{||U||}{\aleph_1}\right)^2\times\left(\frac{||I||}{\aleph_1}\right)^2-\left(\frac{\braket{U|I}}{\aleph_1}\right)^2}
\]

Dans notre cas précis :

\[
Q = \sqrt{\left(\frac{U_0 I_0}{2}\right)^2-\left(\frac{U_0 I_0}{2}\cos(\varphi)\right)^2}
\]

\[
Q = \frac{U_0 I_0}{2}\sqrt{1-\cos^2(\varphi)}
\]

Et par un autre théorème trigonométrique :

\[
Q = \frac{U_0 I_0}{2}\sin(\varphi)
\]

\section{Opérateurs}

\subsection{Structure algébrique}

\textcolor{red}{(compléter)}

\subsection{Théorie spectrale}

\textcolor{red}{(compléter)}

\section{Transformation de Fourier}

On s'intéresse dans cette section à un espace fonctionnel où cohabitent deux bases souvent pratiques à l'emploi dans des contextes particuliers : la première étant la base canonique composée des distributions de Dirac translatées, et la seconde étant définie par :

\[
(x,k) : \rightarrow e^{jk x}
\]

Le produit scalaire entre une distribution $f$ et un élément de cette base est donné par :

\[\boxed{
\braket{e^{jk x}|f} = \int_\mathbb{R} e^{-jk x} f(x) dx
}\]

Et la fonction qui a tout $k$ associe ce produit scalaire est appellée \emph{transformée de Fourier} de $f$ :

\[\boxed{
\mathcal{F}(f) : k \rightarrow \braket{e^{jk x}|f(x)}
}\]

Cet outil extrêmement répandu trouve des applications dans toute la physique et l'ingénierie. Ceci s'explique par sa capacité à transformer les \emph{équations différentielles} linéaires (que nous traiterons ultérieurement) en équations algébriques, du fait que :

\[
\braket{e^{jk x}|\frac{\partial f}{\partial x}(x)} = -\braket{\frac{\partial e^{jk x}}{\partial x}|f(x)}
\]

\[
\braket{e^{jk x}|\frac{\partial f}{\partial x}(x)} = -\braket{jke^{jk x}|f(x)}
\]

Et parce-que le Bra est conjugé dans la définition du produit scalaire :

\[
\braket{e^{jk x}|\frac{\partial f}{\partial x}(x)} = jk\braket{e^{jk x}|f(x)}
\]

\[\boxed{
\mathcal{F}\left(\frac{\partial f}{\partial x}\right) = jk\mathcal{F}(f)
}\]

Dans l'espace de Fourier, dériver revient donc à multiplier par $jk$.

Plus formellement, la base constituée des $e^{jk x}$ est la base propre de l'\emph{opérateur dérivée}, de valeurs propres respectives $jk$. Cette manière de penser est celle qu'on adopte en physique quantique. En effet, l'opérateur \emph{quantité de mouvement} correspond à une dérivée spatiale, et si l'on est capable d'écrire une quelconque fonction d'onde $f$ (peut importe qu'elle soit un Bra ou un Ket à ce stade) comme une superposition d'exponentielles :

\[
f(x) = \int_\mathbb{R} \braket{e^{jk x'}|f(x')}e^{jk x} dk
\]

Alors on sera capable de calculer la quantité de mouvement car la dérivée sera donnée par :

\[
\frac{\partial f}{\partial x}(x) = \int_\mathbb{R} \braket{e^{jk x'}|f(x')}\frac{\partial e^{jk x}}{\partial x} dk
\]

\[
\frac{\partial f}{\partial x}(x) = \int_\mathbb{R} jk \braket{e^{jk x'}|f(x')}e^{jk x} dk
\]

Le fait qu'on ne puisse pas sortir $jk$ de l'intégrale correspond au phénomène d'interférence.

Puisque $f$ est originellement fonction de l'espace, nous n'avons fait qu'effectuer un changement de base de celle des Diracs vers celle des exponentielles. Autrement dit, cette manipulation à demandé de calculer l'ensemble des $\braket{e^{jk x'}|f(x')}$, qui se trouve être la transformée de Fourier.

Nous avons alors pû en déduire les coefficients de $\frac{\partial f}{\partial x}$ dans cette même base, ce qui constitue une étape de la résolution de l'\emph{équation de Schrodinger}.

\subsubsection{Inversion de la transformée de Fourier}

Une fois l'équation résolue, on se retrouve de nouveau avec une fonction d'onde décomposée dans la base des exponentielles, autrement dit la connaissance de tous les $\tilde{g}(k) = \braket{e^{jk x}|g(x)}$. Pour repasser dans celle des Diracs, on doit donc calculer cette intégrale :

\[
g(x) = \int_\mathbb{R} \tilde{g}(k) e^{jk x} dk
\]

Qui se trouve correspondre au produit scalaire :

\[
g(k) = \braket{\tilde{g}^*(k)|e^{jk x}}
\]

C'est donc tout naturellement qu'on définira la \emph{transformée de Fourier inverse} :

\[\boxed{
\mathcal{F}^{-1}(\tilde{g}) : x \rightarrow \braket{e^{-jk x}|\tilde{g}(k)}
}\]

Avec évidemment :

\[
\mathcal{F}^{-1}(\mathcal{F}(f))(x) = \braket{e^{-jk x}|\braket{e^{jk x'}|f(x')}}
\]

\[
\mathcal{F}^{-1}(\mathcal{F}(f))(x) = \int_\mathbb{R}e^{jk x}\int_\mathbb{R} e^{-jk x'} f(x') \; dx' \; dk
\]

\[
\mathcal{F}^{-1}(\mathcal{F}(f))(x) = \int_\mathbb{R}\int_\mathbb{R} e^{jk (x-x')} \; dk \; f(x') \; dx' 
\]

\textcolor{red}{(trouver une démonstration de $\braket{e^{jkx}|1} = \delta(k)$)}

\[
\mathcal{F}^{-1}(\mathcal{F}(f))(x) = \int_\mathbb{R} \delta(x-x') \; f(x') \; dx' 
\]

\[\boxed{
\mathcal{F}^{-1}(\mathcal{F}(f))(x) =f(x)
}\]

\subsubsection{Transformée de Fourier et produit de convolution}

Un autre résultat remarquable est que la transformation de Fourier transforme le produit de convolution en produit scalaire :

\[
\braket{e^{jk x}|(f * g)(x)} = \int_\mathbb{R} e^{-jk x} \int_\mathbb{R} f(x-t)g(t) dt \; dx
\]

\[
\braket{e^{jk x}|(f * g)(x)} = \iint_{\mathbb{R}^2} e^{-jk x} f(x-t)g(t) dx \; dt
\]

Par un changement de variable :

\[
\braket{e^{jk x}|(f * g)(x)} = \iint_{\mathbb{R}^2} e^{-jk (y+u)} f(y)g(u) dy \; du
\]

\[
\braket{e^{jk x}|(f * g)(x)} = \int_\mathbb{R} e^{-jky} f(y) dy \; \int_\mathbb{R} e^{-jku}g(u) du
\]

\[
\braket{e^{jk x}|(f * g)(x)} = \braket{e^{jk x}|f}\braket{e^{jk x}|g}
\]

\[\boxed{
\mathcal{F}(f * g) = \mathcal{F}(f)\mathcal{F}(g)
}\]

Par ailleurs on peut effectuer une démonstration analogue :

\[
\braket{e^{jk x}|f(x)g(x)} = \int_\mathbb{R} e^{-jk x} f(x)g(x) dx
\]

\textcolor{red}{(compléter)}

Pour aboutir à la relation :

\[\boxed{
\mathcal{F}(fg) = \mathcal{F}(f)*\mathcal{F}(g)
}\]

\subsubsection{Corrélations}

On s'intéresse à la transformée de Fourier du produit d'une fonction $f$ par le conjugé d'une fonction $g$, ce qui correspond à une puissance en physique (on accède donc ainsi à la \emph{densité spectrale de puissance}) :

\[\boxed{
\mathcal{F}(fg^*)(k) = \int_\mathbb{R} e^{-jk x} f(x)g^*(x) dx
}\]

D'après ce qui précède immédiatement :

\[
\mathcal{F}(fg^*)(k) = \mathcal{F}(f) * \mathcal{F}(g^*)
\]

En notation Braket :

\[
\mathcal{F}(fg^*)(k) = \braket{e^{jk x}|f(x)} * \braket{e^{jk x}|g^*(x)}
\]

\[
\mathcal{F}(fg^*)(k) = \braket{e^{jk x}|f(x)} * \braket{e^{j(-k) (-x)}|g^*(x)}
\]

\[
\mathcal{F}(fg^*)(k) = \braket{e^{jk x}|f(x)} * \braket{e^{j(-k) x'}|g^*(-x')}
\]

\[\boxed{
\mathcal{F}(fg^*)(k) = \mathcal{F}(f)(k) * \mathcal{F}(g^*_-)(-k)
}\]

Avec $g_-(x) = g(-x)$. Si l'on veut expliciter ce résultat :

\[
\mathcal{F}(fg^*)(k) = \int_\mathbb{R} \mathcal{F}(f)(k-k') \mathcal{F}(g_-^*)(-k') dk'
\]

\[\boxed{
\mathcal{F}(fg^*)(k) = \int_\mathbb{R} \mathcal{F}(f)(k+k'') \mathcal{F}(g_-^*)(k'') dk''
}\]

\textcolor{red}{(inexact, à corriger)}

\subsubsection{Relation d'incertitude d'Heisenberg}

\textcolor{red}{(compléter)}

\subsubsection{\textcolor{red}{Paragraphe à supprimer}}

Du fait de la relation $\mathcal{F}(f * g) = \mathcal{F}(f)\mathcal{F}(g)$ établie précédemment, et parce-que l'identité du produit de convolution est $\delta$ :

\[
\braket{\delta|e^{jk x}} = \braket{\delta|e^{jk x}}\times 1
\]

\[
\braket{\delta|e^{jk x}} = \frac{\braket{\delta|e^{jk x}}\braket{\delta|e^{jk x}}}{\braket{\delta|e^{jk x}}}
\]

\[
\braket{\delta|e^{jk x}} = \frac{\braket{\delta * \delta|e^{jk x}}}{\braket{\delta|e^{jk x}}}
\]

\[
\braket{\delta|e^{jk x}} = \frac{\braket{\delta|e^{jk x}}}{\braket{\delta|e^{jk x}}}
\]

\[
\braket{\delta|e^{jk x}} = 1
\]

\textcolor{red}{(trouver une démonstration de $\braket{1|e^{jkx}} = \delta(k)$)}

L'opérateur inverse à la transformée de Fourier est donné par :

\[
\mathcal{F}^{-1}(f) : x \rightarrow \braket{f|e^{-jk x}}
\]

Du fait que :

\[
\mathcal{F}^{-1}(\mathcal{F}(f))(x') = \int_{\mathbb{R}}\int_{\mathbb{R}} f(x) e^{-jkx} \; dx \; e^{jkx'} \; dk
\]

\[
\mathcal{F}^{-1}(\mathcal{F}(f))(x') = \int_{\mathbb{R}} f(x) \int_{\mathbb{R}} e^{-jk(x-x')} \; dk \; dx
\]

\[
\mathcal{F}^{-1}(\mathcal{F}(f))(x') = \int_{\mathbb{R}} f(x) \braket{1|e^{jk(x-x')}} \; dx
\]

Qui, comme nous l'avons vu (la variable du produit scalaire est $k$), revient à :

\[
\mathcal{F}^{-1}(\mathcal{F}(f))(x') = \int_{\mathbb{R}} f(x) \delta(x-x') \; dx
\]

\[
\mathcal{F}^{-1}(\mathcal{F}(f))(x') = f(x')
\]

\[
\mathcal{F}^{-1}(\mathcal{F}(f)) = f
\]

\subsection{Séries de Fourier}

Une distribution $f$ est dite \emph{$\lambda-$périodique} si :

\[\boxed{
\forall x, \; \braket{\delta(x-x')|f(x')} = \braket{\delta(x + \lambda-x')|f(x')}
}\]

Nous allons montrer que sa transformée de Fourier est un \emph{peigne} (valeurs non-nulles isolées et régulièrement espacées). Pour cela, commençons par introduire le \emph{peigne de Dirac normé} :

\[\boxed{
\mathrm{III}_\lambda = \frac{1}{\aleph_0}\sum_{i\in \mathrm{Z}}\delta(x-\lambda i)
}\]

La division par $\aleph_0$ est nécessaire pour garantir une norme unitaire.

La transformée de Fourier du peigne de Dirac est la somme de celle de chacun des Dirac translaté :

\[
\mathcal{F}(\mathrm{III}_\lambda)(k) = \int_\mathbb{R} \frac{1}{\aleph_0}\sum_{i\in \mathrm{Z}}e^{-jkx}\delta(x-i\lambda) dx 
\]

\[
\mathcal{F}(\mathrm{III}_\lambda)(k) = \frac{1}{\aleph_0}\sum_{i\in \mathrm{Z}}\braket{e^{jkx}|\delta(x-i\lambda)} 
\]

\[
\mathcal{F}(\mathrm{III}_\lambda)(k) = \frac{1}{\aleph_0}\sum_{i\in \mathrm{Z}}e^{-jki\lambda} 
\]

\textcolor{red}{(..., askip la suite de la démonstration n'est pas simple)}

\[\boxed{
\mathcal{F}(\mathrm{III}_\lambda)(k) = \frac{1}{\lambda}\mathrm{III}_{\frac{2\pi}{\lambda}}
}\]

Regardons ce que donne la convolution de $\mathrm{III}_\lambda$ par $f$ :

\[
\left(\mathrm{III}_\lambda * f\right)(x) = \int_\mathbb{R} f(x-t)\frac{1}{\aleph_0}\sum_{i\in \mathrm{Z}}\delta\left(t-\lambda i\right) \; dt
\]

\[
\left(\mathrm{III}_\lambda * f\right)(x) = \frac{1}{\aleph_0}\sum_{i\in \mathrm{Z}} \int_\mathbb{R} f(x-t)\delta\left(t-\lambda i\right) \; dt
\]

\[
\left(\mathrm{III}_\lambda * f\right)(x) = \frac{1}{\aleph_0}\sum_{i\in \mathrm{Z}} f\left(x-\lambda i\right) \; dt
\]

Par la $\lambda-$périodicité de $f$ :

\[
\left(\mathrm{III}_\lambda * f\right)(x) = \frac{1}{\aleph_0}\sum_{i\in \mathrm{Z}} f(x)
\]

Plus de dépendance en $i$ ! La somme s'annule avec $\aleph_0$. On en conclu que :

\[\boxed{
\mathrm{III}_\lambda * f = f
}\]

Autrement dit, $\mathrm{III}_\lambda$ est un élément neutre pour la convolution si $f$ est $\lambda-$périodique.

Si on prend la transformée de Fourier de cette relation :

\[
\mathcal{F}(f*\mathrm{III}_\lambda) = \mathcal{F}(f)
\]

On se doute que la prochaine ligne de ce calcul est :

\[
\mathcal{F}(f)\mathcal{F}(\mathrm{III}_\lambda) = \mathcal{F}(f)
\]

Et selon un résultat précédent :

\[
\frac{1}{\lambda}\mathcal{F}(f)\mathrm{III}_\frac{2\pi}{\lambda} = \mathcal{F}(f)
\]

Résultat qui a cela d'intéressant qu'il permet de conclure que partout où le peigne de Dirac est nul, $f$ l'est également. Autrement dit, $\mathcal{F}(f)$ ne peut-être qu'un peigne lui-même ; et chaque pointe de ce peigne est ce que nous appellerons un \emph{harmonique}. Les pointes sont espacées d'un intervalle $\frac{2\pi}{\lambda}$ qui est également le nombre d'onde $k_1$ du premier harmonique.

Une distribution quelconque peut se décomposer dans l'espace de Fourier comme :

\[
f = \int_\mathbb{R} \braket{e^{jk x}|f}e^{jk x} dk
\]

C'est pourquoi on peut affirmer qu'une fonction périodique peut s'écrire :

\[\boxed{
f = \sum_{n \in \mathrm{Z}} \braket{e^{jk_n x}|f}e^{jk_n x}
}\]

Les $k_n$ étant multiples de $k_1 = \frac{2\pi}{\lambda}$.

Avec pour expression générale des coefficients de Fourier (pour rappel, la taille des pointes du peigne) :

\[
\braket{e^{jk x}|f} = \int_\mathbb{R} e^{-jk x} f(x) dx
\]

La fonction étant $\lambda-$périodique, il vient immédiatement :

\[\boxed{
\mathcal{F}(f)(k_n) = \frac{1}{\lambda} \int_{-\frac{\lambda}{2}}^{\frac{\lambda}{2}} e^{-j k_n x } f(x) dx
}\]

\textcolor{red}{(décomposer $f$ en cosinus et sinus pour une fonction à valeurs réelles)}

\section{Transformée de Laplace}

La transformée de Fourier est un outil puissant pour étudier des fonctions périodiques, mais elle fonctionne mal quand il s'agit de s'intéresser à des phénomènes transitoires. En effet, si l'on prend par exemple la fonction d'Heaviside :

\[\boxed{
\left\{ \begin{matrix}
\forall x<0, H : x \rightarrow 0 \\
\forall x>0, H : x \rightarrow 1
\end{matrix} \right.
}\]

Alors la transformée de Fourier de cette fonction semble difficile à écrire :

\[
\braket{H|e^{jkx}} = \int_\mathbb{R} H(x)e^{-jk x} dx
\]

\[
\braket{H|e^{jkx}} = \int_0^\infty e^{-jk x} dx
\]

\[
\braket{H|e^{jkx}} = \frac{1}{-jk} \left( \lim_{x\rightarrow \infty} e^{-jk x} - 1 \right)
\]

Or, il est bien évident que si l'on sait que le module de $e^{-jk x}$ vaut $1$, on n'a pas d'information sur son argument à l'infini, et l'intégrale est indéfinie.

Cela est en partie dû au fait que $H$ soit de norme infinie. Toutefois, des éléments plus poussés de théorie des distributions permettent tout de même de trouver un sens à donner à $\braket{H|e^{jkx}}$, mais au prix de complications mathématiques ennuyantes.

La façon usuelle de lever ce problême est de restreindre l'usage de la transformée de Fourier aux fonctions périodiques, et d'utiliser pour les phénomènes transitoires la \emph{transformée de Laplace} :

\[\boxed{
\mathcal{L}(f) : p \rightarrow \braket{e^{-p^*x}|f} = \int_\mathbb{R} e^{-p x}f(x) dx
}\]

Avec $p \in \mathbb{C}$. L'idée est donc de généraliser la transformée de Fourier, et de décomposer sur un plus grand nombre exponentielles et pas seulement sur celles qui ont un imaginaire pur en exposant comme le fait la transformée de Fourier.

Reprenons l'exemple de l'échelon d'Heaviside, et appliquons cette fois la transformée de Laplace :

\[
\mathcal{L}(H)(p) = \int_\mathbb{R} H(x)e^{-px} dx
\]

\[
\mathcal{L}(H)(p) = \int_0^\infty e^{-px} dx
\]

\[
\mathcal{L}(H)(p) = \frac{1}{-p} \left(\lim_{x\rightarrow \infty} e^{-px} - 1 \right)
\]

Décomposons $p$ de façon carthésienne dans l'exponentielle:

\[
\mathcal{L}(H)(p) = \frac{1}{p} \left(1 - \lim_{x\rightarrow \infty} e^{-(a+jb)x}) \right)
\]

\[
\mathcal{L}(H)(p) = \frac{1}{p} \left(1 - \lim_{x\rightarrow \infty} e^{-ax}e^{-jbx}) \right)
\]

Qui converge si et seulement si $a>0$, auquel cas on trouve :

\[\boxed{
\mathcal{L}(H)(p) = \frac{1}{p}
}\]

La différence avec la transformée de Fourier étant que les fonctions $e^{-p^*x}$ dans laquelle nous avons décomposé l'échelon sont des spirales (lesquelles semblent intuitivement plus pertinentes pour l'étude de phénomènes transitoires) et non des cercles dans le plan complexe.

Le spectre au sens de Laplace se représente donc dans le plan complexe :

\textcolor{red}{(figure)}

\subsubsection{Inversion de la transformée de Laplace}

Si la démonstration de la forme de la transformée de Fourier inverse pouvait sembler bien ardue pour le résultat qu'on lui trouve, c'est pire encore pour la \emph{transformée de Laplace inverse}.

\textcolor{red}{(Par le théorème des résidus, cf formule de Bromwich-Mellin)}

\textcolor{red}{(Transformée inverse de $\frac{1}{p-\alpha}$)}

\chapter{Equations différentielles}

\section{Equations différentielles ordinaires linéaire}

Une équation différentielle est une équation liant une fonction à ses dérivées.

Elle est linéaire si on peut l'écrire comme une combinaison linéaire des dérivées.

Elle peut faire intervenir un second membre - c'est à dire une autre fonction supposée connue - et les dérivées de celui-ci.

Elle admet en général tout un espace de solutions, dont on en choisira une à partir des \emph{conditions aux limites} qui sont des contraites venants du contexte et imposées sur la solution.

Commençons par résoudre une équation différentielle tellement simple qu'on peut se demander si elle mérite vraiment cette appellation :

\[
\frac{df}{dx} = g
\]

Où $g$ est le second membre, supposé connu.

Les solutions sont obtenues immédiatement par une simple intégration :

\[
f(x) = \lambda + \left(\int g\right)(x)
\]

Le $\lambda$ est une constante déterminée par la condition aux limites. Formellement, les solutions de l'équation différentielle sont toutes les primitives de $g$.

Compliquons légèrement la chose :

\[
\frac{d^2 f}{dx^2} = g
\]

\[
f(x) = \mu + \lambda x + \left(\int \int g\right)(x)
\]

Cette fois-ci, on a besoin de deux conditions aux limites. Notez qu'elle ne sont pas nécessairement données explicitement, et qu'il peut falloir effectuer des manipulations relativement complexes pour trouver ces constantes $\lambda$ et $\mu$.

\subsection{Equations différentielles linéaires d'ordre 1}

\subsubsection{Solution générale}

Compliquons encore :

\[
\frac{df}{dx} = f
\]

Cette fois-ci, on peut vraiment parler d'équation différentielle, car elle ne se ramène pas à une bête intégration. On en connaît déjà une solution. En effet, nous avons déjà écrit cette ligne plus tôt dans ce cours, lorsque nous avions fait la théorie de l'exponentielle \textcolor{red}{(pas fait)}. Et effectivement, l'exponentielle est une solution de cette équation. Mais par linéarité de ladite équation, ses solutions sont un espace de dimension $1$, c'est à dire que :

\[
f(x) = \lambda e^x
\]

Fonctionne $\forall \lambda$.

Plus généralement, pour deux constantes $k_0$ et $G$, une équation différentielle d'ordre $1$ dite \emph{passe-bas} s'écrit :

\[\boxed{
\frac{df}{dx} + k_0 f = G k_0 g
}\]

De solutions :

\[\boxed{
f(x) = \lambda e^{-k_0 x} + p(x)
}\]

Avec $p$ une solution particulière de l'équation, que nous ne pouvons pas écrire car elle dépend de $g$. Cependant, on peut passer dans le domaine de Laplace pour obtenir un renseignement spectral (avec la notation légère $F=\mathcal{L}(f)$) :

\[
pF(p) + k_0 F(p) = G k_0 G(p)
\]

Pardonnez à ce stade la présence de deux entitées appellées $G$ totalement indépendantes.

\[
F(p) = \frac{Gk_0 G(p)}{p+k_0}
\]

Qui est une fraction rationnelle si $G(p)$ l'est également, nous permettant alors de sortir l'artillerie de l'arithmétique des polynômes pour la simplifier et tenter de repasser dans le domaine temporel. 

Physiquement, $g$ sera l'entrée d'un \emph{système} (qu'on impose) et $f$ sa sortie. Par exemple, si on excite l'entrée par un Dirac (de transformée de Laplace unitaire), la sortie sera la \emph{réponse impulsionnelle} de transformée :

\[
F(p) = \frac{Gk_0}{p+k_0}
\]

Qui est la transformée de l'exponentielle :

\[
f(x) = Gk_0 e^{-k_0 x}
\]

Et l'ensemble des solutions est simplement :

\[
f(x) = \lambda e^{-k_0 x}
\]

Mais si c'est un échelon de Heaviside qu'on applique, l'intégrale du Dirac de transformée $\frac{1}{p}$ :

\[
F(p) = \frac{Gk_0}{p(p+k_0)}
\]

Est décomposable en éléments simples :

\[
F(p) = \frac{A_1}{p} + \frac{A_2}{p+k_0}
\]

En appliquant la méthode décrite dans la section dédiée du chapitre d'analyse :

\[
\left\{\begin{matrix}
\frac{Gk_0}{p+k_0} = A_1 + \frac{A_2}{p+k_0}p \\
\frac{Gk_0}{p} = \frac{A_1}{p}(p+k_0) + A_2
\end{matrix}\right.
\]

On pose $p=0$ en haut et $p=-k_0$ en bas :

\[
\left\{\begin{matrix}
G = A_1 \\
-G = A_2
\end{matrix}\right.
\]

Ce qui donne donc :

\[
F(p) = G\left(\frac{1}{p}-\frac{1}{p+k_0}\right)
\]

Est apparue la transformée de Laplace de l'échelon $\frac{1}{p}$. On repasse dans le domaine temporel, l'ensemble des solutions est :

\[
f(x) = G H(x) - \lambda e^{-k_0 x}
\]

Par exemple, pour un gain unitaire, et une initialisation à $\lambda - G=0$ le tracé de cette réponse ressemble à :

\begin{sagesilent}
k_coords = [(2*pi*10^n).n() for n in srange(-2.5,2.5,0.01)];
x_coords = [x for x in srange(0,1.1,0.01)];
x = var('x');
p = var('p');
k0 = 2*pi; F(p) = 1/(1+p/k0);

y1mod_coords = [abs(F(I*k)).n() for k in k_coords];
y1arg_coords = [arg(F(I*k)).n() for k in k_coords];

f(x) = inverse_laplace(F(p)/p, p, x);
y1resp_coords = [f(x).n() for x in x_coords];
    
outputmod = "";
outputarg = "";
for i in range(0,len(k_coords)-1):
    outputmod += "\draw[black, thick] ("+str(log(k_coords[i]/2/pi,10).n())+","+str(log(y1mod_coords[i],10))+")--("+str(log(k_coords[i+1]/2/pi,10).n())+","+str(log(y1mod_coords[i+1],10))+");\n";
    outputarg += "\draw[black, thick] ("+str(log(k_coords[i]/2/pi,10).n())+","+str(y1arg_coords[i])+")--("+str(log(k_coords[i+1]/2/pi,10).n())+","+str(y1arg_coords[i+1])+");\n";

outputresp = "";
for i in range(0,len(x_coords)-1):
    outputresp += "\draw[black, thick] ("+str(x_coords[i])+","+str(y1resp_coords[i])+")--("+str(x_coords[i+1])+","+str(y1resp_coords[i+1])+");\n";
\end{sagesilent}

\begin{center}
\begin{tikzpicture}[x=5\textwidth/6/1.1), y=\textwidth/3/1.3]
 // Dimensions
 \tikzmath{
 	\width = 1.1;
 	\height = 1.3;
 };
 
 // Axes
 \draw[->] (0,0) -- (\width,0) node[right] {$x$};
 \draw[->] (0,0) -- (0,\height) node[above] {$f(x)$};
 //Graduation angulaire
 \foreach \x in {1}
      \draw (\x,\height/40) -- ++(0,-\height/20) node[anchor=north] {$\x$};
  
 \draw (0,0.95) node[left] {};
 \draw[dotted] (0,0.95) -- ++(\width,0);
 \draw (0,1) node[left] {$1$};
 \draw[dotted] (0,1) -- ++(\width,0);
 \draw (0,1.05) node[left] {};
 \draw[dotted] (0,1.05) -- ++(\width,0);
 \draw[dotted] (1/2/pi,0) node[below] {$\frac{1}{k_0}$} |- (0,0.63) node[left] {$0.63$};
 \draw[dashed] (0,0) -- ++(1/2/pi,1);

 \if\releaseversion1
 \sagestr{outputresp}
 \fi
\end{tikzpicture}
\end{center}

En pratique, $\frac{1}{k_0}$ sera une longueur ou une durée (alors plutôt notée $\frac{1}{\omega_0}$) caractéristique des réponses du système.

La difficulté pour trouver la solution particulière étant d'être capable d'inverser la transformée de Laplace.

Pour l'étude des régimes stationnaires (réponse à l'excitation d'une fonction périodique), on posera $p=jk$ de façon à passer dans le domaine de Fourier. On observe alors que $g(x)=e^{jkx}$ est une fonction propre de l'opérateur $\frac{F}{G}$ \textcolor{red}{(pas fait)}. La valeur propre associée est un complexe $|\frac{F}{G}(jk)|e^{j\left(\arg \frac{F}{G}(jk)\right)}$. 

Cette valeur propre, appellée improprement \emph{transformée complexe} de la fonction $\cos(kx)$ \textcolor{red}{(préciser pourquoi cos)} est abondamment utilisée en électricité.

Il est alors d'usage d'employer l'outil consacré pour la représentation spectrale des systèmes qui porte le nom de \emph{diagramme de Bode} et qui met conjointement en lumière le gain $|\frac{F}{G}(jk)|$ et la phase $\arg \frac{F}{G}(jk)$ de la valeur propre introduite en réponse à l'excitation harmonique.

\begin{center}
\begin{tikzpicture}[x=5\textwidth/6/5, y=\textwidth/3/3.5]
 // Dimensions
 \tikzmath{
 	\width = 5.2;
 	\height = 2.5;
 };
  
 // Axes
 \draw[->] (-\width/2,0) -- (\width/2,0) node[right] {$k$};
 \draw[->] (0,-\height) -- (0,0.5) node[above] {$\log|\frac{F}{G}(jk_0)|$};
 //Graduation logarithmique
 \foreach \n in {-2,-1,1,2}
      \draw (\n,\height/30) -- ++(0,-\height/15) node[anchor=north] {$2\pi 10^{\n}$};
 \foreach \n in {-2,-1}
      \draw (\width/80,\n) -- ++(-\width/40,0) node[left] {$\n$};
      
 \draw (0.15,-0.05) node[above] {$k_0$};

 \draw (-\width/80,-0.1505) -- (\width/80,-0.1505);
 \draw (0,-0.1505) node[below left] {$-\frac{1}{2}\log(2)$};
 
 \draw[dotted] (0,-1) -| (1,-0.5);

 \if\releaseversion1
 \sagestr{outputmod}
 \fi
\end{tikzpicture}
\end{center}

\begin{center}
\begin{tikzpicture}[x=5\textwidth/6/5, y=\textwidth/3/(2*pi)]
 // Dimensions
 \tikzmath{
 	\width = 5.2;
 	\height = 2.5;
 };
  
 // Axes
 \draw[->] (-\width/2,0) -- (\width/2,0) node[right] {$k$};
 \draw[->] (0,-pi/2) -- (0,0.5) node[above] {$\arg \frac{F}{G}(jk_0)$};
 //Graduation logarithmique
 \foreach \n in {-2,-1,1,2}
      \draw (\n,\height/30) -- ++(0,-\height/15) node[anchor=north] {$2\pi 10^{\n}$};
      
 \draw (0.15,-0.1) node[above] {$k_0$};
  
 \draw (0,-pi/2) node[left] {$-\frac{\pi}{2}$};
 \draw[dotted] (0,-pi/2) -- ++(2.5,0);

 \draw (-\width/80,-pi/4) -- (\width/80,-pi/4);
 \draw (0,-pi/4+0.1) node[right] {$-\frac{\pi}{4}$};

 \if\releaseversion1
 \sagestr{outputarg}
 \fi
\end{tikzpicture}
\end{center}

L'ordonnée logarithmique est utilisée dans la représentation du module, à la fois pour des raisons historiques (son dessin manuel étant facilité) et pour satisfaire aux besoins de l'\emph{acoustique} (la perception des niveaux sonores par l'humain étant grossièrement logarithmique).

En plus du passe-bas de \emph{transmittance} mise sous une forme dite \emph{canonique} :

\[\boxed{
\frac{F(p)}{G(p}) = \frac{G}{1+\frac{p}{k_0}}
}\]

On trouve au premier ordre les structures suivantes :

\begin{itemize}
\item Le \emph{passe-haut}, de transmittance :

\[\boxed{
\frac{F(p)}{G(p}) = \frac{G \frac{p}{k_0}}{1+\frac{p}{k_0}}
}\]

De réponse à un échelon :

\begin{sagesilent}
k_coords = [(2*pi*10^n).n() for n in srange(-2.5,2.5,0.05)];
x_coords = [x for x in srange(0,1.1,0.01)];
x = var('x');
p = var('p');
k0 = 2*pi; F(p) = p/k0/(1+p/k0);

y1mod_coords = [abs(F(I*k)).n() for k in k_coords];
y1arg_coords = [arg(F(I*k)).n() for k in k_coords];

f(x) = inverse_laplace(F(p)/p, p, x);
y1resp_coords = [f(x).n() for x in x_coords];
    
outputmod = "";
outputarg = "";
for i in range(0,len(k_coords)-1):
    outputmod += "\draw[black, thick] ("+str(log(k_coords[i]/2/pi,10).n())+","+str(log(y1mod_coords[i],10))+")--("+str(log(k_coords[i+1]/2/pi,10).n())+","+str(log(y1mod_coords[i+1],10))+");\n";
    outputarg += "\draw[black, thick] ("+str(log(k_coords[i]/2/pi,10).n())+","+str(y1arg_coords[i])+")--("+str(log(k_coords[i+1]/2/pi,10).n())+","+str(y1arg_coords[i+1])+");\n";

outputresp = "";
for i in range(0,len(x_coords)-1):
    outputresp += "\draw[black, thick] ("+str(x_coords[i])+","+str(y1resp_coords[i])+")--("+str(x_coords[i+1])+","+str(y1resp_coords[i+1])+");\n";
\end{sagesilent}

\begin{center}
\begin{tikzpicture}[x=5\textwidth/6/1.1), y=\textwidth/3/1.3]
 // Dimensions
 \tikzmath{
 	\width = 1.1;
 	\height = 1.3;
 };
 
 // Axes
 \draw[->] (0,0) -- (\width,0) node[right] {$x$};
 \draw[->] (0,0) -- (0,\height) node[above] {$f(x)$};
 //Graduation angulaire
 \foreach \x in {1}
      \draw (\x,\height/40) -- ++(0,-\height/20) node[anchor=north] {$\x$};
  
 \draw (0,1) node[left] {$1$};
 \draw[dotted] (1/2/pi,0) node[below] {$\frac{1}{k}$} |- (0,0.37) node[left] {$0.37$};
 \draw[dashed] (0,1) -- ++(1/2/pi,-1);

 \if\releaseversion1
 \sagestr{outputresp}
 \fi
\end{tikzpicture}
\end{center}

Et de diagramme de Bode :

\begin{center}
\begin{tikzpicture}[x=5\textwidth/6/5, y=\textwidth/3/3.5]
 // Dimensions
 \tikzmath{
 	\width = 5.2;
 	\height = 2.5;
 };
  
 // Axes
 \draw[->] (-\width/2,0) -- (\width/2,0) node[right] {$k$};
 \draw[->] (0,-\height) -- (0,0.5) node[above] {$\log|\frac{F}{G}(jk_0)|$};
 //Graduation logarithmique
 \foreach \n in {-2,-1,1,2}
      \draw (\n,\height/30) -- ++(0,-\height/15) node[anchor=north] {$2\pi 10^{\n}$};
  \foreach \n in {-2,,-1}
      \draw (-\width/80,\n) -- ++(\width/40,0) node[right] {$\n$};
 
 \draw (0.15,-0.05) node[above] {$k_0$};

 \draw (-\width/80,-0.1505) -- (\width/80,-0.1505);
 \draw (-0.05,-0.1505) node[below right] {$-\frac{1}{2}\log(2)$};

 \draw[dotted] (0,-1) -| (-1,-0.5);

 \if\releaseversion1
 \sagestr{outputmod}
 \fi
\end{tikzpicture}
\end{center}

\begin{center}
\begin{tikzpicture}[x=5\textwidth/6/5, y=\textwidth/3/(2*pi)]
 // Dimensions
 \tikzmath{
 	\width = 5.2;
 	\height = 2.5;
 };
  
 // Axes
 \draw[->] (-\width/2,0) -- (\width/2,0) node[right] {$k_0$};
 \draw[->] (0,0) -- (0,pi/2+0.5) node[above] {$\arg \frac{F}{G}(jk_0)$};
 //Graduation logarithmique
 \foreach \n in {-2,-1,1,2}
      \draw (\n,\height/30) -- ++(0,-\height/15) node[anchor=north] {$2\pi 10^{\n}$};
 
 \draw (-0.15,-0.1) node[above] {$k_0$};
  
 \draw (0,pi/2) node[right] {$\frac{\pi}{2}$};
 \draw[dotted] (0,pi/2) -- ++(-2.5,0);

 \draw (-\width/80,pi/4) -- (\width/80,pi/4);
 \draw (0,pi/4+0.1) node[right] {$-\frac{\pi}{4}$};

 \if\releaseversion1
 \sagestr{outputarg}
 \fi
\end{tikzpicture}
\end{center}

\item Ainsi que le \emph{passe-bande}, de transmittance :

\[\boxed{
\frac{F(p)}{G(p}) = \frac{G\left(1+\frac{p}{k_1}\right)}{1+\frac{p}{k_0}}
}\]

\textcolor{red}{(compléter)}
\end{itemize}

\subsubsection{Identification des paramètres}

Lorsqu'on étudie un système physique, on est souvent amené à le modéliser par une équation différentielles dont les paramètres (les constantes introduites dans cette équation différentielle) sont déterminables à partir de l'observation de la réponse du système.

Concrètement, je vous donne un moteur, sans jamais avoir à l'ouvrir vous déduisez d'essais et de mesures les paramètres du modèle linéaire de ce moteur, ce qui vous permet ensuite de le simuler dans des cas différents plus ou moins fidèlement selon que l'équation différentielle est bien choisie ou non.

Pour un système du premier ordre, on regardera simplement sa réponse à un échelon. En effet, dans la transformée de Laplace du passe-bas :

\[
\frac{F(p)}{G(p}) = \frac{G}{1+\frac{p}{k_0}}
\]

La constante $G$ est le \emph{gain} du système :

\[\boxed{
G = \lim_{x\rightarrow \infty} f(x)
}\]

Reste à déterminer $k_0$, ce qui peut-être fait de deux manières indépendantes. En premier lieu, à partir d'une mesure de la pente à l'origine de la réponse temporelle :

\[
\frac{df}{dx}(0) = \lambda k_0
\]

\[\boxed{
\frac{1}{k_0} = \frac{\lambda}{\frac{df}{dx}(0)}
}\]

Pour un passe-haut, on prendra son opposée. Ou alors en regardant l'abscisse lorsque la réponse s'est déplacée de $1-e^{-1}\sim 63\%$ de son déplacement total, du fait de :

\[
f\left(\frac{1}{k_0} \right) = \lambda e^{-1} + G
\]

\[
f\left(\frac{1}{k_0} \right) = \left(f(0)-\lim_{x\rightarrow \infty}f(x)\right) e^{-1} + \lim_{x\rightarrow \infty}f(x)
\]

\[
f\left(\frac{1}{k_0} \right) = f(0) e^{-1} + (1-e^{-1})\lim_{x\rightarrow \infty}f(x)
\]

\[\boxed{
f\left(\frac{1}{k_0} \right) = f(0) + \left(\lim_{x\rightarrow \infty}f(x) - f(0)\right)(1- e^{-1})
}\]

Cette seconde expression pouvant être utilisée telle quelle pour le passe-bas tout comme le passe-haut.

\textcolor{red}{(passe-bande et coupe-bande)}

\subsection{Equations différentielles linéaires d'ordre 2}

\subsubsection{Solution générale}

Une équation différentielle d'ordre $2$ passe-bas s'écrit dans un cadre général :

\[\boxed{
\frac{d^2f}{dx^2} + \frac{k_0}{Q}\frac{df}{dx} + k_0^2 f = g
}\]

Nous allons passer par le domaine de Laplace pour trouver ses solutions. La transformée de Laplace d'une équation d'ordre 2 est :

\[
\left(p^2 + \frac{k_0}{Q}p + k_0^2\right)F(p) = G(p)
\]

\[\boxed{
F(p) = \frac{G(p)}{p^2 + \frac{k_0}{Q}p + k_0^2}
}\]

Dans $\mathbb{C}$, le dénominateur est toujours factorisable :

\[
F(p) = \frac{G(p)}{(p-r_1)(p-r_2)}
\]

Supposons ces racines connues (nous les calculerons ultérieurement). On a alors deux cas possibles :

\begin{itemize}
\item $r_1 \neq r_2$ : On peut faire une décomposition sur deux pôles simples.

\[
F(p) = \frac{G(p)}{(p-r_1)(p-r_2)} = \left(\frac{A_1}{p-r_1} + \frac{A_2}{p-r_2}\right)G(p)
\]

D'une simplicité telle que je vous épargne la détermination des numérateurs :

\[
F(p) = \frac{G(p)}{r_2-r_1}\left(\frac{1}{p-r_2}-\frac{1}{p-r_1} \right)
\]

La réponse implusionnelle est donc de transformée :

\[
F(p) = \frac{1}{r_2-r_1}\left(\frac{1}{p-r_2}-\frac{1}{p-r_1} \right)
\]

On reconnait deux échelons, et comme les solutions d'une équation différentielle d'ordre deux sont un espace de dimension $2$ \textcolor{red}{(pourquoi ?)}, on a :

\[\boxed{
f(x) = \lambda e^{r_1 x}+ \mu e^{r_2 x} + p(x)
}\]

\item $r_2 = r_1$ : C'est le cas où $Q = \frac{1}{2}$. Puisqu'il nous faut un espace des solutions d'ordre 2, on doit effectuer une décomposition sur un pôle double pour faire apparaître un pôle simple :

\[
F(p) = \frac{G(p)}{(p-r)^2} = \frac{A_{11}}{p-r} + \frac{A_{12}}{(p-r)^2}
\]

Pour laquelle on s'empressera de ne PAS calculer les $A$ pour en conclure que :

\[\boxed{
f(x) = (\lambda + \mu x)e^{rx} + p(x)
}\]

Est l'espace des solutions engendré par transformation inverse, la transformée de Laplace de $xe^{rx}$ étant $\frac{1}{(p-r)^2}$.

\end{itemize}

Si cette démonstration vous semble un peu vaseuse, il vous suffit de vérifier que les solutions proposées sont bien solutions.

$Q$ est appellé le \emph{facteur de qualité}, et il détermine le régime de l'équation différentielle. En vue de déterminer ses racines, on écrit le discriminant du polynôme au dénominateur de la transformée de Laplace :

\[
\Delta = \frac{k_0^2}{Q^2}-4k_0^2
\]

\begin{itemize}
\item Si $Q<\frac{1}{2}$, qui correspond au régime apériodique, les racines sont réelles et situées en :

\[
(r_1, r_2) = \frac{-\frac{k_0}{Q}\pm \sqrt{\Delta}}{2}
\]

\[
(r_1, r_2) = k_0\left(-\frac{1}{2Q}\pm \sqrt{\frac{1}{4Q^2}-1}\right)
\]

Et on a une somme de réponses d'ordre $1$.
\item Si $Q=\frac{1}{2}$, le régime est dit \emph{critique} et si $r=k_0$ est pris proche de $0$, la solution peut atteindre des niveaux très élevés.
\item Si $Q>\frac{1}{2}$, le régime est dit \emph{pseudo-périodique}, c'est à dire qu'il est le produit d'un régime oscillant (l'exponentielle complexe) par un amortissement (la partie réelle de $r$ étant toujours négative pour un système réel). En effet, les racines sont complexes conjugées :

\[
(r, r^*) = \frac{-\frac{k_0}{Q}\pm j\sqrt{-\Delta}}{2}
\]

\[
(r, r^*) = k_0\left(-\frac{1}{2Q}\pm j\sqrt{1-\frac{1}{4Q^2}}\right)
\]

\end{itemize}

Voici par exemple différentes réponses à un échelon :

\begin{sagesilent}
k_coords = [(2*pi*10^n).n() for n in srange(-2.5,2.5,0.05)];
x_coords = [x for x in srange(0,2,0.02)];
x = var('x');
p = var('p');

k0 = 2*pi; Q = 1/3; F(p) = 1/(1+p/Q/k0+p^2/k0^2);
y1mod_coords = [abs(F(I*k)).n() for k in k_coords];
y1arg_coords = [arg(F(I*k)).n() for k in k_coords];
f(x) = inverse_laplace(F(p)/p, p, x);
y1resp_coords = [f(x).n() for x in x_coords];

k0 = 2*pi; Q = 1/2; G(p) = 1/(1+p/Q/k0+p^2/k0^2);
y2mod_coords = [abs(G(I*k)).n() for k in k_coords];
y2arg_coords = [arg(G(I*k)).n() for k in k_coords];
g(x) = inverse_laplace(G(p)/p, p, x);
y2resp_coords = [g(x).n() for x in x_coords];

k0 = 2*pi; Q = 1/sqrt(2); H(p) = 1/(1+p/Q/k0+p^2/k0^2);
y3mod_coords = [abs(H(I*k)).n() for k in k_coords];
y3arg_coords = [arg(H(I*k)).n() for k in k_coords];
h(x) = inverse_laplace(H(p)/p, p, x);
y3resp_coords = [h(x).n() for x in x_coords];

k0 = 2*pi; Q = 1; U(p) = 1/(1+p/Q/k0+p^2/k0^2);
y4mod_coords = [abs(U(I*k)).n() for k in k_coords];
y4arg_coords = [arg(U(I*k)).n() for k in k_coords];
u(x) = inverse_laplace(U(p)/p, p, x);
y4resp_coords = [u(x).n() for x in x_coords];

k0 = 2*pi; Q = 2; V(p) = 1/(1+p/Q/k0+p^2/k0^2);
y5mod_coords = [abs(V(I*k)).n() for k in k_coords];
y5arg_coords = [arg(V(I*k)).n() for k in k_coords];
v(x) = inverse_laplace(V(p)/p, p, x);
y5resp_coords = [v(x).n() for x in x_coords];
    

outputresp = "";
for i in range(0,len(x_coords)-1):
    outputresp += "\draw[red, thick] ("+str(x_coords[i])+","+str(y1resp_coords[i])+")--("+str(x_coords[i+1])+","+str(y1resp_coords[i+1])+");\n";
    outputresp += "\draw[green, thick] ("+str(x_coords[i])+","+str(y2resp_coords[i])+")--("+str(x_coords[i+1])+","+str(y2resp_coords[i+1])+");\n";
    outputresp += "\draw[blue, thick] ("+str(x_coords[i])+","+str(y3resp_coords[i])+")--("+str(x_coords[i+1])+","+str(y3resp_coords[i+1])+");\n";
    outputresp += "\draw[purple, thick] ("+str(x_coords[i])+","+str(y4resp_coords[i])+")--("+str(x_coords[i+1])+","+str(y4resp_coords[i+1])+");\n";
    outputresp += "\draw[orange, thick] ("+str(x_coords[i])+","+str(y5resp_coords[i])+")--("+str(x_coords[i+1])+","+str(y5resp_coords[i+1])+");\n";
    
outputmod = ""; outputarg = "";
for i in range(0,len(k_coords)-1):
    outputmod += "\draw[red, thick] ("+str(log(k_coords[i]/2/pi,10).n())+","+str(log(y1mod_coords[i],10))+")--("+str(log(k_coords[i+1]/2/pi,10).n())+","+str(log(y1mod_coords[i+1],10))+");\n";
    outputarg += "\draw[red, thick] ("+str(log(k_coords[i]/2/pi,10).n())+","+str(y1arg_coords[i])+")--("+str(log(k_coords[i+1]/2/pi,10).n())+","+str(y1arg_coords[i+1])+");\n";
    outputmod += "\draw[green, thick] ("+str(log(k_coords[i]/2/pi,10).n())+","+str(log(y2mod_coords[i],10))+")--("+str(log(k_coords[i+1]/2/pi,10).n())+","+str(log(y2mod_coords[i+1],10))+");\n";
    outputarg += "\draw[green, thick] ("+str(log(k_coords[i]/2/pi,10).n())+","+str(y2arg_coords[i])+")--("+str(log(k_coords[i+1]/2/pi,10).n())+","+str(y2arg_coords[i+1])+");\n";
    outputmod += "\draw[blue, thick] ("+str(log(k_coords[i]/2/pi,10).n())+","+str(log(y3mod_coords[i],10))+")--("+str(log(k_coords[i+1]/2/pi,10).n())+","+str(log(y3mod_coords[i+1],10))+");\n";
    outputarg += "\draw[blue, thick] ("+str(log(k_coords[i]/2/pi,10).n())+","+str(y3arg_coords[i])+")--("+str(log(k_coords[i+1]/2/pi,10).n())+","+str(y3arg_coords[i+1])+");\n";
    outputmod += "\draw[purple, thick] ("+str(log(k_coords[i]/2/pi,10).n())+","+str(log(y4mod_coords[i],10))+")--("+str(log(k_coords[i+1]/2/pi,10).n())+","+str(log(y4mod_coords[i+1],10))+");\n";
    outputarg += "\draw[purple, thick] ("+str(log(k_coords[i]/2/pi,10).n())+","+str(y4arg_coords[i])+")--("+str(log(k_coords[i+1]/2/pi,10).n())+","+str(y4arg_coords[i+1])+");\n";
    outputmod += "\draw[orange, thick] ("+str(log(k_coords[i]/2/pi,10).n())+","+str(log(y5mod_coords[i],10))+")--("+str(log(k_coords[i+1]/2/pi,10).n())+","+str(log(y5mod_coords[i+1],10))+");\n";
    outputarg += "\draw[orange, thick] ("+str(log(k_coords[i]/2/pi,10).n())+","+str(y5arg_coords[i])+")--("+str(log(k_coords[i+1]/2/pi,10).n())+","+str(y5arg_coords[i+1])+");\n";
\end{sagesilent}

\begin{center}
\begin{tikzpicture}[x=5\textwidth/6/2), y=\textwidth/3/1.5]
 // Dimensions
 \tikzmath{
 	\width = 2;
 	\height = 1.5;
 };
 
 // Axes
 \draw[->] (0,0) -- (\width,0) node[right] {$x$};
 \draw[->] (0,0) -- (0,\height) node[above] {$f(x)$};
 //Graduation angulaire
 \foreach \x in {1}
      \draw (\x,\height/40) -- ++(0,-\height/20) node[anchor=north] {$\x$};
  
 \draw (0,0.95) node[left] {};
 \draw[dotted] (0,0.95) -- ++(2,0);
 \draw (0,1) node[left] {$1$};
 \draw[dotted] (0,1) -- ++(2,0);
 \draw (0,1.05) node[left] {};
 \draw[dotted] (0,1.05) -- ++(2,0);
	
 \draw (0.85,0) node[below] {$t_{5\%}$};
 \draw[dotted, thick, purple] (0.85,0) -- ++(0,1.05);
	
 \draw (0.159,0) node[below] {$x_i$};
 \draw[dotted, thick, green] (0.159,0) -- ++(0,0.264);
 \draw (0,0.264) node[left] {$f_i$};
 \draw[dotted, thick, green] (0,0.264) -- ++(0.159,0);
	
 \draw (0,1.444) node[left] {$1+D$};
 \draw[dotted, thick, orange] (0,1.444) -- ++(0.516,0);
	
 \draw (0.65,0.68) node[anchor=west] {\textcolor{red}{$\frac{1}{3}$}};
 \draw (0.6,0.82) node[anchor=west] {\textcolor{green}{$\frac{1}{2}$}};
 \draw (0.55,0.97) node[anchor=west] {\textcolor{blue}{$\frac{1}{\sqrt{2}}$}};
 \draw (0.5,1.08) node[anchor=west] {\textcolor{purple}{$1$}};
 \draw (0.42,1.32) node[anchor=west] {\textcolor{orange}{$Q=2$}};
 \if\releaseversion1
 \sagestr{outputresp}
 \fi
\end{tikzpicture}
\end{center}

Avec pour diagrammes de Bode correspondants :

\begin{center}
\begin{tikzpicture}[x=5\textwidth/6/5, y=\textwidth/3/3.5]
 // Dimensions
 \tikzmath{
 	\width = 5.2;
 	\height = 4.5;
 };
  
 // Axes
 \draw[->] (-\width/2,0) -- (\width/2,0) node[right] {$k$};
 \draw[->] (0,-\height) -- (0,0.5) node[above] {$\log|\frac{F}{G}(jk_0)|$};
 //Graduation logarithmique
 \foreach \n in {-2,-1,1,2}
      \draw (\n,\height/30) -- ++(0,-\height/15) node[anchor=north] {$2\pi 10^{\n}$};
   \foreach \n in {-4,-3,...,-1}
      \draw (\width/80,\n) -- ++(-\width/40,0) node[left] {$\n$};

 \draw (0.15,-0.05) node[above] {$k_0$};

 \draw (-\width/80,-0.301) -- (\width/80,-0.301);
 \draw (-0.05,-0.301) node[below left] {$-\log(2)$};

 \draw[dotted] (0,-2) -| (1,-0.6);

 \if\releaseversion1
 \sagestr{outputmod}
 \fi
\end{tikzpicture}
\end{center}

\begin{center}
\begin{tikzpicture}[x=5\textwidth/6/5, y=\textwidth/3/(2*pi)]
 // Dimensions
 \tikzmath{
 	\width = 5.2;
 	\height = 4;
 };
  
 // Axes
 \draw[->] (-\width/2,0) -- (\width/2,0) node[right] {$k_0$};
 \draw[->] (0,-pi) -- (0,0.5)  node[above] {$\arg \frac{F}{G}(jk_0)$};
 //Graduation logarithmique
 \foreach \n in {-2,-1,1,2}
      \draw (\n,\height/30) -- ++(0,-\height/15) node[anchor=north] {$2\pi 10^{\n}$};
  
 \draw (-0.15,-0.1) node[above] {$k_0$};
  
 \draw (0,-pi) node[left] {$-\pi$};
 \draw[dotted] (0,-pi) -- ++(2.5,0);

 \draw (-\width/80,-pi/2) -- (\width/80,-pi/2);
 \draw (0,-pi/2+0.2) node[right] {$-\frac{\pi}{2}$};

 \if\releaseversion1
 \sagestr{outputarg}
 \fi
\end{tikzpicture}
\end{center}

Les exponentielles complexes doivent-êtres comprises comme des cosinus et sinus, au sens où si l'on restreint notre problême à $\mathbb{R}$ (ce que l'on fait toujours en physique classique), en premier lieu les racines sont conjugées mais $\lambda$ et $\mu$ également :

\[
f(x) = \lambda e^{(\alpha+j\beta) x}+ \lambda^* e^{(\alpha-j\beta) x} + p(x)
\]

Avec $\boxed{\alpha = -\frac{k_0}{2Q}}$ l'amortissement et $\boxed{\beta = k_0\sqrt{1-\frac{1}{4Q^2}}}$ la \emph{pseudo-pulsation}.

De façon à faire apparaître des demi-sommes et demi-différences d'exponentielles conjugés, lesquels sont des cosinus et sinus :

\[
f(x) = \lambda e^{(\alpha+j\beta) x}+ \lambda^* e^{(\alpha-j\beta) x} + p(x)
\]

\[
f(x) = e^{\alpha x}\left(\frac{1}{2}(a-jb) e^{j\beta x}+ \frac{1}{2}(a+jb) e^{-j\beta x}\right) + p(x)
\]

\[
f(x) = e^{\alpha x}\left(a \frac{e^{j\beta x} + e^{-j\beta x}}{2}+ b \frac{e^{j\beta x} - e^{-j\beta x}}{2j}\right) + p(x)
\]

\[\boxed{
f(x) = e^{\alpha x}\left(a \cos(\beta x)+ b \sin(\beta x)\right) + p(x)
}\]

\textcolor{red}{(résonnance $k_0 \sqrt{1-\frac{1}{2Q^2}}$)}

Nous verrons dans le chapitre sur le contrôle des systèmes qu'une valeur de $Q$ intéressante est $Q=\frac{1}{\sqrt{2}}$, auquel cas on dit que la \emph{rapidité} est maximale (mais ce peut-être au détriment de la \emph{stabilité}, comme nous le verrons)\textcolor{red}{(pas fait)}.

Je vous laisse vérifier que les pôles sont alors en $r = k_0 e^{\pm j\frac{3\pi}{4}}$.

\subsubsection{Identification des paramètres}

Suivant le type de réponse du deuxième ordre (pseudo-périodique ou pas) qu'on a, on peut appliquer l'une ou l'autres des méthodes suivantes pour déterminer la position des pôles.

\begin{itemize}
\item Pour une réponse pseudo-oscillante, on mesure le dépassement qu'on normalise en le divisant par l'amplitude de la réponse (valeur finale soustraite de la valeur initiale).

La courbe suivante permet alors de déterminer directement $Q$ (à faire).

Une fois $Q$ connu, le produit $k_0 x_r$ est constant quelque soit l'amplitude de l'échelon en entrée. Cela nous amène à trouver $k_0$.

Les pôles complexes s'écrivent \textcolor{red}{(compléter)}.

\item Pour une réponse somme d'exponentielles, on se base sur le point d'inflexion qui est souvent localisable par le biais de l'optimum de la grandeur dérivée quand celle-ci est mesurable.

\[
\left\{\begin{matrix}
f_i = \frac{r_1r_2}{r_2-r_1}\left(\ln(r_2)-\ln(r_1)\right) \\
x_i = 1-(1+\frac{r_2}{r_1})\left(\frac{r_2}{r_1}\right)^{-\frac{r_2}{r_2-r_1}}
\end{matrix}\right.
\]

Le répérage du point d'inflexion permet donc de localiser les pôles.

\end{itemize}

\section{Equations ordinaires non-linéaires}

Cette section est dédiée à l'analyse de toutes les équations différentielles ordinaires, qu'elles soient linéaires ou non. Les outils qui seront développés se prêteront particulièrement ``au non-linéaire'', domaine d'étude dans lequel toutes sortes de phénomènes exclus en linéaire deviennent possibles.

\subsection{Représentation d'état}

On défini formellement un système par sa représentation d'état :

\[\boxed{
\left\{\begin{matrix}
\frac{dX}{dx}=f(X,U) \\
Y=g(X,U)
\end{matrix}\right.
}\]

Où $U$, $X$ et $Y$ sont respectivement les vecteurs \emph{de commande} (qu'on peut perçevoir comme les entrées du système), \emph{d'état} et \emph{de sorties}. L'absence de flêche malgrès la qualification de vecteur est une convention de l'automatique, les composantes de ces vecteurs n'étant pas nécessairement de mêmes dimensions, ces vecteurs n'admettent pas d'interprétation physique. 

Prenons pour l'exemple l'équation différentielle ordinaire non-linéaire suivante :

\[
\ddot{y} + \dot{y}^2 + y(y-3) = 0
\]

On commence par choisir le vecteur d'état $
X=
\left(\begin{matrix}
\dot{y} \\
y
\end{matrix}\right)$, et le système se récrit :

\[
\left\{\begin{matrix}
\left(\begin{matrix}
\ddot{y} \\
\dot{y}
\end{matrix}\right)=
\left(\begin{matrix}
-\dot{y}^2-y(y-3) \\
\dot{y}
\end{matrix}\right) \\
y=\left(\begin{matrix}0 & 1\end{matrix}\right)\left(\begin{matrix}
\dot{y} \\
y
\end{matrix}\right)
\end{matrix}\right.
\]

\[
\left\{\begin{matrix}
\frac{d}{dx}\left(\begin{matrix}
X_1 \\
X_2
\end{matrix}\right)=
\left(\begin{matrix}
-X_1^2-X_2(X_2-3) \\
X_1
\end{matrix}\right) \\
y=\left(\begin{matrix}0 & 1\end{matrix}\right)\left(\begin{matrix}
X_1 \\
X_2
\end{matrix}\right)
\end{matrix}\right.
\]

\subsection{Linéarisation}

La représentation d'état d'une équation différentielle linéaire est de la forme :

\[\boxed{
\left\{\begin{matrix}
\frac{dX}{dx}=AX+BU \\
Y=CX+DU
\end{matrix}\right.
}\]

L'intéret est que quelque soit l'ordre de l'équation différentielle de départ, on pourra la ramener à une équation différentielle matricielle d'ordre $1$. Le calcul de ses solutions est alors rendu possible par l'usage de l'\emph{exponentiation de matrice} définie au sens des séries entières \textcolor{red}{(pas fait)} :

\[
e^A : A \rightarrow \sum_{k\in\mathbb{N}}\frac{A^k}{k!}
\]

Nous verrons à la fin de ce cours quelques exemples d'utilisation de cet outil extrêmement puissant.

Pour l'étude locale (au voisinage d'une point) d'une équation différentielle non-linéaire, on devra donc se préter au jeu de la linéarisation en estimant les matrices $A$ et $B$ associées à l'hyperplan tangent de l'équation différentielle non-linéaire en ce point (mais peu importe la désignation technique).

Tout comme la droite tangente à une courbe est paramétrée par la dérivée de cette courbe, ici on s'intéressera naturellement au gradient de $f$ (aussi désignée sous l'appellation de \emph{matrice Jacobienne}.

\[\boxed{
\underline{\underline{\nabla}} f =
\left(\begin{matrix}
\frac{\partial f_1}{\partial X_1} & \frac{\partial f_1}{\partial X_2} \\
\frac{\partial f_2}{\partial X_1} & \frac{\partial f_2}{\partial X_2}
\end{matrix}\right)
}\]

Pour notre exemple :

\[
\underline{\underline{\nabla}}f =
\left(\begin{matrix}
-2X_1 & -2X_2+3 \\
1 & 0
\end{matrix}\right)
\]

\subsection{Etude des équilibres}

On appelle \emph{points d'équilibres} les vecteurs $X$ stationnaires, qui n'évoluent pas avec $x$ au premier ordre, c'est-à-dire qui satisfont :

\[\boxed{
\frac{dX}{dx}=\vec{0}
}\]

Pour notre exemple, on trouve les points $\left(\begin{matrix}0 \\ 0\end{matrix}\right)$ et $\left(\begin{matrix}0 \\ 3\end{matrix}\right)$.

Si notre système est dans cet état, alors il n'évoluera spontanément plus. Nous allons maintenant voir ce qu'il advient s'il est presque dans cet état mais pas tout à fait.

Il est intéressant d'évaluer la Jacobienne aux points d'équilibres.

\[
\underline{\underline{\nabla}}f\left(\begin{matrix}0 \\ 0\end{matrix}\right)=
\left(\begin{matrix}
0 & 3 \\
1 & 0
\end{matrix}\right)
\]

\[
\underline{\underline{\nabla}}f\left(\begin{matrix}0 \\ 3\end{matrix}\right)=
\left(\begin{matrix}
0 & -3 \\
1 & 0
\end{matrix}\right)
\]

On comprend qu'au voisinage des points d'équilibre, le comportement du système sera donné par un système linéaire :

\[
\frac{d}{dx}\left(\begin{matrix}
X_1 \\
X_2
\end{matrix}\right)=
\left(\begin{matrix}
0 & -3 \\
1 & 0
\end{matrix}\right)
\left(\begin{matrix}
X_1 \\
X_2
\end{matrix}\right)
\]

Au voisinage du premier point d'équilibre et :

\[
\frac{d}{dx}\left(\begin{matrix}
X_1 \\
X_2
\end{matrix}\right)=
\left(\begin{matrix}
0 & 3 \\
1 & 0
\end{matrix}\right)
\left(\begin{matrix}
X_1 \\
X_2
\end{matrix}\right)
\]

Au voisinage du second, qui décrivent tous deux des équations différentielles linéaires d'ordre $2$.

Nous allons voir maintenant une puissante méthode qui nous permet d'étudier la structure globale de cette équation sans avoir à regarder les Bode et réponses. Ces matrices sont toutes deux diagonalisables. Les polynômes caractéristiques s'écrivent \textcolor{red}{(pas parlé avant)} :

\[
\det\left(\underline{\underline{\nabla}}f\left(\begin{matrix}0 \\ 0\end{matrix}\right)-x\underline{\underline{I}}\right) = 
\left|\begin{matrix}
-x & -3 \\
1 & -x
\end{matrix}\right| = x^2+3
\]

\[
\det\left(\underline{\underline{\nabla}}f\left(\begin{matrix}0 \\ 3\end{matrix}\right)-x\underline{\underline{I}}\right) = 
\left|\begin{matrix}
-x & 3 \\
1 & -x
\end{matrix}\right| = x^2-3
\]

Dont les racines - c'est-à-dire les valeurs propres \textcolor{red}{(pas parlé avant)} des Jacobiennes - s'écrivent :

\[
\lambda_1 = i\sqrt{3} \; , \; \lambda_2 = -i\sqrt{3}
\]

Et :

\[
\lambda_1 = \sqrt{3} \; , \; \lambda_2 = -\sqrt{3}
\]

Ces deux points d'équilibres admettent donc chacun deux sous-espaces propres, qui sont générés par des vecteurs propres \textcolor{red}{(pas parlé avant, justifier l'existence du noyau)} que nous allons maintenant calculer :

\[\boxed{
\vec{e}_{\lambda} \in \ker\left(\underline{\underline{\nabla}}f-\lambda\underline{\underline{I}}\right)
}\]

\[
\vec{e}_{\lambda} \in \ker\left(\begin{matrix}
-\lambda & -3 \\
1 & -\lambda
\end{matrix}\right)
\]

Et parce-qu'on sait qu'une matrice de déterminant nul n'est pas inversible, il y a nécessairement dépendance linéaire entre ses lignes \textcolor{red}{(à faire, éclaircir)}. On peut n'en garder qu'une des deux dans l'écriture du noyau :

\[
\vec{e}_{\lambda} \in \ker\left(\begin{matrix}
1 & -\lambda \end{matrix}\right)
\]

Qui revient finalement à proposer :

\[
\vec{e}_{\lambda} =
\left(\begin{matrix}
1 \\
\frac{1}{\lambda}
\end{matrix}\right)
\]

Ainsi, pour le premier point d'équilibre, on a :

\[
\vec{e}_{\lambda_1} =
\left(\begin{matrix}
1 \\
\frac{-i}{\sqrt{3}}
\end{matrix}\right)
\]

\[
\vec{e}_{\lambda_2} =
\left(\begin{matrix}
1 \\
\frac{i}{\sqrt{3}}
\end{matrix}\right)
\]

Et pour le second :

\[
\vec{e}_{\lambda_1} =
\left(\begin{matrix}
1 \\
\frac{1}{\sqrt{3}}
\end{matrix}\right)
\]

\[
\vec{e}_{\lambda_2} =
\left(\begin{matrix}
1 \\
\frac{-1}{\sqrt{3}}
\end{matrix}\right)
\]

\textcolor{red}{(portrait de phase, analyse locale)}

\section{Equation aux dérivées partielles}

Les équations différentielles qui viennent d'être présentés sont dites \emph{ordinaires} car leurs solutions sont fonctions d'une seule variable de $\mathbb{R}$. Or, le monde classique étant contenu dans un espace-temps quadri-dimensionnel, les formulations locales des théories physiques s'expriment dans $\mathbb{R}^4$ ce qui nous amène à introduire un nouveau type d'équations différentielles dont les solutions seront des fonctions de plusieurs variables.

\textcolor{red}{(évoquer analyse vectorielle)}

\subsection{Opérateurs différentiels}

Les seuls opérateurs des équations différentielles ordinaires sont les dérivations simple ou multiples : $\frac{d}{dx}$, $\frac{d^2}{dx^2}$, $\frac{d^n}{dx^n}$...

Comme son nom l'indique, les équations aux dérivées partielles font intervenir les dérivées par rapport à une variable particulière. Par exemple, pour une équation physique à quatre variables, on pourra écrire :

\[
\frac{\partial f}{\partial x}
\]

Pour indiquer que l'on dérive explicitement par rapport à la variable $x$. Les autres variables seront alors prises constantes.

Nous allons nous placer pour cette discussion dans un cadre non-relativiste, où temps et espace sont découplés, car la métrique particulière de l'espace-temps de Minkowski (l'essence même de la relativité) introduit une complication dans l'écriture des quadri-opérateurs qui n'est pas notre propos ici.

Le premier opérateur différentiel vraiment nouveau est donc le \emph{gradient} :

\[\boxed{
\nabla f = \left(\begin{matrix}
\frac{\partial f}{\partial x} \\
\frac{\partial f}{\partial y} \\
\frac{\partial f}{\partial z}
\end{matrix}\right)
}\]

Notez qu'ici $f$ est une grandeur scalaire (on dira même un \emph{champ} scalaire puisque c'est une fonction de l'espace et du temps), mais le gradient peut-être généralisé à des espaces tensoriels plus grandes, par exemple le gradient d'un vecteur est une matrice :

\[
\nabla \vec{f} = \nabla^\top f_x \vec{e_x}+ \nabla^\top f_y \vec{e_y} + \nabla^\top f_z \vec{e_z}
\]

\[\boxed{
\nabla \vec{f} =
\left(\begin{matrix}
\frac{\partial f_x}{\partial x} & \frac{\partial f_x}{\partial y} & \frac{\partial f_x}{\partial z} \\
\frac{\partial f_y}{\partial x} & \frac{\partial f_y}{\partial y} & \frac{\partial f_y}{\partial z}\\
\frac{\partial f_z}{\partial x} & \frac{\partial f_z}{\partial y} & \frac{\partial f_z}{\partial z}
\end{matrix}\right)
}\]

Cette matrice contient toute l'information sur les variations locales du \emph{champ vectoriel} $\vec{f}(x,y,z)$. Néanmoins, nous verrons qu'en physique il est souvent pertinent d'extraire de cette matrice deux informations particulières, la première étant la \emph{divergence} :

\[\boxed{
\nabla \cdot \vec{f} = \mathrm{Tr}(\nabla \vec{f}) = \frac{\partial f_x}{\partial x} + \frac{\partial f_y}{\partial y} + \frac{\partial f_z}{\partial z}
}\]

Et la seconde le \emph{rotationnel} \textcolor{red}{(trouver le lien avec la matrice jacobienne)} :

\[\boxed{
\nabla \times \vec{f} = \left(\begin{matrix}
\frac{\partial f_z}{\partial y} - \frac{\partial f_y}{\partial z} \\
\frac{\partial f_x}{\partial z} - \frac{\partial f_z}{\partial x} \\
\frac{\partial f_y}{\partial x} - \frac{\partial f_y}{\partial x}
\end{matrix}\right)
}\]

Qui sont tous deux des opérateurs différentiels à l'interprétation complexe \textcolor{red}{(trouver un moyen de les illustrer)}.

C'est tout pour les opérateurs d'ordre $1$, mais on peut aussi faire mention des \emph{laplaciens} qui sont des opérateurs d'ordre $2$ :

\[\boxed{
\Delta f = \nabla \cdot \nabla f
}\]

\[\boxed{
\Delta \vec{f} = \nabla (\nabla \cdot \vec{f}) - \nabla \times \nabla \times \vec{f}
}\]

Il existe en fait deux laplaciens, le premier s'appliquant sur un champ scalaire et le second sur un champ vectoriel.

\subsection{Théorème de Green-Riemann}

Le théorème de Green-Riemann est la pierre angulaire de toutes les tentatives de résolution analytique d'équations aux dérivées partielles que l'on est à même d'imaginer. Il permet leur considération globale, exactement au même titre que l'intégration classique.

Soit un \emph{domaine} $\Omega$ muni d'un contour $\partial \Omega$ ; le concept de domaine étant la généralisation dans un espace vectoriel de la notion d'intervalle. Alors on a pour une fonction $f$ :

\[\boxed{
\int_\Omega \frac{\partial f}{\partial x_i} \; d\Omega = \oint_{\partial \Omega} f (\vec{n} \cdot \vec{e_i}) \;  d(\partial \Omega)
}\]

Avec $\vec{n}$ la normale sortante à $\partial \Omega$ au point qu'on considère. Le rond dans l'intégrale de droite n'est rien de plus qu'un ornement exprimant le caractère \emph{fermé} du contour $\partial\Omega$.

On comprend mieux ce théorème en se plaçant en dimension $1$, où il devient une évidence :

\[
\int_a^b \frac{df}{dx} dx = f(b) - f(a)
\]

Le théorème de Green-Riemann admet plusieurs variantes suivant qu'on s'intéresse à l'un ou à l'autre des opérateurs à notre disposition, que nous voyons maintenant.

\subsubsection{Théorème de Green-Ostrogradski}

On peut commencer par donner un premier résultat sur la divergence :

\[
\int_\Omega \nabla \cdot \vec{f} \; d\Omega = \int_\Omega \sum_i \frac{\partial f_i}{\partial x_i} \; d\Omega
\]

\[
\int_\Omega \nabla \cdot \vec{f} \; d\Omega = \sum_i \int_\Omega  \frac{\partial f_i}{\partial x_i} \; d\Omega
\]

\[
\int_\Omega \nabla \cdot \vec{f} \; d\Omega = \sum_i \oint_{\partial \Omega} f_i (\vec{n} \cdot \vec{e_i}) \;  d(\partial \Omega)
\]

\[
\int_\Omega \nabla \cdot \vec{f} \; d\Omega = \oint_{\partial \Omega} \sum_i f_i \vec{e_i} \cdot \vec{n} \;  d(\partial \Omega)
\]

\[\boxed{
\int_\Omega \nabla \cdot \vec{f} \; d\Omega = \oint_{\partial \Omega} \vec{f} \cdot \vec{n} \;  d(\partial \Omega)
}\]

Théorème qui sera - tout comme le suivant - abondamment utilisé pour la résolution des problêmes d'électromagnétisme.

\subsubsection{Théorème de Stokes}

Dans l'espace, on a également un théorème sur le rotationnel :

\[
\int_\Omega \nabla \times \vec{f} \; d\Omega 
= \int_\Omega \sum_{i \in \frac{\mathbb{Z}}{3\mathbb{Z}}} \left( \frac{\partial f_{i+2}}{\partial x_{i+1}} - \frac{\partial f_{i+1}}{\partial x_{i+2}} \right) \vec{e_i} \; d\Omega 
\]

\[
\int_\Omega \nabla \times \vec{f} \; d\Omega 
= \sum_{i \in \frac{\mathbb{Z}}{3\mathbb{Z}}} \left( \int_\Omega \frac{\partial f_{i+2}}{\partial x_{i+1}} \; d\Omega - \int_\Omega \frac{\partial f_{i+1}}{\partial x_{i+2}}  \; d\Omega \right) \vec{e_i}
\]

\[
\int_\Omega \nabla \times \vec{f} \; d\Omega 
= \sum_{i \in \frac{\mathbb{Z}}{3\mathbb{Z}}} \left( \oint_{\partial \Omega} f_{i+2} (\vec{n} \cdot \vec{e_{i+1}}) \;  d(\partial \Omega) - \oint_{\partial \Omega} f_{i+1} (\vec{n} \cdot \vec{e_{i+2}}) \;  d(\partial \Omega) \right) \vec{e_i}
\]

\[
\int_\Omega \nabla \times \vec{f} \; d\Omega 
= \oint_{\partial \Omega} \sum_{i \in \frac{\mathbb{Z}}{3\mathbb{Z}}} \left( f_{i+2} (\vec{n} \cdot \vec{e_{i+1}}) \;  - f_{i+1} (\vec{n} \cdot \vec{e_{i+2}}) \right) \vec{e_i} \;  d(\partial \Omega) 
\]

\[\boxed{
\int_\Omega \nabla \times \vec{f} \; d\Omega = \oint_{\partial \Omega} \vec{f} \times \vec{n} \;  d(\partial \Omega)
}\]

Si maintenant l'on se place sur un domaine $\Omega$ surfacique :

\textcolor{red}{(démonstration du théorème de Stokes)}

\subsection{Formulation faible}

Une équation aux dérivées partielles est donc une équation reliant une fonction - à valeurs scalaires ou vectorielles - à ses dérivées partielles, éventuellement par le biais des opérateurs qui viennent d'être présentés.

Celles-ci parsemant toute la physique, le développement de méthodes de résolution efficaces est un enjeu majeur pour toute l'ingénierie. Ce qui suit est une introduction au formalisme mathématique qui nous permettra ultérieurement de présenter la \emph{méthode des éléments finis} \textcolor{red}{(pas fait)}, seule à même aujourd'hui de permettre la simulation fine de systèmes physiques ; les solutions analytiques étant rares et complexes.

La \emph{formulation variationnelle} d'une équation aux dérivées partielles est :

\textcolor{red}{(compléter)}

\textcolor{red}{(espaces de Sobolev)}

\chapter{Probabilités \& Statistiques}

\section{Probabilités}

\subsection{Espaces de probabilité}

Une \emph{expérience aléatoire} est définie par l'ensemble $\Omega$ de ses issues possibles (les résultats d'un jet de dé par exemple) et par les \emph{probabilités} associées à chacune des issues.

Dans le cas continu (issues à valeurs dans $\mathbb{R}$ ou dans un ensemble en bijection avec $\mathbb{R}$), c'est à une densité de probabilité et non à une probabilité même que nous associons chacune des issues.

On dit qu'un \emph{évènement} est un élément de $\mathcal{P}(\Omega)$ (l'ensemble des sous-ensembles de $\Omega$, ou encore l'ensemble des réunions d'issues), au sens où l'union de deux issues est l'évènement représentant la réalisation d'une de ces deux issues.

Par exemple, pour le jet d'un dé, l'évènement $\{2;4;6\}$ est l'évènement correspondant à l'obtention d'un numéro pair.

Deux évènements sont dits \emph{disjoints} si leur intersection est nulle, c'est à dire s'ils n'ont aucune issue en commun.

Une façon souvent employée pour partitionner un ensemble $\Omega$ consiste à associer un nombre $x$ à chacune des issues $\omega$, et de considérer comme un évènement l'ensemble des issues permettant d'obtenir un même nombre.

On parle alors de la \emph{variable aléatoire} $X : \omega \rightarrow x$. Les évènements sont alors nécessairement disjoints.

Par exemple, le jet de $n$ dés peut-être représenté par une suite de valeurs dans $\{1,2,3,4,5,6\}$. La loi de probabilité naturelle à définir est la somme des termes de la suite (le résultat du jet).

Tout l'enjeu de la théorie des probabilités est d'attribuer à chaque évènement $A$ une probabilité $\mathbb{P}(A) \in [0;1]$, définie mathématiquement par :

\[\boxed{
\left\{\begin{matrix}
\mathbb{P}(\Omega) = 1 \\
\mathbb{P}(\emptyset) = 0 \\
\forall A, \; \mathbb{P}(A) = 1 - \mathbb{P}(\Omega \setminus A) \\
\forall (A,B), \; \mathbb{P}(A\cup B) = \mathbb{P}(A) + \mathbb{P}(B) - \mathbb{P}(A\cap B)
\end{matrix}\right.
}\]

On fera intervenir la probabilité conditionnelle de \emph{$A$ sachant $B$} définie par :

\[\boxed{
\mathbb{P}(A|B) = \frac{\mathbb{P}(A\cap B)}{\mathbb{P}(B)}
}\]

Qui nous permet d'écrire facilement la \emph{formule de Bayes}, laquelle nous rappellera l'écriture de la décomposition d'un vecteur dans une base. En effet, Pour une partition $\phi = (\phi_1, \phi_2,...)$ de l'univers $\Omega$, on a :

\[\boxed{
\mathbb{P}(A) = \sum_i \mathbb{P}(A|\phi_i)\mathbb{P}(\phi_i)
}\]

On retiendra donc que $\mathbb{P}(A|B)$ est d'une certaine façon le produit scalaire de $\mathbb{P}(A)$ par $\mathbb{P}(B)$ (d'où la notation proche de celle du Bra-Ket) \textcolor{red}{(sous une métrique particulières alors car ce produit n'est pas commutatif)}.

Enfin on dira que deux évènements sont indépendants ssi :

\[\boxed{
\mathbb{P}(A|B)=\mathbb{P}(A)
}\]

Auquel cas $\mathbb{P}(A\cap B) = \mathbb{P}(A)\mathbb{P}(B)$.

Par exemple, pour un jet de deux dés les évènements \emph{$\{2;4;6\}$ pour le premier jet} et \emph{trouver $7$ pour résultat} sont des évènements indépendants car :

\[
\mathbb{P}(x=7|\{2;4;6\})=\mathbb{P}(x=7|\{2\}) + \mathbb{P}(x=7|\{4\}) + \mathbb{P}(x=7|\{6\})
\]

\[
\mathbb{P}(x=7|\{2;4;6\})=\mathbb{P}(\{5\}) + \mathbb{P}(\{3\}) + \mathbb{P}(\{1\})
\]

\[
\mathbb{P}(x=7|\{2;4;6\})=\frac{1}{6} +\frac{1}{6} + \frac{1}{6}
\]

\[
\mathbb{P}(x=7|\{2;4;6\})=\frac{1}{2}
\]

Qui comme nous le verrons est bien égal à $\mathbb{P}(x=7)$ \textcolor{red}{(pas fait)}.


Alors que \emph{$\{2;4;6\}$ pour le premier jet} et \emph{trouver $6$ pour résultat} ne le sont pas :

\[
\mathbb{P}(x=6|\{2;4;6\})=\mathbb{P}(x=6|\{2\}) + \mathbb{P}(x=6|\{4\}) + \mathbb{P}(x=6|\{6\})
\]

Mais la dernière probabilité est nulle ! Si j'obtiens un six au premier jet, je ferai forcément plus de six au total :

\[
\mathbb{P}(x=6|\{2;4;6\})=\frac{1}{3}
\]

Qui n'est pas égal à $\mathbb{P}(x=6)$, comme nous le verrons également \textcolor{red}{(pas fait)}.

\subsection{Lois de probabilité}

Une loi de probabilité $\mathbb{P}(X)$ satisfait la propriété suivante :

\[\boxed{
\sum_{x\in X(\Omega)} \mathbb{P}(x) = 1
}\]

Si $X(\Omega)$ est un ensemble ordonné (typiquement $\mathbb{N}$ ou $\mathbb{R}$), on définira sa fonction de répartition :

\[\boxed{
\mathcal{F}_k : x \rightarrow \sum_{x=-\infty}^k \mathbb{P}(x) 
}\]

De sorte que :

\[
\mathcal{F}_n = 1
\]

\subsubsection{Loi binomiale}

La loi de probabilité la plus élémentaire - mais pas la plus simple - est la loi binomiale au coeur des probabilités discrètes :

\[\boxed{
\mathbb{P}_{\mathcal{B}(n,p)} : k \rightarrow C_n^k (1-p)^{n-k} p^k
}\]

Pour montrer qu'elle satisfait la propriété sus-citée, on peut remarquer que sa fonction de répartition jusque $n$ est un binôme de Newton :

\[
\mathcal{F}_n =\sum_{k=0}^n C_n^k (1-p)^{n-k} p^k = (1-p+p)^n = 1
\]

Elle est donne la probabilité pour que $k$ piles soient tirés pour $n$ jets de pièce si la pièce est déséquilibrée et que la probabilité d'obtention de pile vaut $p$. Si la pièce est équilibrée, on a $p=\frac{1}{2}$ et :

\[
\mathbb{P}_{\mathcal{B}(n,\frac{1}{2})}(k) = C_n^k 2^{-n}
\]

Cette formule demande à être comprise. Si l'on jette $n$ pièces et alors qu'il y a pour chaque pièce deux issues, il y a $\#\Omega = 2^n$ issues possibles.

Maintenant, combien y-a-t'il de façons d'obtenir $k$ piles ? On se convaincra que ce problême revient à compter les combinaisons de $n$ objets pour qu'il y en ai $k$ d'un coté et $n-k$ de l'autre. Les probabilités étant uniforme, on divise donc naturellement $C_n^k$ par $\#\Omega$.

\subsubsection{Loi géométrique}

Dans la continuité de la loi binomiale, la loi géométrique détermine la probabilité d'obtention du premier pile au $k$-ième lancé :

\[\boxed{
\mathbb{P}_{\mathcal{G}(p)} : k \rightarrow (1-p)^{k-1} p
}\]

Cette loi se comprend en remarquant que $\mathbb{P}_{\mathcal{G}(p)}(k)$ doit être le produit de la probabilité $p$ de tirer pile après que $k-1$ faces aient été tirés.

\subsubsection{Loi hypergéométrique}

\textcolor{red}{(compléter)}

\subsubsection{Loi de Poisson}

Les trois lois qui précèdent constituent les fondements des probabilités. Toutes les autres sont des approximations de celles-ci sous certaines hypothèse. Par exemple, la \emph{loi de Poisson} est ce en quoi se réduit la loi binomiale pour $p$ petit (évènement rare). On réduira les deux degrés de liberté de la loi à un seul $\lambda=np$.

\[\boxed{
\mathbb{P}_{\mathcal{P}(\lambda)} : k \rightarrow \frac{\lambda ^k}{k!}e^{-\lambda}
}\]

\textcolor{red}{(démonstration)}

\subsubsection{Loi exponentielle}

Cette nouvelle loi est définie sur $\mathbb{R}$, et donc en termes de densité de probabilité. Elle est ce vers quoi tend la loi géométrique pour $n$ grand \textcolor{red}{(démonstration)} :

\[\boxed{
\mathbb{P}_{\mathcal{E}(\lambda)} : x \rightarrow \lambda e^{-\lambda x}
}\]

C'est à dire qu'on ne pourra plus attribuer une probabilité à un résultat $x$, mais à l'évènement correspondant à un intervalle $[a;b]$ de $\mathbb{R}$ :

\[
\mathbb{P}_{\mathcal{E}(\lambda)}(x\in I) = \int_a^b \mathbb{P}_{\mathcal{E}(\lambda)}(x) dx = \mathcal{F}(b)-\mathcal{F}(a)
\]

En l'occurence, la fonction de répartition est facile à calculer :

\[
\mathcal{F}(x) = \int_0^x \lambda e^{-\lambda t} dt
\]

\[
\mathcal{F}(x) = 1-e^{-\lambda x}
\]

\subsubsection{Loi normale}

\textcolor{red}{(compléter)}

\subsection{Moments}

Le \emph{moment d'ordre $n$} d'une loi de probabilité est donné par :

\[\boxed{
\mathbb{E}(X^n) = \sum_{k\in\Omega} k^n \mathbb{P}(X=k)
}\]

Le moment d'ordre $0$ est donc égal à $1$. Le moment d'ordre $1$ est l'\emph{espérance} :

\[\boxed{
\mathbb{E}(X) = \sum_{k\in\Omega} k \mathbb{P}(X=k)
}\]

Elle vaut pour les lois précédentes :

\[
\left\{ \begin{matrix}
\mathbb{E}_{\mathcal{B}(n,p)}(X) = np \\
\mathbb{E}_{\mathcal{G}(p)}(X) = \frac{1}{p} \\
\mathbb{E}_{\mathcal{P}(\lambda)}(X) = \lambda \\
\mathbb{E}_{\mathcal{E}(\lambda)}(X) = \frac{1}{\lambda} \\
\mathbb{E}_{\mathcal{N}(\mu,\sigma)}(X) = \mu
\end{matrix}\right.
\]

On la transpose en statistique à l'idée de \emph{moyenne}, au sens où si j'effectue suffisament de fois l'expérience la moyenne tend vers l'espérance.

Le moment d'ordre $2$ est le sujet le plus intéressant de ce chapitre. Il est une grandeur qui se transpose dans toutes sortes de situations, en traitement du signal, en mécanique du solide ou en électricité pour ne citer que les plus évidentes.

Le moment d'ordre $2$ a un vrai sens physique émergent de l'intervention des probabilités en mécanique quantique. :

\[\boxed{
\mathbb{E}(X^2) = \sum_{k\in\Omega} k^2 \mathbb{P}(X=k)
}\]

Le théorème de Huygens nous dit que lui-même soustrait de l'espérance au carré satisfait la définition de la \emph{variance} :


\[\boxed{
var(X) = \mathbb{E}\left(\left(X-\mathbb{E}(X)\right)^2\right)
}\]

En effet :

\[
var(X) = \mathbb{E}\left(X^2-2X\mathbb{E}(X)+\mathbb{E}^2(X)\right)
\]

Par linéarité de l'espérance (qui pour rappel est une constante) :

\[
var(X) = \mathbb{E}(X^2)-2\mathbb{E}(X)\mathbb{E}(X)+\mathbb{E}^2(X)
\]

\[\boxed{
var(X) = \mathbb{E}(X^2)-\mathbb{E}^2(X)
}\]

Sa racine est \emph{l'écart-type}. Il permet de quantifier la \emph{dispersion} d'une loi de probabilité autour de sa valeur moyenne, et ce dans l'unité physique de celle-ci.

\textcolor{red}{(Ecart-type de la loi uniforme)}

\subsection{Théorème central limite}

\subsection{Convolution de lois de probabilités}

\part{Fondements de la physique}
\setcounter{chapter}{0}

\part{Mécanique}
\setcounter{chapter}{0}

\part{Electromagnétisme}
\setcounter{chapter}{0}

\chapter{Fondements de l'électromagnétisme}

\section{Formulation en potentiels}

\subsection{Equations de Maxwell}

\subsubsection{Formulation relativiste}

Les équations de Maxwell permettent le calcul des champs électromagnétiques en présence de charge électrique.

Soit le \emph{quadrivecteur courant} défini dans tout l'espace, et décrivant la répartition et les mouvements des charges électriques :

\[\boxed{
j^\mu(x^\nu) = \left( \begin{matrix} \rho(x,y,z,t)c \\ j_x(x,y,z,t) \\ j_y(x,y,z,t) \\ j_z(x,y,z,t) \end{matrix} \right)
}\]

S'interroger sur la raison pour laquelle la composante temporelle doit être distinguée des composantes spatiales revient à se demander pourquoi le temps est de nature différente de l'espace (ceci ne serait-ce parceque le principe de causalité s'applique seulement au premier) ; ce qui est une question profonde qu'affectionnent particulièrement les physiciens.

L'électromagnétisme stipule que ce quadrivecteur est la source du \emph{quadrivecteur potentiel} :

\[\boxed{
A^\mu(x^\nu) = \left( \begin{matrix} \phi(x,y,z,t)/c \\ A_x(x,y,z,t) \\ A_y(x,y,z,t) \\ A_z(x,y,z,t) \end{matrix} \right)
}\]

Dans la suite, on se placera dans le système d'unité où la vitesse de la lumière $c = 1$, qui est certainement le système d'unité naturel à considérer une fois qu'on a conscience des implications de la relativité.

L'\emph{électrodynamique quantique} se concrétise dans le monde classique au travers des équations de Maxwell covariantes :

\[\boxed{
\left\{\begin{matrix}
\partial_{\mu}F_{\nu\lambda}
 +  \partial_{\nu}F_{\lambda\mu}  +  \partial_{\lambda}F_{\mu\nu}  =  0 \\
\partial_{\mu}F^{\mu\nu}  =  -  \mu_{0}  j^{\nu}
\end{matrix}\right.
}\]

Où $F^{\mu\nu}$ est le tenseur de Maxwell :

\[\boxed{
F_{\mu\nu}  =  \partial_{\mu} A_{\nu} - \partial_{\nu} A_{\mu}
}\]

Qu'on peut tenter de préciser par :

\[
F_{\mu\nu}  =  \left( \begin{matrix}  
0 & \left(\frac{\partial \vec{A}}{\partial t} - \nabla\phi \right)^\top   \\
\nabla\phi - \frac{\partial \vec{A}}{\partial t} & \nabla\vec{A}-(\nabla\vec{A})^\top
\end{matrix} \right)
\]

Ce qui n'est pas très éclairant, mais révèle l'existence d'une structure matricielle particulière propre à $F_{\mu\nu}$.

Notamment, le tenseur de Maxwell est antisymétrique :

\[
F_{\mu\nu}  =  \partial_{\mu} A_{\nu} - \partial_{\nu} A_{\mu} = - ( \partial_{\nu} A_{\mu} - \partial_{\mu} A_{\nu} ) = - F_{\nu\mu}
\]

Cette formulation nécessite évidemment une étude approfondie, puisqu'elle constitue l'essence de l'électromagnétisme classique. Cf \emph{dérivée extérieure} et \emph{dualité de Hodge}.

\subsubsection{Conservation de la charge électrique}

L'équation de Maxwell \emph{avec sources} implique la divergence nulle du quadricourant :

\[ 
\partial_{\mu}F^{\mu\nu}  =  -  \mu_{0}  j^{\nu}
\]

\[
\partial_{\nu} \left(\partial_{\mu}F^{\mu\nu}\right)  = \frac{1}{2}\partial_{\nu} \partial_{\mu}\left(F^{\mu\nu}-F^{\nu\mu}\right)  =  0  =  -  \mu_{0}  \partial_{\nu} j^{\nu} 
\]

\[
\quad \partial_{\nu} j^{\nu}  =  0 
\]

Cette expression est une équation de conservation :

\[\boxed{
\frac{\partial \rho}{\partial t} + \nabla \cdot \vec{j} = 0
}\]

Qui rappelle les équations de conservation de masse ou de quantité de matière qu'on trouve en mécanique des milieux continus. Pourtant, dans ce cas-ci l'équation de conservation est déduite des équations de Maxwell ; alors que d'habitude elle est plutôt un postulat émanante de l'absence de source dans un problême.

\subsubsection{Invariance de jauge}

On remarque qu'étant donnée la structure de ces équations où seul le tenseur de Maxwell ainsi construit apparaît, on peut noter que le quadripotentiel est défini à une jauge près. C'est à dire qu'une transformation :

\[
A'_{\mu}  =  A_{\mu} - \partial_{\mu} f
\]

Laisse invariant le tenseur de Maxwell :

\[
F'_{\mu\nu}  =  \partial_{\mu} A'_{\nu} - \partial_{\nu} A'_{\mu}
\]

\[
F'_{\mu\nu}  =  \partial_{\mu} A_{\nu} + \partial_{\mu} \partial_{\nu} f - \partial_{\nu} A_{\mu} - \partial_{\nu} \partial_{\mu} f
\]

\[
F'_{\mu\nu}  =  \partial_{\mu} A_{\nu} - \partial_{\nu} A_{\mu} = F_{\mu\nu}
\]

Dans ces conditions, on choisira communément la \emph{jauge de Lorenz} où la divergence du quadripotentiel $\partial_{\mu} A^{\mu}$ est nulle :

\[\boxed{
\frac{\partial\phi}{\partial t}  + \nabla \cdot \vec{A} = 0
}\]

Ce qui permet d'écrire une forme simple (dite \emph{de Poisson}) de l'équation de Maxwell \emph{avec sources} :

\[ 
\partial_{\mu}F^{\mu\nu}  =  -  \mu_{0}  j^{\nu}
\]

\[ 
\partial_{\mu} (\partial^{\mu} A^{\nu} - \partial^{\nu} A^{\mu})
= -  \mu_{0}  j^{\nu}
\]

\[\boxed{
\partial_{\mu} \partial^{\mu} A^{\nu}
= -  \mu_{0}  j^{\nu}
}\]

\subsection{Interaction électromécanique}

L'évolution du système est décrit par le couplage des deux quadrivecteurs précédemment évoqués. La densité d'énergie potentielle s'écrit simplement \textcolor{red}{(justification)} :

\[
w = j^\mu A_{\mu}
\]

Ou en explicitant la différence qu'on peut faire entre le temps et l'espace :

\[\boxed{
w = \rho \phi + \vec{j} \cdot \vec{A}
}\]

On peut alors invoquer une relation vue en mécanique \textcolor{red}{(pas fait)} pour arriver finalement à l'expression de la quadriforce :

\[
f^\mu = \partial^\mu w
\]

\textcolor{red}{(compléter)}

\[
f^\mu = F^\mu_{\;\;\nu} j^\nu
\]

Qu'on réecrit comme toujours en séparant le temps de l'espace :

\[
f^\mu = \eta^{\mu\lambda} F_{\lambda \nu} j^\nu
\]

\[
f^\mu = \left( \begin{matrix} -1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1 \end{matrix} \right) 
\left( \begin{matrix}  
0 & \left(\frac{\partial \vec{A}}{\partial t} - \nabla\phi \right)^\top   \\
\nabla\phi - \frac{\partial \vec{A}}{\partial t} & \nabla\vec{A}-(\nabla\vec{A})^\top
\end{matrix} \right)
\left( \begin{matrix} \rho c \\ \vec{j} \end{matrix} \right)
\]

\[
f^\mu = \left( \begin{matrix} \left( \frac{\partial \vec{A}}{\partial t} - \nabla\phi \right) \cdot \vec{j} 
\\ \left( \nabla\phi - \frac{\partial \vec{A}}{\partial t} \right) \rho + \left( \nabla\vec{A}-(\nabla\vec{A})^\top \right) \vec{j} \end{matrix} \right)
\]

On a ainsi un couplage entre les équations de champs et les équations de mouvements des sources, même si sa formulation est tout à fait inintelligible en l'état.

On peut montrer \textcolor{red}{(à faire)} que poser les \emph{champ électrique} :

\[\boxed{
\vec{E} = - \frac{\partial \vec{A}}{\partial t} - \nabla\phi
}\]

Et \emph{champ magnétique} :

\[\boxed{
\vec{B} = \nabla \times \vec{A}
}\]

Revient à écrire la quadriforce comme :

\[\boxed{
\left( \begin{matrix}  p 
\\ \vec{f} \end{matrix} \right) = \left( \begin{matrix} - \vec{j} \cdot \vec{E}
\\ \rho \vec{E} + \vec{j} \times \vec{B} \end{matrix} \right)
}\]

Une conséquence intéressante est qu'une charge évoluant dans un champ purement magnétique ne travaille pas.
 
\section{Formulation en champs électriques et magnétiques}

J'ai fais le choix de continuer d'utiliser les formulations relativistes, à base de quadri-vecteur. Elles permettent de considérer à la fois les champs électriques et magnétiques ; car ceux-ci sont de même nature en relativité comme nous venons de le voir. Il se concrétisent différement dans notre monde et leur union est difficile à appréhender, mais on peut dire que le champ magnétique n'est à l'espace rien de plus que ce que le champ électrique est au temps.

\subsection{Equations de Maxwell}

Je rappelle les équations couplées de l'électromagnétisme, en se plaçant dans la jauge de Lorentz :

\[
\partial_{\mu}F_{\nu\lambda}
 +  \partial_{\nu}F_{\lambda\mu}  +  \partial_{\lambda}F_{\mu\nu}  =  0
\]

\[ 
\partial_{\mu} \partial^{\mu} A^{\nu}
= -  \mu_{0}  j^{\nu}
\]

\subsubsection{Equations de Maxwell avec sources}

Commençons par séparer les dérivées temporelles des dérivées spatiales. Pour ce faire, nous alons utiliser l'indice de sommation $\alpha$ qui va par convention de $1$ à $3$, ignorant la composante temporelle :

\[ 
\partial_{t} \partial^{t} A^{\nu}+\partial_{\alpha} \partial^{\alpha} A^{\nu}
= -  \mu_{0}  j^{\nu}
\]

Cette double dérivée sommée sur les composantes spatiales se rapproche d'un laplacien vectoriel au sens tridimensionnel, bien que s'appliquant sur un quadrivecteur. Pour comprendre tout à fait bien ce qu'il se passe, nous allons expliciter les composantes de cet opérateur :

\[
\partial_{\mu} \partial^{\nu}
= -\frac{\partial^2}{\partial t^2} + \Delta
\]

Le signe moins provenant de la métrique de Lorentz \textcolor{red}{(pas parlé avant)}. Pour lever toute ambiguitée sur son sens, on reécrira :

\[ 
\partial_{\mu} \partial^{\mu} \left( \begin{matrix}  \phi \\ A_x \\ A_y \\ A_z \end{matrix} \right)
= - \mu_{0}  \left( \begin{matrix} \rho \\j_x \\ j_y \\ j_z \end{matrix} \right)
\]

Sous la forme :

\[ 
\left( \begin{matrix} 
-\frac{\partial^2 \phi}{\partial t^2} + \Delta \phi \\ 
-\frac{\partial^2 A_x}{\partial t^2} + \Delta A_x \\ 
-\frac{\partial^2 A_y}{\partial t^2} + \Delta A_y \\ 
-\frac{\partial^2 A_z}{\partial t^2} + \Delta A_z \end{matrix} \right)
= - \mu_{0}  \left( \begin{matrix} \rho \\j_x \\ j_y \\ j_z \end{matrix} \right)
\]

Ou plus intelligible, en introduisant le laplacien vectoriel :

\[\boxed{
\left( \begin{matrix}  -\frac{\partial^2\phi}{\partial t^2} + \Delta \phi \\ 
-\frac{\partial^2 \vec{A}}{\partial t^2} + \Delta \vec{A} \end{matrix} \right)
= - \mu_{0}  \left( \begin{matrix} \rho \\ \vec{j} \end{matrix} \right)
}\]

Il est donc question de deux champs de potentiels, le premier scalaire et le second vectoriel, dont les sources respectives sont la densité de charge et leurs courants de déplacement.

On peut expliciter les laplaciens :

\[ 
\left( \begin{matrix}  -\frac{\partial^2\phi}{\partial t^2} + \nabla \cdot \nabla \phi \\ 
-\frac{\partial^2\vec{A}}{\partial t^2} +\nabla ( \nabla  \cdot \vec{A} ) - \nabla \times \nabla  \times \vec{A} \end{matrix} \right)
= - \mu_{0}  \left( \begin{matrix} \rho \\ \vec{j} \end{matrix} \right)
\]

En faisant intervenir la jauge de Lorenz $\frac{\partial\phi}{\partial t}  + \nabla \cdot \vec{A} = 0$:

\[
\left( \begin{matrix}
\frac{\partial}{\partial t}\left(\nabla \cdot \vec{A}\right) + \nabla \cdot \nabla \phi \\ 
- \frac{\partial^2\vec{A}}{\partial t^2} - \nabla \left( \frac{\partial\phi}{\partial t} \right) - \nabla \times \nabla  \times \vec{A} \end{matrix} \right)
= - \mu_{0}  \left( \begin{matrix} \rho \\ \vec{j} \end{matrix} \right)
\]

En utilisant les définitions de $\vec{E} = - \frac{\partial\vec{A}}{\partial t} - \nabla\phi$ et $\vec{B} = \nabla \times \vec{A}$ :

\[ 
\left( \begin{matrix}  
\frac{\partial}{\partial t}\left(\nabla \cdot \vec{A}\right) + \nabla \cdot \left(-\frac{\partial\vec{A}}{\partial t}-\vec{E}\right) \\ 
\frac{\partial\vec{E}}{\partial t} - \nabla \times \vec{B} \end{matrix} \right)
= - \mu_{0}  \left( \begin{matrix} \rho \\ \vec{j} \end{matrix} \right)
\]

Ou encore, qui met en évidence les deux champs préalablement introduits :

\[\boxed{
\left( \begin{matrix}  \nabla \cdot \vec{E} \\ \nabla \times \vec{B} \end{matrix} \right)
= \left( \begin{matrix} \mu_{0} \rho \\ \mu_{0}\vec{j} + \frac{\partial\vec{E}}{\partial t} \end{matrix} \right)
}\]

Cette dernière forme est la forme usuelle des équations de Maxwell \emph{avec sources} en régime quasi-statique.

Ces deux champs ont le bon goût d'être - d'une certaine façon - plus \emph{uniques} que les potentiels car l'invariance de jauge ne s'applique pas à eux \textcolor{red}{(à éclaircir)}. C'est pourquoi on a longtemps pensé que ces champs étaient plus \emph{physiques} que les potentiels.

La physique quantique nous a depuis donné tort. Cf. \emph{Effet Aharonov-Bohm}.

\subsubsection{Equations de Maxwell sans source}

On peut alors s'intéresser à l'écriture de l'autre objet essentiel de l'électromagnétisme relativiste en considérant les champs électriques et magnétiques : le tenseur de Maxwell.

\[
F_{\mu\nu}  =  \partial_{\mu} A_{\nu} - \partial_{\nu} A_{\mu}
\]

Ou sous sa forme matricielle :

\[
F_{\mu\nu}  =  \left( \begin{matrix}  
0 & -\partial_{t} A_{x}-\partial_{x}\phi & -\partial_{t} A_{y}-\partial_{y}\phi & -\partial_{t} A_{z}-\partial_{z}\phi \\
\partial_{x}\phi+\partial_{t} A_{x} & 0 & \partial_{x} A_{y} - \partial_{y} A_{x} & \partial_{x} A_{z} - \partial_{z} A_{x} \\
\partial_{y}\phi+\partial_{t} A_{y} & \partial_{y} A_{x} - \partial_{x} A_{y} & 0 & \partial_{y} A_{z} - \partial_{z} A_{y}\\
\partial_{z}\phi+\partial_{t} A_{z} & \partial_{z} A_{x} - \partial_{x} A_{z}  & \partial_{z} A_{y} - \partial_{y} A_{z} & 0\end{matrix} \right)
\]

Je vous laisse vérifier qu'il peut-être réecrit sous une forme plus intelligible :

\[\boxed{
F_{\mu\nu}  =  \left( \begin{matrix}  
0 & E_x & E_y & E_z \\
- E_x & 0 & B_z & - B_y \\
- E_y & - B_z & 0 & B_x \\
- E_z & B_y & - B_x & 0\end{matrix} \right)
}\]

C'est alors qu'il devient intéressant de voir la forme que prend l'équation de Maxwell \emph{sans source} :

\[
\partial_{\mu}F_{\nu\lambda}
 +  \partial_{\nu}F_{\lambda\mu}  +  \partial_{\lambda}F_{\mu\nu}  =  0
\]

Il s'agit en fait de 16 équations, dont certaines sont redondantes voir identiquement nulles. La façon intelligente de traiter ce problême consiste à remarquer la symétrie de permutation, et de s'appuyer sur celle-ci pour se contenter d'affecter successivement 0, 1, 2 puis 3 indices à la spatialité.

\begin{itemize}
\item Cas où aucun indice n'est spatial, $F_{tt}$ étant nulle on ne peut rien en tirer.
\item Cas où un seul indice est spatial :
\[
\partial_{t}F_{t\alpha}
 +  \partial_{t}F_{\alpha t}  +  \partial_{\alpha}F_{tt}  =  0
\]
Par l'antisymétrie, cette expression est encore une fois identiquement nulle.
\item Cas où deux indices sont spatiaux :
\[
\partial_{t}F_{\alpha\beta}
 +  \partial_{\alpha}F_{\beta t}  +  \partial_{\beta}F_{t\alpha}  =  0
\]

On reconnait que :

\[
\partial_{t}F_{\alpha\beta} + \partial_{\alpha}E_{\beta}  -  \partial_{\beta}E_{\alpha}  =  0
\]

L'antisymétrie nous donne alors trois équations :

\[ 
\left\{ \begin{matrix}
\frac{\partial B_z}{\partial t} + \frac{\partial E_y}{\partial x} - \frac{\partial E_x}{\partial y} = 0 \\
-\frac{\partial B_y}{\partial t} + \frac{\partial E_z}{\partial x} - \frac{\partial E_x}{\partial z} = 0 \\
\frac{\partial B_x}{\partial t} + \frac{\partial E_z}{\partial y} - \frac{\partial E_y}{\partial z} = 0
\end{matrix} \right.
\]

Ou sous une forme plus compacte :

\[\boxed{
\nabla \times \vec{E} = - \frac{\partial\vec{B}}{\partial t}
}\]

\item Cas où tous les indices sont spatiaux :
On fini par le cas les plus délicat. On peut déjà s'épargner l'étude des sous-cas où certains indices sont similaires, pour les mêmes raisons liées à l'antisymétricité que précédement.

Il ne reste alors plus qu'à voir :

\[
\partial_{\gamma}F_{\alpha\beta}
 +  \partial_{\alpha}F_{\beta\gamma}  +  \partial_{\beta}F_{\gamma\alpha}  =  0
\]

Qu'on fera l'effort d'expliciter puisque cette forme ne semble en faite correspondre qu'à deux équations (suivant le groupes de permutation liés à $(\alpha,\beta,\gamma)$ qu'on choisi) :

\[
\partial_{z}F_{xy}
 +  \partial_{x}F_{yz}  +  \partial_{y}F_{zx}  =  0
\]

Et :

\[
\partial_{z}F_{yx}
 +  \partial_{y}F_{xz}  +  \partial_{x}F_{zy}  =  0
\]

Mais par antisymétrie on voit tout de suite que ces deux équations sont redondantes. On finira alors par écrire :

\[
\partial_{z}Bz
 +  \partial_{x}Bx  +  \partial_{y}By  =  0
\]

Ou sous sa forme compacte :

\[\boxed{
\nabla \cdot \vec{B} = 0
}\]

\end{itemize}

Pour résumer, les équations de Maxwell peuvent s'écire :

\[\boxed{
\left\{ \begin{matrix}  \nabla \cdot \vec{E} = \mu_0 \rho  \\
\nabla \times \vec{E} = - \frac{\partial\vec{B}}{\partial t} \\
\nabla \cdot \vec{B} = 0 \\
\nabla \times \vec{B} = \mu_{0}\vec{j} + \frac{\partial\vec{E}}{\partial t}
\end{matrix} \right.
}\]

Usuellement, on écrit ces équations dans un système d'unité où $c \ne 1$. On introduit alors la constante $\epsilon_0$ telle que $\epsilon_0 =\frac{1}{\mu_0 c^2}$. Les équations de Maxwell s'écrivent alors :

\[\boxed{
\left\{ \begin{matrix}  \nabla \cdot \vec{E} = \frac{\rho}{\epsilon_0}  \\
\nabla \times \vec{E} = - \frac{\partial\vec{B}}{\partial t} \\
\nabla \cdot \vec{B} = 0 \\
\nabla \times \vec{B} = \mu_{0}\left(\vec{j} + \epsilon_0\frac{\partial\vec{E}}{\partial t}\right)
\end{matrix} \right.
}\]

Cette formulation malmène beaucoup moins le sens commun que celle en potentiels, à cause du rapprochement entre les champs $\vec{E}$ et $\vec{B}$ et les forces. Le couplage entre ces deux champs apporte une complication supplémentaire dans la résolution des problêmes ; complication qui vient s'ajouter à celle émanante du couplage entre champ et positions des sources et qu'on trouvait déjà en gravitation Newtonienne. 

\subsection{Interaction électromécanique}

\subsubsection{Formulation énergétique}

Lorsque nous avons introduit les champs, fût écrit :

\[
\left( \begin{matrix}  p 
\\ \vec{f} \end{matrix} \right) = \left( -\begin{matrix} \vec{j} \cdot \vec{E}
\\ \rho \vec{E} + \vec{j} \times \vec{B} \end{matrix} \right)
\]

Einstein nous dit que cette quadriforce est le quadrigradient d'une densité d'énergie :

\[
f^\mu = \partial^\mu w
\]

Cette énergie se calcule donc - à une constante près - en intégrant sur l'espace-temps :

\[
w = \int f^\mu \; dx_\mu
\]

\[
w = \int p \; dt + \int \vec{f} \cdot \vec{dx}
\]

\[
w = -\int \vec{j} \cdot \vec{E} \; dt + \int (\rho \vec{E} + \vec{j} \times \vec{B}) \cdot \vec{dx}
\]

\[
w = -\int \vec{j} \cdot \vec{E} \; dt + \int (\rho \vec{E} + \vec{j} \times \vec{B}) \cdot \vec{dx}
\]

C'est alors qu'on peut s'arranger avec les équations de Maxwell pour faire disparaître la charge et le courant à droite :

\[
w = -\int \vec{j} \cdot \vec{E} \; dt + \int \left(\epsilon_0 (\vec{\nabla} \cdot \vec{E})\cdot \vec{E} + \frac{1}{\mu_0}(\vec{\nabla} \times \vec{B})\times \vec{B}\right) \cdot \vec{dx}
\]

Le terme en $\vec{E}$ est mathématiquement de la forme $f\times \frac{df}{dx}$, qu'on se convaincra être égal à $\frac{1}{2} \frac{df^2}{dx}$. On développe également le double produit scalaire sur $\vec{B}$ \textcolor{red}{(pas parlé avant)} :

\[
w = -\int \vec{j} \cdot \vec{E} \; dt + \int \left(\epsilon_0\frac{\vec{\nabla}||\vec{E}||^2 }{2}
+ \frac{1}{\mu_0}((\vec{\nabla}\cdot \vec{B})\vec{B}-\vec{\nabla}(\vec{B}\cdot \vec{B})\right) 
\cdot \vec{dx}
\]

Pour finir, en se souvenant que la divergence de $\vec{B}$ est nulle :

\[
w = -\int \vec{j} \cdot \vec{E} \; dt + \int \vec{\nabla} \left(\epsilon_0\frac{||\vec{E}||^2 }{2}
- \frac{||\vec{B}||^2}{\mu_0}\right) 
\cdot \vec{dx}
\]

Qui n'est autre que :

\[\boxed{
w = -\int \vec{j} \cdot \vec{E} \; dt + \epsilon_0\frac{||\vec{E}||^2 }{2}
- \frac{||\vec{B}||^2}{\mu_0}
}\]

\textcolor{red}{(faux pour $\vec{B}$, où est le vecteur de Poynting ?)}

\subsubsection{Tenseur des contraintes}

\section{Induction}

\subsection{Induction mutuelle}

Nous allons nous appuyer sur le problême suivant pour appréhender le phénomême d'induction magnétique :

\def\svgwidth{\textwidth}
\input{induction.pdf_tex}

On considère un fil rectiligne infini parcouru par un courant variable $I$ vis-à-vis duquel on trouve une spire (fil circulaire).

Nous n'avons pas encore abordé le sujet du \emph{conducteur électrique}, et cette notion bien que connue de tous peut vous sembler obscure. Nous verrons dans le prochain chapitre ce qu'est vraiment un fil électrique, mais dans cette exemple vous pouvez le considérer comme une sorte de "guide de charge" qui contraint leur mouvement sans pour autant les freiner. Un \emph{supraconducteur}, en fait, même si ce terme qui désigne un phénomême compliqué est encore loin de pouvoir être introduit.

Dans notre exemple, on impose seulement le courant dans le fil rectiligne. Ce sera donc lui qu'on considérera comme la source en premier lieu.

Les deux équations de Maxwell sur $\vec{B}$ sont :

\[
\left\{ \begin{matrix}
\nabla \cdot \vec{B} = 0 \\
\nabla \times \vec{B} = \mu_{0}\left(\vec{j} + \epsilon_0\frac{\partial\vec{E}}{\partial t}\right)
\end{matrix} \right.
\]

Pour le moment, considérons simplement le courant $I$ de densité de courant $\vec{j}$ :

\[
\left\{ \begin{matrix}
\nabla \cdot \vec{B} = 0 \\
\nabla \times \vec{B} = \mu_{0}\vec{j}
\end{matrix} \right.
\]

Pour calculer le champ au niveau de la spire, il va falloir passer de cette formulation locale à une formulation globale. Pour ce faire, on utilise deux puissants théorèmes de l'analyse vectorielle ; les théorèmes de Green-Ostrogradski et de Stokes :

\[
\left\{ \begin{matrix}
\iiint_V \left(\nabla \cdot \vec{f}\right) dV = \oiint_{\partial V}  \vec{f} \cdot \vec{dS} \\
\iint_{\mathcal{S}} \left(\nabla \times \vec{f}\right) \vec{dS} = \oint_{\partial S}  \vec{f} \cdot \vec{dl} \\
\end{matrix} \right.
\]

Pour écrire que :

\[
\left\{ \begin{matrix}
\oiint_{\partial V}  \vec{B} \cdot \vec{dS} = 0\\
\oint_{\partial S}  \vec{B} \cdot \vec{dl} =  \mu_{0}\iint_{\mathcal{S}} \vec{j} \cdot \vec{dS}\\
\end{matrix} \right.
\]

La pertinence de cette formulation globale réside tout entière dans notre capacité à intégrer ces grandeurs. Cela est loin d'être toujours possible analytiquement, et quand ça l'est c'est souvent difficile. C'est pourquoi dans la plupart des cas on aura recours au calcul numérique.

Ici, il existe une solution simple si l'on considère la spire à une distance $R$ du fil très grande devant son rayon $r$. On peut alors émettre l'hypothèse que $\vec{B}$ est constant sur tout le disque dont la spire est le bord.

En considérant comme volume d'intégration un cylindre infini de rayon $R$ et ayant pour axe le fil, ainsi qu'une section de ce cylindre orthogonale à son axe, on peut écrire sur la base de considérations de symétrie :

\[
\left\{ \begin{matrix}
\pi R^2\vec{B}\cdot \vec{e_r} = 0\\
2\pi R\vec{B}\cdot \vec{e_{\theta}} =  \mu_{0}I\\
\end{matrix} \right.
\]

Dont on déduit après considération de l'invariance selon $\vec{e_z}$ :

\[
\vec{B} =  \frac{\mu_{0}I}{2\pi R}\vec{e_{\theta}}
\]

De la même façon, les deux équations de Maxwell sur $\vec{E}$ sont :

\[ 
\left\{ \begin{matrix}  \nabla \cdot \vec{E} = \frac{\rho}{\epsilon_0}  \\
\nabla \times \vec{E} = - \frac{\partial\vec{B}}{\partial t} \\
\end{matrix} \right.
\]

En l'absence de charge électrique à échelle plus grande que celle des particules (neutralité de l'atome), on a simplement :

\[ 
\left\{ \begin{matrix}  \nabla \cdot \vec{E} = 0  \\
\nabla \times \vec{E} = - \frac{\partial\vec{B}}{\partial t} \\
\end{matrix} \right.
\]

Ou sous forme globale :

\[
\left\{ \begin{matrix}
\oiint_{\partial V}  \vec{E} \cdot \vec{dS} = 0\\
\oint_{\partial S}  \vec{E} \cdot \vec{dl} =  - \mu_{0}\iint_{\mathcal{S}} \frac{\partial\vec{B}}{\partial t} \cdot \vec{dS}\\
\end{matrix} \right.
\]

En choisissant le tore dont la spire est une section droite et le disque que décrit la spire comme variétés d'intégration :

\[
\left\{ \begin{matrix}
\pi r^2\vec{E}\cdot \vec{e_{r'}} = 0\\
2\pi r\vec{E}\cdot \vec{e_{\theta '}} =  - \mu_{0}\pi r^2 \frac{\partial\vec{B}}{\partial t}\\
\end{matrix} \right.
\]

Autrement écrit, considérant l'invariance selon $\vec{e_{\theta}}$ :

\[
\vec{E} =  - \frac{\mu_{0} r}{2} \frac{\partial ||\vec{B}||}{\partial t}\vec{e_{\theta '}}\\
\]

En remplaçant $\vec{B}$ par son expression précédemment établie, sur la spirale :

\[
\vec{E} =  - \frac{\mu_{0} r}{2} \frac{\partial}{\partial t} \left( \frac{\mu_{0}I}{2\pi R}\right) \vec{e_{\theta '}}\\
\]

\[
\vec{E} =  - \frac{\mu_{0}^2 r}{4 \pi R} \frac{\partial I}{\partial t} \vec{e_{\theta '}}\\
\]

Ce qui achève la première partie de ce calcul. En toute honnêteté, nous avons oublié certaines choses mais nous allons pourtant essayer de continuer d'avancer, jusqu'à arriver aux équations du mouvement ; nous reviendrons juste après sur les choses passées sous silence. On rappelle l'expression de la quadriforce :

\[
\left( \begin{matrix}  p 
\\ \vec{f} \end{matrix} \right) = \left( \begin{matrix} -\vec{j} \cdot \vec{E}
\\ \rho \vec{E} + \vec{j} \times \vec{B} \end{matrix} \right)
\]

On considère maintenant la spire. Comme déjà précisé lors de la présentation de ce problême, nous allons supposer que les charges sont simplement contraintes à elles. Prenons pour l'exemple des électrons de masse $m$ et de charge $-q_e$ et de concentration $c_e$ (et ignorons le courant de trous associé, qui sera évoqué ultérieurement), la quadriforce s'écrit :

\[
\left( \begin{matrix}  p 
\\ \vec{f} \end{matrix} \right) = \left( \begin{matrix} -\vec{j_e} \cdot \vec{E}
\\ -c_e q_e \vec{E} + \vec{j_e} \times \vec{B} \end{matrix} \right)
\]

On reconnait alors une force tangentielle ainsi qu'une force centrifuge (ou centripète suivant la valeur de $\vec{j}$). Seule la première travaille. L'expression de la puissance étant non-nulle et le problême étant mécaniquement encastré, il est clair que les électrons sont accélérés dans la spire, avec une accélération :

\[
\vec{a} = \frac{-c_e q_e}{m} \vec{E} = \frac{\mu_{0}^2 c_e q_e r}{4 \pi R m} \frac{\partial I}{\partial t} \vec{e_{\theta '}}
\]

Ou encore, leur vitesse est :

\[
\vec{v} = \frac{\mu_{0}^2 c_e q_e r I}{4 \pi R m} \vec{e_{\theta '}}
\]

Avec $I$ toujours étant le courant dans le fil rectiligne.

On peut se demander pourquoi les électrons dans la spire ont une vitesse même quand $I$ est constant. Ce n'est pas habituel dans la réalité, comme nous le verrons, mais pour un supraconducteur c'est effectivement le cas.

Des charges qui se déplacent - comme c'est manifestement le cas ici - ne prennent d'autre nom que celui de courant électrique. Il apparaît donc que courant $I$ (qu'on appelera dorénavant $I_1$) \emph{induit} dans la spire un courant $I_2$ :

\[
I_2 = -\iint_S c_e q_e \vec{v} \cdot \vec{dS} = -\frac{\mu_{0}^2 c_e^2 q_e^2 S r}{4 \pi R m}I_1
\]

\textcolor{red}{(essayer d'obtenir le même résultat par le théorème de Poynting)}

En fait, ce courant $I_2$ contient une partie de l'énergie apportée par l'inducteur (le fil rectiligne) pour que ses électrons puissent être accélérés jusqu'à $I_1$. Mais - en supposant le fil rectiligne comme également supraconducteur - une fois le régime établi, il n'y a plus de puissance en jeu et donc plus d'échange d'énergie ; et l'énergie cinétique des électrons dans les deux fils est conservée.

D'ailleurs, en toute rigueur il faudrait plutôt se contraindre à écrire :

\[
dI_2 = -\frac{\mu_{0}^2 c_e^2 q_e^2 S r}{4 \pi R m}dI_1
\]

Car cette relation  - contrairement à la précédente - reste vraie même si la spire n'est pas parfaitement calibrés pour avoir un courant nul au démarrage de l'expérience (avant l'application de $I_1$).

\subsection{Auto-induction}

Ceci dit, nous n'en avons pas terminé avec notre problême, et ce pour deux raisons :

\begin{itemize}
\item Premièrement, nous n'avons pas considéré l'expression complète de Maxwell-Ampère, et avons ignoré l'influence des variations du champ électrique sur le champ magnétique. 
\item Deuxièmement, le courant $I_2$ induit modifie le champ $\vec{B}$, ce qui pourrait bien avoir des effets rétroactifs ($I_2$ ayant été calculé sur la base du seul champ $\vec{B_1}$ produit par $I_1$).
\end{itemize}

L'attitude à adopter pour répondre à ces deux problêmes revient à tenter d'appliquer correctement Maxwell-Ampère autour de la spire, de façon à calculer le champ $\vec{B}_2$ causé par $I_2$ :

\[
\nabla \times \vec{B_2} = \mu_{0}\left(\vec{j_2} + \epsilon_0\frac{\partial\vec{E}}{\partial t}\right)
\]

En reconnaissant que $c^2 = \frac{1}{\mu_0 \epsilon_0}$ :

\[
\nabla \times \vec{B_2} = \mu_{0}\vec{j_2} + c^{-2}\frac{\partial\vec{E}}{\partial t}
\]

Reprenons maintenant l'expression de $\vec{E}$ sur la spire :

\[
\vec{E} =  - \frac{\mu_{0}^2 r}{4 \pi R} \frac{\partial I_1}{\partial t} \vec{e_{\theta '}}\\
\]

Pour écrire :

\[
\nabla \times \vec{B_2} = \mu_{0}\vec{j_2} + \frac{\mu_{0}^2 c^{-2} r}{4 \pi R}\frac{\partial^2 I_1}{\partial t^2}\vec{e_{\theta '}}
\]

Or, quand on fait l'application numérique de $\frac{\mu_{0}^2 c^{-2}}{4 \pi}$, on aperçoit que cette grandeur est extrêmement petite. Pour que ce terme ne soit pas négligeable devant le terme en $\vec{j_2}$, il faudrait que $\frac{\partial^2 I_1}{\partial t^2}$ soit très grand ce qui n'est possible qu'en travaillant à très haute fréquence.

Ce cadre d'étude - dit \emph{propagatif} - est la version la plus correcte mais la plus lourde de l'électro-magnétodynamique. Elle est à la base de l'optique géométrique, comme nous le verrons ultérieurement \textcolor{red}{(pas fait)}. On peut cependant très largement s'en passer dans l'étude des machines électriques et des circuits magnétiques, qui ne travaillent jamais à ces niveaux de fréquence ; mais pas dans les semi-conducteurs de puissance.

Laissons donc tomber le terme propagatif ; les choses sont bien assez compliquées comme ça :

\[
\nabla \times \vec{B_2} = \mu_{0}\vec{j_2}
\]

Le courant $I_2$ est la source d'un champ magnétique $\vec{B_2}$ qui vient s'ajouter partout et en tout temps au champ précédemment calculé $\vec{B_1}$, de sorte que l'énergie du champ résultant $\vec{B} = \vec{B_1} + \vec{B_2}$ - champ difficilement calculable (c'est possible analytiquement, mais compliqué et pas très intéressant) soit connue car elle est le défaut pris dans celle de $\vec{B_1}$ et qui est venu alimenter $I_2$ :

\[
E(\vec{B}) = E(\vec{B_1}) - E(\vec{B_2})
\]

Le champ résultant $\vec{B}$ est trop compliqué à calculer, mais par des considérations de symétrie auxquelles je vous laisse réfléchir, il est impossible que $\vec{B_2}$ suive une autre direction que $\vec{e_z}$ dans le plan qui contient notre système. Ainsi, Maxwell-Faraday donne un produit $\vec{j} \cdot \vec{E}$ nul partout dans le fil rectiligne ; ce qui brise le couplage. C'est la raison pour laquelle ce problême peut-être étudié en quelques pages.

Néanmoins, rien ne dit que ce champ additionnel $\vec{B_2}$ crée n'interfère pas avec la spire elle-même. On parlerait alors d'\emph{auto-induction}. Ce phénomême existe effectivement, et peut-être traité analytiquement même sans connaître précisément $\vec{B_2}$.

Le point délicat est que $I_2$ et $\vec{B_2}$ sont couplés au sens où chacun est la source de l'autre.

Revenons un instant sur l'étape de calcul suivante :

\[
\vec{B_1} =  \frac{\mu_{0}I_1}{2\pi R}\vec{e_{\theta}}
\]

On voit que $\vec{B_1}$ est proportionnel à $I_1$ ; mais que ledit facteur de proportionnalité dépend du point considéré. Ce résultat peut-être formulé de façon globale :

\[
\iint_{\mathcal{S}} \vec{B_1} \cdot \vec{dS} =  \iint_{\mathcal{S}} \frac{\mu_{0}I_1}{2\pi R}\vec{e_{\theta}} \cdot \vec{dS}
\]

Appelons cette intégrale \emph{flux magnétique} :

\[
\Phi_{1,S} =  \frac{\mu_{0}r^2}{2 R}I_1
\]

On défini de cette façon l'\emph{inductance mutuelle} du fil rectiligne sur la spire :

\[
\Phi_{1,S} =  M_{12} I_1
\]

Cette relation de proportionnalité est vraie en toute généralité. Elle est la conséquence de la linéarité des équations de Maxwell.

De la même façon, parce-que $I_2$ n'a pas d'influence sur $I_1$, on peut écrire que :

\[
\Phi_{2,R} = 0
\]

Et donc :

\[
M_{21} = 0
\]

Pour finir, on peut considérer l'auto-induction de la même façon, en définissant les \emph{inductances propres} :

\[
\left\{ \begin{matrix}
\Phi_{1,R} =  L_1 I_1 \\
\Phi_{2,S} =  L_2 I_2
\end{matrix} \right.
\]

On peut déjà dire que d'après la même raison que pour $M_{21}$, $L_1$ est nul. En effet, le fil rectiligne produit un champ $\vec{B_1}$ qui suit $\vec{e_{\theta}}$ et donc $\vec{e_z}$ dans le plan d'étude.

Tout l'intéret de formulation en inductance émane de la possibilité qui nous est offerte à pouvoir affecter une valeur à $L_2$ sur la base de simples considérations énergétiques. Revenons sur :

\[
p = -\vec{j} \cdot \vec{E}
\]

La spire est un sous-système de notre problême, qui vient absorber une partie de l'énergie du champ $\vec{B_1}$ pour l'envoyer tout entière dans $\vec{B_2}$ \textcolor{red}{(vérifier ça)}.

L'énergie absorbée par la spire pendant toute son existence est donc :

\[
W_2 = -\int \iiint_\Omega \left( \vec{j_2} \cdot \vec{E} \right) dV dt
\]

Après s'être placé dans le système de coordonées adéquat (cylindrique en l'occurence) :

\[
W_2 = -\int I_2 \oint_C \vec{E} \cdot \vec{dl} dt
\]

D'après le théorème de Stokes :

\[
W_2 = -\int I_2 \iint_S \left(\nabla \times \vec{E} \right) \cdot \vec{dS}  dt
\]

Ou encore :

\[
W_2 = \int I_2 \iint_S \frac{\partial\vec{B}}{\partial t} \cdot \vec{dS}  dt
\]

\[
W_2 = \int I_2 \frac{\partial}{\partial t}\iint_S \vec{B} \cdot \vec{dS}  dt
\]

\[
W_2 = \int I_2 \frac{\partial\Phi_{S}}{\partial t}  dt
\]

D'après la définition de l'inductance :

\[
W_2 = L_2 \int I_2 \frac{\partial I_2}{\partial t}  dt
\]

\[
W_2 = L_2 \int I_2 dI_2
\]

\[
W_2 = \frac{1}{2} L_2 I_2^2
\]

Ou encore :

\[
W_2 = \frac{1}{2}I_2 \Phi_{S}
\]

\textcolor{red}{(et pour $L_2$ du coup ?)}

On peut alors introduire la \emph{matrice inductance} :

\[\boxed{
\left( \begin{matrix} \Phi_1 \\ \Phi_2 \end{matrix} \right) = 
\left( \begin{matrix}  
L_1 & M_{21} \\
M_{12} & L_2
\end{matrix} \right)
\left( \begin{matrix} I_1 \\ I_2 \end{matrix} \right)
}\]

Ou symboliquement :

\[\boxed{
\Phi^{i} = L^{i}_{\; j} I^{j}
}\]

Nous utiliserons abondamment ce concept quand nous présenterons la méthode analytique de résolution des problêmes de circuits magnétiques ou machines électriques.


\chapter{Electrodynamique des milieux continus}

\section{Polarisation}

\textcolor{red}{(modélisation à partir du dossier aimants flottants)}

\section{Ferromagnétisme}

\textcolor{red}{(modélisation à partir du dossier aimants flottants)}

\centerline{\includegraphics[width=0.8\textwidth]{bh.png}}

\section{Equations de Maxwell dans les milieux}

La forme des équations de Maxwell qui a été présentée jusqu'à présent est la seule façon correcte de les écrire en toute généralité. Simplement, les comportements de la matière qui viennent d'être présentés ont permis d'introduire deux nouveaux champs $\vec{D}$ et $\vec{H}$ qui correspondent aux parts des champs causés par les charges et courants macroscopiques.

Les vrais champs électriques $\vec{E}$ et magnétiques $\vec{B}$ sont la sommes des contributions de $\vec{D}$ et de $\vec{H}$ avec celles de la polarisation $\vec{P}$ et de l'aimantation $\vec{M}$ :

\[\boxed{
\left\{ \begin{matrix}
\vec{E} =  \frac{1}{\epsilon_0}\left(\vec{D}-\vec{P}\right) \\
\vec{B} =  \mu_0(\vec{H}+\vec{M})
\end{matrix} \right.
}\]

Ne connaissant pas toujours les valeurs de $\vec{P}$ et $\vec{M}$, on préfèrera se référer aux lois de comportement bien que celles-ci soient rarement simple à manipuler.

Dans ce cadre, on peut alors réecrire les équations de Maxwell en prenant compte de l'influence des milieux ; lesquels ne font qu'ajouter des sources. Les deux équations sans source sont donc inchangées mais on exprime les équations avec source en fonction des champs nouvellement introduits :

\textcolor{red}{(faire la démo)}

\[\boxed{
\left\{ \begin{matrix}  \nabla \cdot \vec{D} = \rho  \\
\nabla \times \vec{E} = - \frac{\partial\vec{B}}{\partial t} \\
\nabla \cdot \vec{B} = 0 \\
\nabla \times \vec{H} = \vec{j} + \frac{\partial\vec{D}}{\partial t}
\end{matrix} \right.
}\]

\section{Interaction électromécanique}

\subsection{Pression magnétique}

\textcolor{red}{(reprendre le théorème de Poynting)}

\subsection{Tenseur des contraintes}

\section{Conductivité}

De la même façon qu'en thermique, un milieu continu en électromagnétisme est décrit par une conductivité $\sigma$ donnant lieu à une équation de diffusion :

\[
\vec{j} = -\sigma \nabla \phi
\]

Ou encore :

\[\boxed{
\vec{j} = \sigma \vec{E}
}\]

Nous donnerons sa justification dans la partie dédiée à la physique statistique \textcolor{red}{(pas fait)}.

On la comprendra simplement telle que dans un milieu continu - et contrairement au vide - l'accélération d'une charge sera très vite contrecarrée par ledit milieu. La densité de puissance s'y écrit :

\[
p = \vec{j} \cdot \vec{E} = \sigma ||\vec{E}||^2 = \sigma^{-1} ||\vec{j}||^2
\]

Dans le chapitre précédent, nous avions écrit l'équation de Maxwell sans source :

\[ 
\left( \begin{matrix}  \frac{\partial^2\phi}{\partial t^2} + \Delta \phi \\ 
\frac{\partial^2 \vec{A}}{\partial t^2} + \Delta \vec{A} \end{matrix} \right)
= - \mu_{0}  \left( \begin{matrix} \rho \\ \vec{j} \end{matrix} \right)
\]

Cette équation à ceci d'important qu'elle constitue à elle seule un moyen pour calculer tous les champs imaginable dans un code de calcul par éléments finis, comme nous le verrons \textcolor{red}{(pas fait)}.

En \emph{magnétostatique}, contexte d'étude dans lequel on considère l'hypothèse d'un régime quasi-statique et donc dans lequel nous négligeons les dérivées temporelles, on a simplement :

\[\boxed{
\left( \begin{matrix} \Delta \phi \\ 
\Delta \vec{A} \end{matrix} \right)
= - \mu_{0}  \left( \begin{matrix} \rho \\ \vec{j} \end{matrix} \right)
}\]

Qui constitue une équation de Poisson, du même type que celle qu'on trouve en gravitation \textcolor{red}{(pas fait)}.

Nous allons maintenant chercher comment celle-ci s'altère en présence de courants induits (dans un matériau à la conductivité finie, donc pas un supraconducteur), on appelle ce cadre d'étude la \emph{magnétodynamique}. Nous nous plaçerons dans un cadre propagatif pour ne pas avoir à le faire plus tard.

Partons de l'équation de Maxwell-Ampère :

\[
\nabla \times \vec{B} = \mu_{0}\left(\vec{j} + \epsilon_0\frac{\partial\vec{E}}{\partial t}\right)
\]

\[
\nabla \times \vec{B} = \mu_{0}\left(\sigma\vec{E} + \epsilon_0\frac{\partial\vec{E}}{\partial t}\right)
\]

On sait que le lien avec les potentiels réside dans les relations $\vec{E} = -\frac{\partial\vec{A}}{\partial t} - \nabla\phi$ et $\vec{B} = \nabla \times \vec{A}$. Continuons :

\[
\nabla \times \nabla \times \vec{A} = \mu_{0}\left(\sigma\left(-\frac{\partial\vec{A}}{\partial t} - \nabla\phi\right) + \epsilon_0\left(-\frac{\partial^2\vec{A}}{\partial t^2} - \nabla\left(\frac{\partial\phi}{\partial t}\right)\right)\right)
\]

La jauge de Lorenz nous simplifie la vie (c'est précisément pour ça qu'on l'utilise) dans le dernier terme de la somme :

\[
\nabla \times \nabla \times \vec{A} = -\mu_{0}\left(\sigma\left(\frac{\partial\vec{A}}{\partial t} + \nabla\phi\right) + \epsilon_0\frac{\partial^2\vec{A}}{\partial t^2} \right) + \nabla(\nabla\cdot\vec{A})
\]

Qu'on fait passer à gauche :

\[
\nabla \times \nabla \times \vec{A} - \nabla(\nabla\cdot\vec{A}) = -\mu_{0}\left(\sigma\left(\frac{\partial\vec{A}}{\partial t} + \nabla\phi\right) + \epsilon_0\frac{\partial^2\vec{A}}{\partial t^2} \right)
\]

Pour reconnaître un Laplacien :

\[
-\Delta \vec{A} = -\mu_{0}\left(\sigma\left(\frac{\partial\vec{A}}{\partial t} + \nabla\phi\right) + \epsilon_0\frac{\partial^2\vec{A}}{\partial t^2} \right)
\]

La magie opère. Que nous donne la mise en forme d'équation d'onde ?

\[\boxed{
\Delta \vec{A} -\mu_{0}\sigma\frac{\partial\vec{A}}{\partial t} - \mu_{0}\epsilon_0\frac{\partial^2\vec{A}}{\partial t^2} = \mu_{0}\sigma\nabla\phi
}\]

\textcolor{red}{(expliquer le terme en $\phi$ s'il est correct et bien signé. Discuter de l'équation en $\phi$)}

Pour un problême électriquement neutre, le gradient de $\phi$ est nul et l'équation devient :

\[
\Delta \vec{A} -\mu_{0}\sigma\frac{\partial\vec{A}}{\partial t} - \mu_{0}\epsilon_0\frac{\partial^2\vec{A}}{\partial t^2} = 0
\]

Par ailleurs, nous y reviendrons plus tard \textcolor{red}{(pas fait)} mais dans les dispositifs électrotechniques on peut largement négliger le terme propagatif :

\[
\Delta \vec{A} -\mu_{0}\sigma\frac{\partial\vec{A}}{\partial t} = 0
\]

Qui est l'\emph{équation de la magnétodynamique}, une version vectorielle de l'\emph{équation de la chaleur}, d'abord découverte comme décrivant le phénomène de \emph{conduction thermique}.

Cette formulation a ceci d'intéressant qu'elle constitue en elle-même une méthode pour traiter entièrement tout problême magnétique tant qu'il est linéaire.

Par ailleurs, parce-que cette équation aux dérivées partielles est linéaire et que son second membre est nul, on peut calculer son rotationnel et sa dérivée temporelle pour réaliser qu'elle admet une variante par champ :

\[
\left\{\begin{matrix}
\Delta \vec{D} -\mu_{0}\sigma\frac{\partial\vec{D}}{\partial t} = 0 \\
\Delta \vec{E} -\mu_{0}\sigma\frac{\partial\vec{E}}{\partial t} = 0 \\
\Delta \vec{B} -\mu_{0}\sigma\frac{\partial\vec{B}}{\partial t} = 0 \\
\Delta \vec{H} -\mu_{0}\sigma\frac{\partial\vec{H}}{\partial t} = 0
\end{matrix}\right.
\]

Ce qui est important de retenir est que si ces formulations ont quelque chose de \emph{magique}, elles ne s'appliquent qu'au cas des milieux linéaires (on se forcera donc à repasser par les équations de Maxwell pour prendre en compte les phénomènes de saturation).

\subsection{Courants de Foucault}

Prenons pour exemple le fer, dont la conductivité vaut $\sigma \sim 10 MS.m^{-1}$.

On appelle \emph{courants de Foucault} les courants induits dans ce fer, qui du fait d'une conductivité finie du fer sont la cause d'une dissipation d'énergie sous forme thermique, comme nous allons le voir.

Considérons un cylindre à section circulaire :

\begin{center}
\begin{tikzpicture}[scale=3]
\clip(-1.1,-1.1) rectangle (1.1,1.1);
\draw [line width=1pt] (0,0) circle (1cm);
\draw [line width=1pt,dotted] (0,0) circle (0.5cm);
\draw [->,line width=1pt] (0.5,0) -- (0.5,0.4);
\draw (0.5248714802263114,0.35634556022439795) node[anchor=north west] {$\vec{j}$};
\draw [line width=1pt] (0.4,-0.4) circle (0.1cm);
\fill[black] (0.4,-0.4) circle (0.02cm);
\draw (0.4918668093096442,-0.32025019356727824) node[anchor=north west] {$\vec{B}$};
\draw (-0.601412914804955,0.6657643500681523) node[anchor=north west] {$\mu,\sigma$};
\end{tikzpicture}
\end{center}

Qu'on soumet à un champ magnétique alternatif homogène :

\[
\vec{B} = |\vec{\underline{B}}|\sqrt{2}\cos(\omega t) \; \vec{e_z}
\]

Pour traiter ce problême en magnétostatique, on commence par écrire la conservation de la charge dans un milieu électriquement neutre :

\[
\nabla \cdot \vec{j} = 0
\]

Cette divergence doit-être écrite dans un système de coordonnées adapté à la géométrie du problême, en l'occurence un disque \textcolor{red}{(démonstration)} :

\[
\frac{1}{r}\frac{\partial(r j_r)}{\partial r} + \frac{1}{r}\frac{\partial j_\theta}{\partial \theta} + \frac{\partial j_z}{\partial z} = 0
\]

Il faut maintenant introduire des considération de symétrie, ici les variations des grandeurs selon $\theta$ ne peuvent qu'êtres nulles et comme on ne s'intéresse pas aux effets de bord il est clair que $j_z = 0$ :

\[
\frac{1}{r}\frac{\partial(r j_r)}{\partial r} = 0
\]

Qu'on intègre en :

\[
j_r = \frac{\lambda}{r}
\]

Néanmoins, ce résultat n'est en général pas physiquement acceptable, car en $r=0$ on aurait un courant et donc une puissance infinie.

Par conséquent, on a nécessairement $\lambda = 0$ et donc $j_r = 0$.

Reste à trouver une valeur à attribuer à $j_\theta$. Pour ce faire, on écrit Maxwell-Faraday :

\[
\nabla \times \vec{E} = -\frac{\partial \vec{B}}{\partial t}
\]

Ce qui induit un rotationnel de densité de courant :

\[
\nabla \times \vec{j} = -\sigma\frac{\partial \vec{B}}{\partial t}
\]

\[
\nabla \times \vec{j} = \sigma\omega |\vec{\underline{B}}|\sqrt{2}\sin(\omega t) \; \vec{e_z}
\]

En coordonnées polaires :

\[
\left(\frac{1}{r}\frac{\partial j_z}{\partial \theta} - \frac{\partial j_\theta}{\partial z}\right)\vec{e_r}
+ \left(\frac{\partial j_r}{\partial z} - \frac{\partial j_z}{\partial r}\right)\vec{e_\theta}
+ \frac{1}{r}\left(\frac{\partial(r j_\theta)}{\partial r} - \frac{\partial j_r}{\partial \theta}\right)\vec{e_z} = \sigma\omega |\vec{\underline{B}}|\sqrt{2}\sin(\omega t) \; \vec{e_z}
\]

En éludant tout ce qui ne concerne pas $j_\theta$ :

\[
- \frac{\partial j_\theta}{\partial z}\vec{e_r}
+ \frac{1}{r}\frac{\partial(r j_\theta)}{\partial r}\vec{e_z} = \sigma\omega |\vec{\underline{B}}|\sqrt{2}\sin(\omega t) \; \vec{e_z}
\]

Qui constitue deux équations. La première est une évidence :

\[
\frac{\partial j_\theta}{\partial z} = 0
\]

Mais la seconde l'est moins :

\[
\frac{1}{r}\frac{\partial(r j_\theta)}{\partial r} = \sigma\omega |\vec{\underline{B}}|\sqrt{2}\sin(\omega t)
\]

\[
\frac{\partial(r j_\theta)}{\partial r} = r\sigma\omega |\vec{\underline{B}}|\sqrt{2}\sin(\omega t)
\]

Qu'on intègre :

\[
r j_\theta = \frac{r^2}{2}\sigma\omega |\vec{\underline{B}}|\sqrt{2}\sin(\omega t)
\]

\[
j_\theta = \frac{r}{2}\sigma\omega |\vec{\underline{B}}|\sqrt{2}\sin(\omega t)
\]

Nous avons trouvé l'expression de la densité de courant de Foucault dans la géométrie qu'on s'est donné :


\[
\vec{j} = \frac{r}{2}\sigma\omega |\vec{\underline{B}}|\sqrt{2}\sin(\omega t) \vec{e_\theta}
\]

La puissance perdue par dissipation calorique de ces courants de Foucault s'écrira naturellement :

\[
P = \iiint \frac{||\vec{j}||^2}{\sigma} \; dV
\]

\[
P = \iiint \frac{r^2}{2}\sigma\omega^2 |\vec{\underline{B}}|^2\sin^2(\omega t) \; dV
\]

En notant $l$ la longueur du cylindre, supposée grande devant son rayon $R$ pour qu'on puisse négliger les effets de bord :

\[
P = l\sigma\omega^2 |\vec{\underline{B}}|^2|\sin^2(\omega t) \int_0^R \int_0^{2\pi} \frac{r^2}{2} r\; d\theta \; dr
\]

\[
P = \frac{\pi \sigma l R^4}{4}|\vec{\underline{B}}|^2 \omega^2 \sin^2(\omega t) 
\]

De valeur moyenne :

\[
\braket{P} = \frac{\pi \sigma l R^4 \omega^2}{4}|\vec{\underline{B}}|^2 \frac{\omega}{2\pi}\int_0^\frac{2\pi}{\omega} \frac{1-\cos(2\omega t)}{2} \; dt
\]

\[
\braket{P} = \frac{\pi \sigma l R^4 \omega^2}{8}|\vec{\underline{B}}|^2
\]

Bien souvent, on aura une relation de proportionnalité entre $\vec{B}$ et un courant $\underline{I}$, ce qui nous amènera à écrire une relation de la forme :

\[\boxed{
\braket{P} = R_f|\underline{I}|^2
}\]

Introduisant la \emph{résistance fer} qui apparaîtra dans les modèles externes des dispositifs électrotechniques tels que le transformateur.

En plus des courants de Foucault, quand on la mesure pour un point de fonctionnement cette résistance fer peut également contenir des contributions liées aux saturations, bien que celles-ci soient des effets non-linéaires qui rendent cette modélisation un peu trop simpliste.

Enfin, il faut faire attention à que les courants de Foucault ne soient pas eux-mêmes la source d'un champ contraire trop important, au risque de contrarier le raisonnement qui vient d'être mené. De quel façon ce raisonnement est-il contrarié ? C'est ce que nous allons voir  maintenant.

\subsection{Chauffage par induction}

Cette section fait suite directe à la précédente, mais avec prise en compte de l'influence des courants induits sur le champ.

Le chauffage par induction est une technique utilisée pour travailler les matériaux ou dans les cuisinières domestiques. Il a l'avantage certain de s'effectuer à distance.

Considérons un cylindre de matière décrite par sa conductivité $\sigma$ et sa perméabilité magnétique $\mu$. Ce cylindre est entouré d'un solénoïde parcouru par un fort courant sinusoïdal $I = \Re\left(\sqrt{2}\underline{I}e^{jwt}\right)$ :

\begin{center}
\definecolor{ffxfqq}{rgb}{1,0.4980392156862745,0}
\definecolor{aqaqaq}{rgb}{0.6274509803921569,0.6274509803921569,0.6274509803921569}
\definecolor{cqcqcq}{rgb}{0.7529411764705882,0.7529411764705882,0.7529411764705882}
\begin{tikzpicture}[scale=2]
\clip(-2.8611420855010756,-2.9781972418819223) rectangle (3.0834428842702497,2.313196950970848);\fill[line width=2pt,color=aqaqaq,fill=aqaqaq,fill opacity=1] (-1,-2) -- (1,-2) -- (1,2) -- (-1,2) -- cycle;\draw [line width=2pt,color=ffxfqq,fill=ffxfqq,fill opacity=1] (-1.5,-1.8) circle (0.2cm);\draw [line width=2pt,color=ffxfqq,fill=ffxfqq,fill opacity=1] (-1.5,-1.2) circle (0.2cm);\draw [line width=2pt,color=ffxfqq,fill=ffxfqq,fill opacity=1] (-1.5,-0.6) circle (0.2cm);\draw [line width=2pt,color=ffxfqq,fill=ffxfqq,fill opacity=1] (-1.5,0) circle (0.2cm);\draw [line width=2pt,color=ffxfqq,fill=ffxfqq,fill opacity=1] (-1.5,0.6) circle (0.2cm);\draw [line width=2pt,color=ffxfqq,fill=ffxfqq,fill opacity=1] (-1.5,1.2) circle (0.2cm);\draw [line width=2pt,color=ffxfqq,fill=ffxfqq,fill opacity=1] (-1.5,1.8) circle (0.2cm);\draw [line width=2pt,color=ffxfqq,fill=ffxfqq,fill opacity=1] (1.5,-1.8) circle (0.2cm);\draw [line width=2pt,color=ffxfqq,fill=ffxfqq,fill opacity=1] (1.5,-1.2) circle (0.2cm);\draw [line width=2pt,color=ffxfqq,fill=ffxfqq,fill opacity=1] (1.5,-0.6) circle (0.2cm);\draw [line width=2pt,color=ffxfqq,fill=ffxfqq,fill opacity=1] (1.5,0) circle (0.2cm);\draw [line width=2pt,color=ffxfqq,fill=ffxfqq,fill opacity=1] (1.5,0.6) circle (0.2cm);\draw [line width=2pt,color=ffxfqq,fill=ffxfqq,fill opacity=1] (1.5,1.2) circle (0.2cm);\draw [line width=2pt,color=ffxfqq,fill=ffxfqq,fill opacity=1] (1.5,1.8) circle (0.2cm);\draw [line width=2pt,color=aqaqaq] (-1,-2)-- (1,-2);\draw [line width=2pt,color=aqaqaq] (1,-2)-- (1,2);\draw [line width=2pt,color=aqaqaq] (1,2)-- (-1,2);\draw [line width=0.4pt,color=aqaqaq] (-1,2)-- (-1,-2);\draw [line width=0.4pt] (1.3583400909576941,-1.6588175996445533)-- (1.6420629772781177,-1.9407768108989438);\draw [line width=0.4pt] (1.3590202503236715,-1.9418615881103825)-- (1.6409797496763279,-1.6581384118896179);\draw [line width=0.4pt] (1.3583400909576941,-1.0588175996445532)-- (1.6420629772781177,-1.3407768108989437);\draw [line width=0.4pt] (1.3583400909576941,-0.45881759964455315)-- (1.6420629772781177,-0.7407768108989436);\draw [line width=0.4pt] (1.3583400909576941,0.14118240035544694)-- (1.6420629772781177,-0.14077681089894356);\draw [line width=0.4pt] (1.3583400909576941,0.741182400355447)-- (1.6420629772781177,0.45922318910105653);\draw [line width=0.4pt] (1.3583400909576941,1.3411824003554471)-- (1.6420629772781177,1.0592231891010566);\draw [line width=0.4pt] (1.3583400909576941,1.9411824003554472)-- (1.6420629772781177,1.6592231891010567);\draw [line width=0.4pt] (1.3590202503236715,-1.3418615881103824)-- (1.6409797496763279,-1.0581384118896178);\draw [line width=0.4pt] (1.3590202503236715,-0.7418615881103823)-- (1.6409797496763279,-0.4581384118896177);\draw [line width=0.4pt] (1.3590202503236715,-0.1418615881103822)-- (1.6409797496763279,0.1418615881103824);\draw [line width=0.4pt] (1.3590202503236715,0.4581384118896179)-- (1.6409797496763279,0.7418615881103825);\draw [line width=0.4pt] (1.3590202503236715,1.058138411889618)-- (1.6409797496763279,1.3418615881103826);\draw [line width=0.4pt] (1.3590202503236715,1.658138411889618)-- (1.6409797496763279,1.9418615881103827);\draw (-1.36,-1.754150659925317) node[anchor=north west] {$I\vec{e_\theta}$};\draw [->] (-1,-1.413655576045751) -- (-1,-0.746027609580759);\draw (-0.9729267303751882,-0.958245931747255) node[anchor=north west] {$\vec{H}(R)$};\draw (-0.9729267303751882,2.0387470447025504) node[anchor=north west] {$$};\draw [dashed] (-1.2,-0.5)-- (-1.2,0.5);\draw [dashed] (-1.2,0.5)-- (-1.8,0.5);\draw [dashed] (-1.8,0.5)-- (-1.8,-0.5);\draw [dashed] (-1.8,-0.5)-- (-1.2,-0.5);\draw (-1.44,-0.21) node[anchor=north west] {$\mathcal{C}_3$};\draw [dashed] (1.1,1.5)-- (1.1,0.5);\draw [dashed] (1.1,0.5)-- (1.2,0.5);\draw [dashed] (1.2,0.5)-- (1.2,1.5);\draw [dashed] (1.2,1.5)-- (1.1,1.5);\draw (1.04,1.7313631496820576) node[anchor=north west] {$\mathcal{C}_2$};\draw [dashed] (1.8,-0.4)-- (2,-0.4);\draw [dashed] (2,-0.4)-- (2,-0.2);\draw [dashed] (2,-0.2)-- (1.8,-0.2);\draw [dashed] (1.8,-0.2)-- (1.8,-0.4);\draw (2,-0.18) node[anchor=north west] {$\mathcal{C}_1$};\draw (-0.8960807566200649,1.9234780840698658) node[anchor=north west] {$\mu, \sigma$};\begin{scriptsize}\draw [fill=black] (-1.5,-1.8) circle (0.5pt);\draw [fill=black] (-1.5,-1.2) circle (0.5pt);\draw [fill=black] (-1.5,-0.6) circle (0.5pt);\draw [fill=black] (-1.5,0) circle (0.5pt);\draw [fill=black] (-1.5,0.6) circle (0.5pt);\draw [fill=black] (-1.5,1.2) circle (0.5pt);\draw [fill=black] (-1.5,1.8) circle (0.5pt);\end{scriptsize}
\end{tikzpicture}
\end{center}

De la même façon que précedemment, ce cylindre est donc plongé dans un champ $\vec{H}$, lequel est néanmoins altéré par la présence du cylindre, lui-même parcouru par des courants de Foucault. On pourrait imaginer mettre en place une méthode récursive de calcul de courants et des champs, mais cela reviendrait à estimer numériquement de façon alambiquée la solution de l'équation aux dérivées partielles de la magnétodynamique :

\[
\Delta \vec{H} -\mu\sigma\frac{\partial\vec{H}}{\partial t} = 0
\]

Avant de chercher à la résoudre analytiquement (ce qui est possible mais lourd), commençons par poser correctement le problême.

L'hypothèse principale est que la longueur du cylindre est grande devant sa largeur, qui sera elle-même grande devant l'espacement des conducteurs.

De cette façon, on peut négliger les effets de bord. Par symétrie, le champ $\vec{H}$ sera donc uniquement selon $\vec{e_z}$. $\mu$ n'a pas à être infini et peut même être proche de $\mu_0$. On ajoute qu'on modélise le solénoïde par une densité linéique de courant plutôt que des tores indépendants.

On applique successivement le théorème d'Ampère aux trois contours explicités :


\[
\oint  \vec{H} \cdot \vec{dl} = \iint \vec{j} \cdot \vec{dS}
\]

Dans le cas de $\mathcal{C}_1$ et $\mathcal{C}_2$, en l'absence de courant enlacé on trouve que $\vec{H}$ est constant. Par ailleurs, du fait que $\mathcal{C}_1$ soit défini sur un domaine infini, $\vec{H}$ partout sur ce domaine est la seule possibilité pour garantir la finitude de l'énergie.

Pour le dernier contour, le théorème d'Ampère s'écrit donc linéiquement :

\[
H_z(R)  = nI
\]

Avec $n$ la densité linéique de conducteurs le long du solénoïde. Ce résultat presque trivial si tant est qu'on sache faire les bonnes hypothèses va constituer la condition de Dirichlet de notre problême.

Il s'agit donc de résoudre l'équation de la magnétodynamique sur un disque et connaissant la valeur (constante) du champ en son bord. Ce ne sont maintenant plus que des maths :

\[
\Delta \vec{H} -\mu\sigma\frac{\partial\vec{H}}{\partial t} = 0
\]

Pour un champ selon $z$, en coordonnées sphériques et avec invariance selon $\theta$ et $z$, le Laplacien prend la forme :

\[
\frac{\partial^2 H_z}{\partial r^2} + \frac{1}{r}\frac{\partial (rH_z)}{\partial r} -\mu\sigma\frac{\partial H_z}{\partial t} = 0
\]

On effectue une étude harmonique, on passe donc en notations complexes :

\[
\frac{\partial^2 \underline{H}_z}{\partial r^2} + \frac{1}{r}\frac{\partial (r\underline{H}_z)}{\partial r} -j\mu\sigma\omega\underline{H}_z = 0
\]

La méthode pour résoudre cette équation consiste à décomposer $\underline{H}_z$ en série entière :

\[
\underline{H}_z=\sum_{n=0}^\infty \underline{a}_n r^n
\]

\[
\sum_{n=2}^\infty \underline{a}_n n(n-1) r^{n-2} + \sum_{n=1}^\infty \underline{a}_n n r^{n-2} -j\mu\sigma\omega\sum_{n=0}^\infty \underline{a}_n r^n = 0
\]

Par un changement d'indice dans le terme de droite :

\[
\frac{\underline{a}_1}{r} + \sum_{n=2}^\infty \underline{a}_n (n-1) r^{n-2} + \sum_{n=2}^\infty \underline{a}_n n r^{n-2} -j\mu\sigma\omega\sum_{n=2}^\infty \underline{a}_{n-2} r^{n-2} = 0
\]

\[
\frac{\underline{a}_1}{r} + \sum_{n=2}^\infty (\underline{a}_n n^2-j\sigma\mu\omega \underline{a}_{n-2}) r^{n-2} = 0
\]

En premier lieu, on remarque que la terme $\frac{\underline{a}_1}{r}$ ne peut-être fini en $r=0$ que pour $\underline{a}_1 = 0$ :

\[
\sum_{n=2}^\infty (\underline{a}_n n^2-j\sigma\mu\omega \underline{a}_{n-2}) r^{n-2} = 0
\]

Par ailleurs, cette somme étant valable pour tout $r$, on peut montrer \textcolor{red}{(comment ?)} qu'on a nécessairement :

\[
\forall n, \underline{a}_n = j\frac{\sigma\mu\omega}{n^2} \underline{a}_{n-2}
\]

Cette relation de récurrence implique trivialement que tous les termes impairs de la suite $\underline{a}$ sont nuls. Pour ce qui est des termes pairs, on peut effectuer le changement de variable $n=2k$ :

\[
\underline{a}_{2k} = j\frac{\sigma\mu\omega}{4k^2} \underline{a}_{2(k-1)}
\]

En partant d'un $\underline{a}_{0}$ qui reste à définir, on aura :

\[
\underline{a}_{2k} = \prod_{k'=1}^{k} \left(j\frac{\sigma\mu\omega}{4k'^2}\right) \underline{a}_0
\]

\[
\underline{a}_{2k} = j^k\left(\frac{\sigma\mu\omega}{2}\right)^k \frac{1}{2^k(k!)^2} \underline{a}_0
\]

En définissant l'\emph{épaisseur de peau} $\delta = \sqrt{\frac{2}{\sigma\mu\omega}}$ qui apparaît très souvent dans les problêmes de magnétodynamique :

\[
\underline{a}_{2k} = \frac{j^k}{2^k\delta^{2k}(k!)^2} \underline{a}_0
\]

Et donc :

\[
\underline{H}_z=\sum_{k=0}^\infty \frac{j^k}{2^k\delta^{2k}(k!)^2} \underline{a}_0 r^n
\]

Cette fonction appartient à la catégorie des \emph{fonctions de Bessel de première espèce}, pénibles à manipuler mais mathématiquement bien connues.

$\underline{a}_0$ est naturellement affecté par le biais de la condition aux limites $H_z(R)  = nI$.

\textcolor{red}{(compléter, discuter de l'épaisseur de peau)}

\chapter{Méthodes analytiques de résolution des problêmes électriques et magnétiques}

Dans ce chapitre, nous allons nous intéresser aux grandeurs globales qui permettent la modélisation des systèmes électriques et magnétiques. Ces méthodes peuvent être utilisées pour faire de la simulation, du dimensionnement, de la conception ou du contrôle.

\section{Electricité}

\subsection{Electrocinétique}

On défini une \emph{tension électrique} comme une différence de potentiels :

\[ 
U_{12} = \phi_2 - \phi_1
\]

On utilise également très naturellement la définition du courant électrique comme l'intégrale de la densité de courant sur une section de câble :

\[ 
I = \iint \vec{j} \cdot \vec{dS}
\]

Un dipôle électrique est une représentation d'un appareil électrique muni de deux bornes, sur la base desquelles toutes les grandeurs d'état du dispositif pourront-êtres calculées.

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
\centering
  \draw	(0,0) to[R, v>=$U$, i<^=$I$] (2,0);
\end{tikzpicture}
\end{center}
\shorthandon{:!}

En fait, un dipôle électrique est décrit par une équation différentielle qui relie $U$ et $I$ (éventuellement par le biais d'une ou plusieurs autres variables qui décriront explicitement le système, comme la position ou l'aimantation d'un élément du dipôle par exemple, et qui pourront introduire des non-linéarités).

Un dipôle électrique trouve donc à ses bornes et à chaque instant une tension électrique et est traversé par un courant électrique.

L'électrocinétique est un modèle très largement vérifié en pratique, qui fait l'hypothèse que la charge est incompressible (hypothèse issue des modèles de mécanique des fluides). La première conséquence de cette approximation est que les courants associés à chacune des bornes d'un dipôle sont en fait égaux.

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
\centering
  \draw	(0,0) to[R, v>=$U$, i<^=$I$, i^<=$I$] (2,0);
\end{tikzpicture}
\end{center}
\shorthandon{:!}

Plus largement, quand le courant en provenance d'un dipôle est réparti entre deux dipôles voisins, on saura que le courant reste conservé. Cette loi est désignée comme la \emph{loi de noeuds}. C'est à dire que dans l'exemple suivant :

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
  \draw (0,0) to[R, i>^=$I_3$] (0:2);
  \draw (0,0) to[R, i<^=$I_1$] (120:2);
  \draw (0,0) to[R, i<^=$I_2$] (240:2);
\end{tikzpicture}
\end{center}
\shorthandon{:!}

On a :

\[
I_3 = I_2 + I_1 
\]

Le sens de la flêche représentant le courant choisi est arbitraire, et la retourner ne fait qu'ajouter un signe \emph{moins}. Par exemple, la représentation suivante :

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
  \draw (0,0) to[R, i>^=$I_3$] (0:2);
  \draw (0,0) to[R, i<^=$I_1$] (120:2);
  \draw (0,0) to[R, i>^=$I_2$] (240:2);
\end{tikzpicture}
\end{center}
\shorthandon{:!}


Illustre la loi des noeuds de façon totalement équivalente à la précédente : 

\[ 
I_3 + I_2 = I_1 
\]

En outre, du fait que le champ électrique dérive d'un potentiel il est conservatif, ce qui explique que dans le schéma suivant :

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
  \draw (0,0) to[R, v^>=$U_1$] (0,2) 
  to[R, v^>=$U_2$] (0,4) -- (3,4)
  to[R, v^<=$U_3$] (3,0) -- (0,0);
\end{tikzpicture}
\end{center}
\shorthandon{:!}

On ait la \emph{loi des mailles} :

\[ 
U_3 = U_2 + U_1 
\]

De la même façon que pour le courant, le sens des flêches est conventionnel :

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
  \draw (0,0) to[R, v^>=$U_1$] (0,2) 
  to[R, v^< =$U_2$] (0,4) -- (3,4)
  to[R, v^<=$U_3$] (3,0) -- (0,0);
\end{tikzpicture}
\end{center}
\shorthandon{:!}

Et la même loi des mailles s'écrit pour ce schéma :

\[ 
U_3 + U_2 = U_1 
\]

L'idée de la méthode qui va être présentée - et qui est à la base du génie électrique - va consister à dessiner des schémas liants les différents dipôles électriques (chacun étant associé à ses deux grandeurs $U$ et $I$) afin de pouvoir écrire un système d'équations différentielles solvable.

Tout la puissance de cette méthode - par rapport aux formulations locales qui ont été discutées jusqu'à présent - réside dans le fait que les équations différentielles à résoudre sont \emph{ordinaires} et non \emph{aux dérivées partielles}, ce qui permet de s'émanciper de la géométrie des consituants par la mise en place d'un \emph{modèle externe}.

\textcolor{red}{(Ajouter un passage sur la puissance électrique)}

\subsection{Dipôles élémentaires}

Commençons par établir une liste des dipôles électriques couramment utilisés dans les schémas :

\subsubsection{La source de tension}

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
\centering
  \draw	(0,0) to[V, v=$E$] (2,0);
\end{tikzpicture}
\end{center}
\shorthandon{:!}

Ce dipôle permet d'enlever un degré de liberté au système en fixant une tension quelque part dans le schéma. Selon le sens du courant par rapport à cette tension (en fait le signe de la puissance $P=EI$), cette source peut absorber l'énergie ou en générer.

On décrira le comportement des composants atemporels par leur \emph{caractéristique de transfert}, et voici celle de la source de tension :

\begin{center}
\begin{tikzpicture}
// Axes
\draw[->] (-1,0) -- (1,0);
\draw (1,0) node[right] {$I$};
\draw [->] (0,0) -- (0,1);
\draw (0,1) node[above] {$U$};

// Contenu
\draw[very thick] (-1,0.5) -- (1,0.5);
\draw (0,0.5) node[below right] {$E$};
\end{tikzpicture}
\end{center}

\subsubsection{La source de courant}

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
\centering
  \draw	(0,0) to[I, i=$I$] (2,0);
\end{tikzpicture}
\end{center}
\shorthandon{:!}

Exactement de la même façon que la précédente, la source de courant fixe le courant quelque part dans le circuit :

\begin{center}
\begin{tikzpicture}
// Axes
\draw[->] (0,0) -- (1,0);
\draw (1,0) node[right] {$I$};
\draw [->] (0,-1) -- (0,1);
\draw (0,1) node[above] {$U$};

// Contenu
\draw[very thick] (0.5,-1) -- (0.5,1);
\draw (0.5,0) node[below right] {$I$};
\end{tikzpicture}
\end{center}

\subsubsection{La résistance}

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
\centering
  \draw	(0,0) to[R, v>=$U$, i<^=$I$] (2,0);
\end{tikzpicture}
\end{center}
\shorthandon{:!}

La résistance décrit des effets purement résistifs, dans un conducteur. C'est-à-dire que sa \emph{loi de comportement} (ce qui lie $U$ à $I$) est une simple relation de proportionnalité appellée \emph{loi d'Ohm} :

\[\boxed{
U = RI
}\]

Avec $R$ sa \emph{résistance}.

\begin{center}
\begin{tikzpicture}
// Axes
\draw[->] (-1,0) -- (1,0);
\draw (1,0) node[right] {$I$};
\draw [->] (0,-1) -- (0,1);
\draw (0,1) node[above] {$U$};

// Contenu
\draw[very thick] (-0.5,-1) -- (0.5,1);
\draw (0.2,0.7) node[below right] {$R$};
\end{tikzpicture}
\end{center}

\subsubsection{La capacité}

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
\centering
  \draw	(0,0) to[C, v>=$U$, i<^=$I$] (2,0);
\end{tikzpicture}
\end{center}
\shorthandon{:!}

Nous avons déjà étudié un exemple de condensateur \textcolor{red}{(en fait non mais faut le faire)}, dans lequel est présent un champ électrique qui vient contrecarrer les variations de tension. On décrit cette effet par une \emph{capacité} $C$ au travers d'une équation différentielle :

\[\boxed{
I = C\frac{dU}{dt}
}\]

La caractéristique de transfert ne peut-être tracée étant donné que le comportement de ce dipôle fait intervenir le temps.

\subsubsection{L'inductance}

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
\centering
  \draw	(0,0) to[L, v>=$U$, i<^=$I$] (2,0);
\end{tikzpicture}
\end{center}
\shorthandon{:!}

Un dispositif inductif fait avec le champ magnétique ce que la capacité fait avec le champ électrique, c'est à dire que :

\[\boxed{
U = L\frac{dI}{dt}
}\]

Remarquez qu'on ne considère ici qu'une inductance propre $L$, ce qui fonctionne pour un solénoïde simple mais pas pour une machine électrique par exemple.

\subsubsection{L'interrupteur}

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
\centering
  \draw	(0,0) to[switch, v<=$U$, i^>=$I$] (2,0);
\end{tikzpicture}
\end{center}
\shorthandon{:!}

Un grand nombre de composants mécanique, électronique ou électrotechnique peuvent en fait faire office d'interrupteur. Ceux-ci servent à ouvrir ou fermer une branche du circuit. Selon qu'il est ouvert ou fermé, on peut ou bien remplacer un interrupteur par une source de courant nulle ou une source de tension nulle.

Parce-qu'une commande extérieure intervient, la caractéristique de transfert n'est pas un outil utilisable en tant que tel pour décrire l'interrupteur. On peut toutefois imaginer une représentation moins juste mais qui resterait parlante :

\begin{center}
\begin{tikzpicture}
// Axes
\draw[->] (-1,0) -- (1,0);
\draw (1,0) node[right] {$I$};
\draw [->] (0,-1) -- (0,1);
\draw (0,1) node[above] {$U$};

// Contenu
\draw[very thick] (0.1,-1) -- (0.1,1);
\draw[very thick] (-1,0.1) -- (1,0.1);
\draw[very thick] (0.2,1) -- (1,1);
\draw[<-, very thick] (1,0.15) -- (1,1);
\draw[very thick] (0.8,0.15) -- (0.8,0.8);
\draw[<-, very thick] (0.15,0.8) -- (0.8,0.8);
\end{tikzpicture}
\end{center}

Ici, on dit que l'\emph{amorçage} et le \emph{blocage} de l'interrupteur sont \emph{commandés}.

\subsubsection{La diode}

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
\centering
  \draw	(0,0) to[D, v<=$U$, i^>=$I$] (2,0);
\end{tikzpicture}
\end{center}
\shorthandon{:!}

La diode est une composant non-linéaire (contrairement à tous les précédents à l'exception du dernier) qui ne tolère une circulation du courant que dans un seul sens. C'est à dire que sa caractéristique de transfert est représentée par :

\begin{center}
\begin{tikzpicture}
// Axes
\draw[->] (0,0) -- (1,0);
\draw (1,0) node[right] {$I$};
\draw [->] (0,-1) -- (0,1);
\draw (0,1) node[above] {$U$};

// Contenu
\draw[very thick] (0.1,-0.1) -- (1,-0.1);
\draw[very thick] (0.1,-0.1) -- (0.1,-1);
\end{tikzpicture}
\end{center}

\subsubsection{Le thyristor à amorçage commandé}

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
\centering
  \draw	(0,0) to[thyristor, v<=$U$, i^>=$I$] (2,0);
  \draw (1.75,0.8) node[below left] {$a$};
\end{tikzpicture}
\end{center}
\shorthandon{:!}

Ce composant est une diode qui doit être \emph{amorçé} par le biais d'une \emph{gâchette} une fois qu'il a été bloqué :

\begin{center}
\begin{tikzpicture}
// Axes
\draw[->] (0,0) -- (1,0);
\draw (1,0) node[right] {$I$};
\draw [->] (0,-1) -- (0,1);
\draw (0,1) node[above] {$U$};

// Contenu
\draw[very thick] (0.1,-1) -- (0.1,1);
\draw[very thick] (0.2,1) -- (1,1);
\draw[<-, very thick] (1,0.05) -- (1,1);
\draw[<-, very thick] (0.15,-0.12) -- (1,-0.12);
\end{tikzpicture}
\end{center}

\subsubsection{Le thyristor à blocage commandé}

Ce composant devient spontanément une diode quand sa tension devient négative, puis peut-être bloqué par le biais de sa gachette.

\begin{center}
\begin{tabular}{cc}

\shorthandoff{:!}
\begin{tikzpicture}
\centering
  \draw	(0,0) to[thyristor, v<=$U$, i^>=$I$] (2,0);
  \draw (1.75,0.8) node[below left] {$b$};
\end{tikzpicture}
\shorthandon{:!} &

\begin{tikzpicture}
// Axes
\draw[->] (0,0) -- (1,0);
\draw (1,0) node[right] {$I$};
\draw [->] (0,-1) -- (0,1);
\draw (0,1) node[above] {$U$};

// Contenu
\draw[<-, very thick] (0.15,0.05) -- (0.15,1);
\draw[<-, very thick] (0.2,1) -- (1,1);
\draw[very thick] (1,0.05) -- (1,1);
\draw[very thick] (0.1,-0.1) -- (1,-0.1);
\draw[very thick] (0.1,-0.1) -- (0.1,-1);
\end{tikzpicture}


\end{tabular}
\end{center}

\subsection{Réduction des circuits électriques}

\subsubsection{Théorème de Thevenin-Norton}

\subsubsection{Dualité}

La dualité est un concept puissant qui émane du fait que des valeurs couplées parsèment toute la théorie de l'électromagnétisme.

Le dual d'un composant a le comportement donné lorsqu'on inverse $U$ et $I$ dans les équations dudit composant. Le tableau suivant en donne les exemples fondamentaux :

\begin{center}
\begin{tabular}{cc|cc}

\shorthandoff{:!}
\begin{tikzpicture}
\centering
  \draw	(0,0) to[V, v=$E$] (2,0);
\end{tikzpicture}
\shorthandon{:!} &

\begin{tikzpicture}
// Axes
\draw[->] (-1,0) -- (1,0);
\draw (1,0) node[right] {$I$};
\draw [->] (0,0) -- (0,1);
\draw (0,1) node[above] {$U$};

// Contenu
\draw[very thick] (-1,0.5) -- (1,0.5);
\draw (0,0.5) node[below right] {$E$};
\end{tikzpicture} &

\shorthandoff{:!}
\begin{tikzpicture}
\centering
  \draw	(0,0) to[I, i=$E$] (2,0);
\end{tikzpicture}
\shorthandon{:!} &

\begin{tikzpicture}
// Axes
\draw[->] (0,0) -- (1,0);
\draw (1,0) node[right] {$I$};
\draw [->] (0,-1) -- (0,1);
\draw (0,1) node[above] {$U$};

// Contenu
\draw[very thick] (0.5,-1) -- (0.5,1);
\draw (0.5,0) node[below right] {$E$};
\end{tikzpicture}

\\ \hline

\shorthandoff{:!}
\begin{tikzpicture}
\centering
  \draw	(0,0) to[R, v>=$U$, i<^=$I$, l=$R$] (2,0);
\end{tikzpicture}
\shorthandon{:!} &

\begin{tikzpicture}
// Axes
\draw[->] (-1,0) -- (1,0);
\draw (1,0) node[right] {$I$};
\draw [->] (0,-1) -- (0,1);
\draw (0,1) node[above] {$U$};

// Contenu
\draw[very thick] (-0.5,-1) -- (0.5,1);
\draw (0.2,0.7) node[below right] {$R$};
\end{tikzpicture} &

\shorthandoff{:!}
\begin{tikzpicture}
\centering
  \draw	(0,0) to[R, v>=$U$, i<^=$I$, l=$\frac{1}{R}$] (2,0);
\end{tikzpicture}
\shorthandon{:!} &

\begin{tikzpicture}
// Axes
\draw[->] (-1,0) -- (1,0);
\draw (1,0) node[right] {$I$};
\draw [->] (0,-1) -- (0,1);
\draw (0,1) node[above] {$U$};

// Contenu
\draw[very thick] (-1,-0.5) -- (1,0.5);
\draw (0.2,0.9) node[below right] {$\frac{1}{R}$};
\end{tikzpicture}

\\ \hline

\shorthandoff{:!}
\begin{tikzpicture}
\centering
  \draw	(0,0) to[C, v>=$U$, i<^=$I$, l=$C$] (2,0);
\end{tikzpicture}
\shorthandon{:!} &

$I = C\frac{dU}{dt}$ &

\shorthandoff{:!}
\begin{tikzpicture}
\centering
  \draw	(0,0) to[L, v>=$U$, i<^=$I$, l=$C$] (2,0);
\end{tikzpicture}
\shorthandon{:!} &

$U = C\frac{dI}{dt}$

\\ \hline

\shorthandoff{:!}
\begin{tikzpicture}
\centering
  \draw	(0,0) to[switch, v<=$U$, i^>=$I$] (2,0);
\end{tikzpicture}
\shorthandon{:!} &

\begin{tikzpicture}
// Axes
\draw[->] (-1,0) -- (1,0);
\draw (1,0) node[right] {$I$};
\draw [->] (0,-1) -- (0,1);
\draw (0,1) node[above] {$U$};

// Contenu
\draw[very thick] (0.1,-1) -- (0.1,1);
\draw[very thick] (-1,0.1) -- (1,0.1);
\draw[very thick] (0.2,1) -- (1,1);
\draw[<-, very thick] (1,0.15) -- (1,1);
\draw[very thick] (0.8,0.15) -- (0.8,0.8);
\draw[<-, very thick] (0.15,0.8) -- (0.8,0.8);
\end{tikzpicture} &

\shorthandoff{:!}
\begin{tikzpicture}
\centering
  \draw	(0,0) to[switch, v<=$U$, i^>=$I$] (2,0);
\end{tikzpicture}
\shorthandon{:!} &

\begin{tikzpicture}
// Axes
\draw[->] (-1,0) -- (1,0);
\draw (1,0) node[right] {$I$};
\draw [->] (0,-1) -- (0,1);
\draw (0,1) node[above] {$U$};

// Contenu
\draw[very thick] (0.1,-1) -- (0.1,1);
\draw[very thick] (-1,0.1) -- (1,0.1);
\draw[very thick] (0.2,1) -- (1,1);
\draw[<-, very thick] (1,0.15) -- (1,1);
\draw[very thick] (0.8,0.15) -- (0.8,0.8);
\draw[<-, very thick] (0.15,0.8) -- (0.8,0.8);
\end{tikzpicture}

\\ \hline

\shorthandoff{:!}
\begin{tikzpicture}
  \draw (0,0) to[D, v^=$U$, i=$I$] ++(2,0);
\end{tikzpicture}
\shorthandon{:!} &

\begin{tikzpicture}
// Axes
\draw[->] (0,0) -- (1,0);
\draw (1,0) node[right] {$I$};
\draw [->] (0,-1) -- (0,1);
\draw (0,1) node[above] {$U$};

// Contenu
\draw[very thick] (0.1,-0.1) -- (1,-0.1);
\draw[very thick] (0.1,-0.1) -- (0.1,-1);
\end{tikzpicture} &

\shorthandoff{:!}
\begin{tikzpicture}
  \draw (2,0) to[D, v>=$U$, i<_=$I$] ++(-2,0);
\end{tikzpicture}
\shorthandon{:!} &

\begin{tikzpicture}
// Axes
\draw[->] (-1,0) -- (1,0);
\draw (1,0) node[right] {$I$};
\draw [->] (0,0) -- (0,1);
\draw (0,1) node[above] {$U$};

// Contenu
\draw[very thick] (-0.1,0.1) -- (-1,0.1);
\draw[very thick] (-0.1,0.1) -- (-0.1,1);
\end{tikzpicture} 

\\ \hline

\shorthandoff{:!}
\begin{tikzpicture}
  \draw (0,0) to[Ty, v=$U$, i=$I$] ++(2,0);
  \draw (1.75,0.8) node[below left] {$a$};
\end{tikzpicture}
\shorthandon{:!} &

\begin{tikzpicture}
// Axes
\draw[->] (0,0) -- (1,0);
\draw (1,0) node[right] {$I$};
\draw [->] (0,-1) -- (0,1);
\draw (0,1) node[above] {$U$};

// Contenu
\draw[very thick] (0.1,-1) -- (0.1,1);
\draw[very thick] (0.2,1) -- (1,1);
\draw[<-, very thick] (1,0.05) -- (1,1);
\draw[<-, very thick] (0.15,-0.12) -- (1,-0.12);
\end{tikzpicture} &

\shorthandoff{:!}
\begin{tikzpicture}
  \draw (0,0) to[Ty] ++(1.5,0) -- ++(0,-1)
  to[D, v^>=$U$] ++(-1.5,0) -- ++(0,1);
  \draw (0,-0.5) -- ++(-0.5,0);
  \draw (1.5,-0.5) -- ++(0.5,0);
  \draw (1.5,0.8) node[below left] {$b$};
\end{tikzpicture}
\shorthandon{:!} &

\begin{tikzpicture}
// Axes
\draw[->] (-1,0) -- (1,0);
\draw (1,0) node[right] {$I$};
\draw [->] (0,0) -- (0,1);
\draw (0,1) node[above] {$U$};

// Contenu
\draw[very thick] (-1,0.1) -- (1,0.1);
\draw[very thick] (1,0.2) -- (1,1);
\draw[<-, very thick] (0.05,1) -- (1,1);
\draw[<-, very thick] (-0.12,0.15) -- (-0.12,1);
\end{tikzpicture}

\\ \hline

\shorthandoff{:!}
\begin{tikzpicture}
\centering
  \draw	(0,0) to[thyristor, v<=$I$, i^>=$I$] (2,0);
  \draw (1.75,0.8) node[below left] {$b$};
\end{tikzpicture}
\shorthandon{:!} &

\begin{tikzpicture}
// Axes
\draw[->] (0,0) -- (1,0);
\draw (1,0) node[right] {$I$};
\draw [->] (0,-1) -- (0,1);
\draw (0,1) node[above] {$U$};

// Contenu
\draw[<-, very thick] (0.15,0.1) -- (0.15,1);
\draw[<-, very thick] (0.2,1) -- (1,1);
\draw[very thick] (1,0.05) -- (1,1);
\draw[very thick] (0.1,-0.1) -- (1,-0.1);
\draw[very thick] (0.1,-0.1) -- (0.1,-1);
\end{tikzpicture} &

\shorthandoff{:!}
\begin{tikzpicture}
  \draw (0,0) to[Ty] ++(1.5,0) -- ++(0,-1)
  to[D, v^>=$U$] ++(-1.5,0) -- ++(0,1);
  \draw (0,-0.5) -- ++(-0.5,0);
  \draw (1.5,-0.5) -- ++(0.5,0);
  \draw (1.5,0.8) node[below left] {$a$};
\end{tikzpicture}
\shorthandon{:!} &

\begin{tikzpicture}
// Axes
\draw[->] (-1,0) -- (1,0);
\draw (1,0) node[right] {$I$};
\draw [->] (0,0) -- (0,1);
\draw (0,1) node[above] {$U$};

// Contenu
\draw[<-, very thick] (0.05,0.15) -- (1,0.15);
\draw[<-, very thick] (1,0.2) -- (1,1);
\draw[very thick] (0.05,1) -- (1,1);
\draw[very thick] (-0.1,0.1) -- (-0.1,1);
\draw[very thick] (-0.1,0.1) -- (-1,0.1);
\end{tikzpicture}

\end{tabular}
\end{center}

\textcolor{red}{(ajouter partie sur les montages duaux)}

\subsubsection{Associations de résistances}

\subsubsection{Calcul de tension}

\subsection{Régime alternatif monophasé}

\subsubsection{Impédance}

Avant de lire cette partie, vous êtes invité à reprendre le passage sur l'inégalité de Cauchy-Schwartz du chapitre sur les espaces fonctionnels. Nous avions écrit :

\[
\frac{||U||}{\aleph_1} = \frac{U_0}{\sqrt{2}}, \; \frac{||I||}{\aleph_1} = \frac{I_0}{\sqrt{2}}
\]

Et avions appelé $\frac{||U||}{\aleph_1}$ et $\frac{||U||}{\aleph_1}$ les valeurs efficaces des deux grandeurs, qu'on interprète en théorie du signal comme des écart-types au sens des statistiques \textcolor{red}{(éclaircir)}.

Une grandeur $G$ sinusoïdale peut s'écrire :

\[
G(t) = G_0 \Re e^{j(\omega t + \phi)}
\]

En introduisant sa valeur efficace :

\[
G(t) = \sqrt{2} \Re \left( \frac{||G||}{\aleph_1} e^{j\phi} e^{j\omega t}\right)
\]

La convention qui prévaut dans toute l'étude des régimes alternatif est de décrire $g$ par le nombre complexe :

\[
\underline{G} = \frac{||G||}{\aleph_1} e^{j\phi}
\]

De sorte que :

\[
G = \sqrt{2} \Re \left(\underline{G} e^{j\omega t}\right)
\]

La raison qu'on peut trouver à cette façon de faire est purement pratique : les complexes sont plus maléables et compacts dans leur écriture (surtout si l'on omet de spécifier le $\omega t$ comme dans $G$) que les fonctions trigonométriques.

On notera que la valeur efficace se retrouve être selon cette définition le module de $G$ :

\[
\frac{||G||}{\aleph_1} = |\underline{G}|
\]

En outre, on écrira les équations qui régissent un circuit en faisant intervenir les \emph{impédances} notées $Z$ qui coïncident avec la notion de résistance, mais contenant en outre l'information sur le déphasage tension-courant introduit. La loi d'ohm deviendra alors :

\[\boxed{
\underline{U} = \underline{Z}\underline{I}
}\]

Imposons une certaine tension $U = U_0 \cos(\omega t)$, notée sous forme complexe $\underline{U} = |\underline{U}| = \frac{U_0}{\sqrt{2}}$ (le zéro temporel est calé sur le maximum de $U$). Aux bornes d'une capacité :

\[ 
I = C\frac{dU}{dt}
\]

\[ 
I = CU_0\frac{d\cos(\omega t)}{dt}
\]

\[ 
I = -C\omega U_0\sin(\omega t)
\]

\[ 
I = -C\omega U_0\cos(\omega t-\frac{\pi}{2})
\]

Noté sous forme complexe :

\[ 
\underline{I} = -C\omega \underline{U} e^{-j\frac{\pi}{2}}
\]

\[ 
\underline{I} = jC\omega \underline{U}
\]

Dont découle que l'impédance d'une capacité est :

\[\boxed{
\underline{Z} = \frac{1}{jC\omega}
}\]

Je vous laisse vérifier que pour une inductance nous trouvons :

\[
\underline{Z} = \frac{L\omega}{-j}
\]

Et parce-que $\frac{1}{z} = \frac{z^*}{zz^*} = \frac{1}{||z||^2}$ :

\[\boxed{
\underline{Z} = jL\omega
}\]

Et bien sûr sans oublier le troisième dipôle passif linéaire élémentaire, la résistance :

\[\boxed{
\underline{Z} = R
}\]

Une fois ceci fait, absolument tous les théorèmes de la section précédente faisant intervenir des résistances en régime continu sont transposables aux impédances en régime alternatif \textcolor{red}{(pas fait, ajouter exemple)}.

\subsubsection{Puissance en régime alternatif}

L'adaptation du formalisme complexe aux considérations énergétiques demande quelques précautions. La puissance électrique est donnée par :

\[
P = U\times I
\]

En régime alternatif, sous forme complexe :

\[
P = \sqrt{2} \Re \left(\underline{U} e^{j\omega t}\right) \times \sqrt{2} \Re \left(\underline{I} e^{j\omega t}\right)
\]

La forme complexe n'est pas directement adapté au calcul sur les énergies, car le produit des parties réelles n'est pas égale à la partie réelle du produit. Soient $U$ et $I$ deux grandeurs alternatives déphasées d'un angle $\varphi$ :

\[
P = U_0\cos(\alpha)I_0\cos(\alpha+\varphi)
\]

Nous avons déjà évoqué cette question quand nous avions évoqué l'inégalité de Cauchy-Schwartz appliquée aux espaces fonctionnels. Nous avions trouvé que :

\[
P = \frac{\braket{U|I}}{\aleph_1}
\]

\[
P = \frac{1}{\aleph_1}\int_\mathbb{R} U\times I
\]

Nous amènent à définir dans l'espace des grandeurs moyennes les puissances actives, réactives et apparentes :

\[\boxed{
\left\{\begin{matrix}
S = \frac{||U||}{\aleph_1} \times \frac{||I||}{\aleph_1} \\
P = \frac{\braket{U|I}}{\aleph_1} \\
Q = \sqrt{S^2-P^2}
\end{matrix}\right.
}\]

La puissance active $P$ étant la seule travaillant sur le long terme.

Nous allons maintenant voir ce que représente physiquement cette puissance réactive $Q$. Revenons à l'expression de la puissance instantanée :

\[
p = U_0\cos(\alpha)I_0\cos(\alpha+\varphi)
\]

\[
p = \frac{U_0I_0}{2}\left(\cos(\varphi)+\cos(2\alpha+\varphi)\right)
\]

Il a suffit d'un bête théorème trigonométrique pour éclaircir le mystère.

La puissance instantanée comprend deux termes, le premier étant la puissance active qui travaille donc de façon constante avec $\Phi$, et le second étant la \emph{puissance fluctuente} qui échange de l'énergie avec le système à la pulsation du réseau et toutes les demi-périodes. Cet échange régulier d'énergie est malvenu, et vient contrarier le bon fonctionnement du réseau, l'obligeant à fonctionner en sous-régime au sens de la puissance active.

La valeur efficace de cette puissance fluctente sera toujours donnée par :

\[
\frac{||F||}{\aleph_1} = \frac{U_0I_0}{2\sqrt{2}}
\]

Qui ne dépend pas de $\varphi$, et est donc inévitable en régime monophasé, et correspond à un échange d'énergie toutes les demi-périodes entre la source et la charge. Au mieux (pour $\varphi=0$), le réseau et les installations devront-être surdimensionnés d'un facteur $1=\frac{1}{\sqrt{2}}$ pour ne pas être endommagées à cause de la contribution aux pertes joules de la puissance fluctente.

Tout ceci peut-être repris en notation complexe. Revenons à :

\[
P = \sqrt{2} \Re \left(\underline{U} e^{j\omega t}\right) \times \sqrt{2} \Re \left(\underline{I} e^{j\omega t}\right)
\]

Et écrivons les parties réelles sans utiliser de cosinus :

\[
P = \frac{1}{2}(\underline{U}e^{j\omega t}+\underline{U}^*e^{-j\omega t})(\underline{I}e^{j\omega t}+\underline{I}^*e^{-j\omega t})
\]

\[
P = \frac{1}{2}(\underline{U}\underline{I}e^{j2\omega t}+\underline{U}^*\underline{I}^*e^{-j2\omega t} + \underline{U}\underline{I}^* + \underline{U}^*\underline{I})
\]

\[
P = \Re \left( \underline{U}\underline{I}^* \right) + \Re \left( \underline{U}\underline{I}e^{j2\omega t} \right)
\]

Si l'on veut faire les choses jusqu'au bout, on ira jusqu'à distinguer $S = UI^*$ la représentation complexe de la puissance utile - qui ne pulse pas et transite continuement d'un organe vers un autre - et $S_f = \frac{UI}{\sqrt{2}}$ celle de la puissance fluctuente, pulsant à $2\omega$ et dont le module $|S_f| = \frac{|U||I|}{\sqrt{2}}$ est la valeur efficace.

\textcolor{red}{(faire le lien avec la puissance réactive s'il y en a un)}

\[
Q = \frac{U_0I_0}{2}\sin(\varphi)
\]

\textcolor{red}{(faire le lien avec la transformée de Fourier)}

\subsection{Régimes polyphasés}

\subsubsection{Régime diphasé}

\subsubsection{Régime triphasé}

En plus des dipôles que nous avons considéré jusqu'à présent, il existe des dispositifs munis de trois bornes ; les tripôles.

En régime monophasé nous venons de voir que la puissance instantanée en présence d'un déphasage entre le tension et le courant pouvait être fluctuante. Pour l'alimentation d'un moteur par exemple, ceci peut-être un désagrément de taille car le couple ne serait pas constant et ledit moteur fonctionnerait en sous-régime (puisque la puissance maximale ne serait pas débitée constamment). En outre, des problêmes de vibration mécaniques sont inévitablement induits.

La solution qui fût trouvée consiste à utiliser des machines triphasée. Leur étude induit une forme de complexité supplémentaire, mais l'intérêt pratique est important.

Soit une charge triphasée couplée \emph{en étoile} :

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
  \draw (0,0) to[R, i<^=$I_1$, v>=$V_1$] (0:2);
  \draw (0,0) to[R, i<^=$I_2$, v>=$V_2$] (120:2);
  \draw (0,0) to[R, i<^=$I_3$, v>=$V_3$] (240:2);
\end{tikzpicture}
\end{center}
\shorthandon{:!}

Cette représentation est relativement eronnée, car il serait faux de penser qu'un tripôle est simplement constitué de trois dipôles. En effet, comme nous l'avons sous-entendu lorsque nous avions présenté la matrice inductance, les grandeurs dans l'une des phase peuvent avoir des répercussions sur celles des autres phases (par le biais des inductances mutuelles).

Le noeud central est appellé \emph{neutre}, et peut-être relié à un élément extérieur au tripôle (bien souvent à la Terre à travers un dipôle).

Dans un souci de praticité, on définira les vecteurs $I_i$ et $V_i$ qui ont pour composante les différentes valeurs instantanées des grandeurs en présence, exactement de la même manière que nous l'avions fait pour introduire la matrice inductance. D'ailleurs, une charge triphasée peut-être définie par une \emph{matrice impédance}.

La manipulation de ces deux vecteurs n'est cependant pas toujours la plus pratique. C'est pourquoi fût inventée la \emph{transformation de Fortescue} qui associe à une grandeur triphasée réelle $G_i$ le complexe $G_d$ et le réel $G_h$ :

\[\boxed{
\left( \begin{matrix}
G_d \\
G_d^* \\
G_h
\end{matrix} \right)
=\frac{1}{\sqrt{3}}
\left( \begin{matrix}
1 & a & a^2 \\
1 & a^2 & a \\
1 & 1 & 1
\end{matrix} \right)
\left( \begin{matrix}
G_1 \\
G_2 \\
G_3
\end{matrix} \right)
= K_i^j G_j
}\]

Avec $a=e^{j\frac{2\pi}{3}}$ et $K_i^j =\frac{1}{\sqrt{3}}
\left( \begin{matrix}
1 & a & a^2 \\
1 & a^2 & a \\
1 & 1 & 1
\end{matrix} \right)$ la \emph{matrice de Fortescue}.

Les deux premières composantes du vecteur résultant sont conjugées, car :

\textcolor{red}{(justification)}

On ne s'intéressera qu'à la première - dite \emph{composante directe} - ainsi qu'à la dernière qui prend le nom de \emph{composante homopolaire}.

La matrice inverse de $K_i^j$ est sa transconjugée $\frac{1}{\sqrt{3}}
\left( \begin{matrix}
1 & 1 & 1 \\
a^2 & a & 1 \\
a & a^2 & 1
\end{matrix} \right)$.

\subsubsection{Couplages}

Reprenons notre charge en étoile, et intéressons-nous aux tensions entre phases (qui est ce que nous imposons bien souvent en pratique) :

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
  \draw (0,0) to[R, i<^=$I_1$, v>=$V_1$] (0:2);
  \draw (0,0) to[R, i<^=$I_2$, v>=$V_2$] (120:2);
  \draw (0,0) to[R, i<^=$I_3$, v>=$V_3$] (240:2);
  \draw (240:2) to[open, v>=$U_{13}$] (0:2)
  to[open, v>=$U_{21}$] (120:2)
  to[open, v>=$U_{32}$] (240:2);
\end{tikzpicture}
\end{center}
\shorthandon{:!}

Laissons-nous convaincre que les lois des mailles s'écrivent :

\[
\left( \begin{matrix}
U_{13} \\
U_{21} \\
U_{32}
\end{matrix} \right)
=
\left( \begin{matrix}
1 & 0 & -1 \\
-1 & 1 & 0 \\
0 & -1 & 1
\end{matrix} \right)
\left( \begin{matrix}
V_1 \\
V_2 \\
V_3
\end{matrix} \right)
\]

Ou en faisant apparaître les transformations de Fortescue :

\[
K_j^i \left( \begin{matrix}
U_d \\
U_d^* \\
U_h
\end{matrix} \right)
=
\left( \begin{matrix}
1 & 0 & -1 \\
-1 & 1 & 0 \\
0 & -1 & 1
\end{matrix} \right)
K_j^i \left( \begin{matrix}
V_d \\
V_d^* \\
V_h
\end{matrix} \right)
\]

\[
\left( \begin{matrix}
U_d \\
U_d^* \\
U_h
\end{matrix} \right)
=
K_i^j \left( \begin{matrix}
1 & 0 & -1 \\
-1 & 1 & 0 \\
0 & -1 & 1
\end{matrix} \right)
K_j^i \left( \begin{matrix}
V_d \\
V_d^* \\
V_h
\end{matrix} \right)
\]

Ce qui vient d'être fait n'est rien d'autre qu'un changement de base. C'est en calculant la nouvelle matrice qu'on réalise avoir affaire à une parfaite illustration de l'intéret de la transformation de Fortescue pour l'étude de système triphasé :

\[
\left( \begin{matrix}
U_d \\
U_d^* \\
U_h
\end{matrix} \right)
=
\left( \begin{matrix}
\sqrt{3}e^{-j\frac{\pi}{6}} & 0 & 0 \\
0 & \sqrt{3}e^{j\frac{\pi}{6}} & 0 \\
0 & 0 & 0
\end{matrix} \right)
\left( \begin{matrix}
V_d \\
V_d^* \\
V_h
\end{matrix} \right)
\]

Autrement dit, la composant homopolaire $U_h$ est nulle en toutes circonstances, tandis que $U_d$ est augmentée d'un facteur $\sqrt{3}$ et déphasée de $\frac{\pi}{6}$. 

Pour illustrer ce qui vient d'être dit, prenons l'exemple du régime sinusoïdal équilibré (pris par convention, \emph{direct}), pour lequel on a :

\[
\left( \begin{matrix}
V_1 \\
V_2 \\
V_3
\end{matrix} \right)
= V_0 
\left( \begin{matrix}
\sin(\omega t) \\
\sin(\omega t + \frac{2\pi}{3}) \\
\sin(\omega t + \frac{4\pi}{3})
\end{matrix} \right)
\]

\textcolor{red}{(Mauvaise convention ! Les tensions de phases sont usuellement :)}

\[
\left( \begin{matrix}
V_1 \\
V_2 \\
V_3
\end{matrix} \right)
= V_0 
\left( \begin{matrix}
\sin(\omega t) \\
\sin(\omega t - \frac{2\pi}{3}) \\
\sin(\omega t - \frac{4\pi}{3})
\end{matrix} \right)
\]

Graphiquement, en posant $\alpha = \omega t$ :

\begin{sagesilent}
x=var('x');
t = var('t');
x_coords = [t for t in srange(0,2*pi,0.01)];
y1_coords = [sin(x).n() for x in x_coords];
y2_coords = [sin(x+2*pi/3).n() for x in x_coords];
y3_coords = [sin(x+4*pi/3).n() for x in x_coords];
output = "";
for i in range(0,len(x_coords)-1):
    output += "\draw[red, thick] ("+str(x_coords[i])+","+str(y1_coords[i])+")--("+str(x_coords[i+1])+","+str(y1_coords[i+1])+");\n";
    output += "\draw[blue, thick] ("+str(x_coords[i])+","+str(y2_coords[i])+")--("+str(x_coords[i+1])+","+str(y2_coords[i+1])+");\n";
    output += "\draw[green, thick] ("+str(x_coords[i])+","+str(y3_coords[i])+")--("+str(x_coords[i+1])+","+str(y3_coords[i+1])+");\n";
\end{sagesilent}

\begin{center}
\begin{tikzpicture}[x=5\textwidth/6/(2*pi)), y=\textwidth/3/1.73/2]
\pgfplotsset{width=15*\textwidth/16,height=5*\textheight/16,compat=newest}
	// Axes
	\draw[->] (0,0) -- (2*pi+0.2,0) node[right] {$\alpha \; [rad]$};
	\draw[->] (0,-1.1) -- (0,1.2) node[above] {$V_i(\alpha)$};
	//Graduation angulaire
	\foreach \x in {1,2,...,12}
     	\draw (3.1416/6*\x,0.08) -- (3.1416/6*\x,-0.08)
		node[anchor=north] {$\frac{\x \pi}{6}$};
		
	\draw (0,1) node[left] {$V_0$};
	\draw[dotted] (0,1) -- ++(3.1416/2,0);
	
	\draw (2*3.1416/3,0.8) node[anchor=south west] {\textcolor{red}{$V_1$}};
	\draw (5*3.1416/3,0.8) node[anchor=north west] {\textcolor{blue}{$V_2$}};
	\draw (4*3.1416/3,0.8) node[anchor=south west] {\textcolor{green}{$V_3$}};
	
	
	\if\releaseversion1
	\sagestr{output}
	\fi
\end{tikzpicture}
\end{center}

Et les tensions entre phases :

\begin{sagesilent}
x=var('x');
t = var('t');
x_coords = [t for t in srange(0,2*pi,0.01)];
y1_coords = [sin(x-pi/6).n() for x in x_coords];
y2_coords = [sin(x+2*pi/3-pi/6).n() for x in x_coords];
y3_coords = [sin(x+4*pi/3-pi/6).n() for x in x_coords];
output = "";
for i in range(0,len(x_coords)-1):
    output += "\draw[red, thick] ("+str(x_coords[i])+","+str(y1_coords[i])+")--("+str(x_coords[i+1])+","+str(y1_coords[i+1])+");\n";
    output += "\draw[blue, thick] ("+str(x_coords[i])+","+str(y2_coords[i])+")--("+str(x_coords[i+1])+","+str(y2_coords[i+1])+");\n";
    output += "\draw[green, thick] ("+str(x_coords[i])+","+str(y3_coords[i])+")--("+str(x_coords[i+1])+","+str(y3_coords[i+1])+");\n";
\end{sagesilent}

\begin{center}
\begin{tikzpicture}[x=5\textwidth/6/(2*pi)), y=\textwidth/3/2]
	// Axes
	\draw[->] (0,0) -- (2*pi+0.2,0) node[right] {$\alpha \; [rad]$};
	\draw[->] (0,-1.1) -- (0,1.2) node[above] {$U_i(\alpha)$};
	//Graduation angulaire
	\foreach \x in {1,2,...,12}
     	\draw (3.1416/6*\x,0.08) -- (3.1416/6*\x,-0.08)
		node[anchor=north] {$\frac{\x \pi}{6}$};
		
	\draw (0,1) node[left] {$U_0$};
	
	\draw (5*3.1416/6,0.8) node[anchor=south west] {\textcolor{red}{$U_{13}$}};
	\draw (11*3.1416/6,0.8) node[anchor=north west] {\textcolor{blue}{$U_{21}$}};
	\draw (9*3.1416/6,0.8) node[anchor=south west] {\textcolor{green}{$U_{32}$}};
	\if\releaseversion1
	\sagestr{output}
	\fi
\end{tikzpicture}
\end{center}

\[
\left( \begin{matrix}
U_{13} \\
U_{21} \\
U_{32}
\end{matrix} \right)
= V_0 \sqrt{3}
\left( \begin{matrix}
\sin(\omega t - \frac{\pi}{6}) \\
\sin(\omega t + \frac{\pi}{2}) \\
\sin(\omega t + \frac{7\pi}{6})
\end{matrix} \right)
\]

Une charge triphasée peut également se présenter comme telle :

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
  \draw (240:2) to[R, i<^=$J_{13}$, v>=$U_{13}$] (0:2)
  to[R, i<^=$J_{21}$, v>=$U_{21}$] (120:2)
  to[R, i<^=$J_{32}$, v>=$U_{32}$] (240:2);
  \draw (0:2) to[short, i<^=$I_1$] ++(0:0.5);
  \draw (120:2) to[short, i<^=$I_2$] ++(120:0.5);
  \draw (240:2) to[short, i<^=$I_3$] ++(240:0.5);
\end{tikzpicture}
\end{center}
\shorthandon{:!}

On parle de couplage \emph{triangle}. Les lois des noeuds s'y écrivent :

\[
\left( \begin{matrix}
I_1 \\
I_2 \\
I_3
\end{matrix} \right)
=
\left( \begin{matrix}
1 & -1 & 0 \\
0 & 1 & -1 \\
-1 & 0 & 1
\end{matrix} \right)
\left( \begin{matrix}
J_{13} \\
J_{21} \\
J_{32}
\end{matrix} \right)
\]

Par un raisonnement exactement semblable au précédent, on aboutit à :

\[
\left( \begin{matrix}
I_d \\
I_d^* \\
I_h
\end{matrix} \right)
=
\left( \begin{matrix}
\sqrt{3}e^{j\frac{\pi}{6}} & 0 & 0 \\
0 & \sqrt{3}e^{-j\frac{\pi}{6}} & 0 \\
0 & 0 & 0
\end{matrix} \right)
\left( \begin{matrix}
J_d \\
J_d^* \\
J_h
\end{matrix} \right)
\]

Qui traduit une situation analogue à ce qu'on trouvait pour les tensions dans l'étoile.

\textcolor{red}{(Démontrer le théorème de Kenelly)}

\subsubsection{Régime triphasé équilibré}

Soit une charge triphasée couplée \emph{en étoile} :

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
  \draw (0,0) to[R, l=$Z_1$, i<^=$I_1$, v>=$V_1$] (0:2);
  \draw (0,0) to[R, l=$Z_2$, i<^=$I_2$, v>=$V_2$] (120:2);
  \draw (0,0) to[R, l=$Z_3$, i<^=$I_3$, v>=$V_3$] (240:2);
\end{tikzpicture}
\end{center}
\shorthandon{:!}

Le noeud central est appellé \emph{neutre}, et n'est ici relié d'aucune façon à un élément extérieur au tripôle. Cela engendre la relation :

\[
I_1 + I_2 + I_3 = 0
\]

Et si la source est \emph{équilibrée}, c'est à dire $Z_1 = Z_2 = Z_3$ :

\[
V_1 + V_2 + V_3 = 0
\]

On parle de \emph{régime équilibré} si cette relation est vérifiée (ce qui, en principe, présuppose l'équilibrage de la source en plus de celui de la charge).

\textcolor{red}{(Démontrer que la composante homopolaire ne travaille pas)}


\section{Magnétisme}

\subsection{Réluctance}

\subsection{Principe des travaux virtuels}

Jusqu'à présent, nous n'avons considéré que des systèmes inertes, sans mouvement mécanique. Dans cette partie, nous allons voir qu'un système magnétique peut apporter un travail mécanique, ce qui est à la base des machines électriques et autres actionneurs électromécaniques.

\subsubsection{Système inductif}

Un système magnétique linéaire (ou linéarisé) est décrit par une matrice inductance $L^{\mu}_{\;\nu}$ qui dépend des coordonées généralisés $q_i$.

Le principe des travaux virtuels se base sur l'expression de la force généralisée en mécanique :

\[ 
\vec{Q} = -\nabla W
\]

Moyennant une expression des dérivées de $W$ dans le système de coordonné adapté, on peut ainsi trouver les forces généralisées de notre problême :

\[ 
\left(\begin{matrix}
Q_1 \\
Q_2 \\
... \\
Q_n
\end{matrix}\right)
=
\left(\begin{matrix}
\frac{\partial W}{\partial q_1} \\
\frac{\partial W}{\partial q_2} \\
... \\
\frac{\partial W}{\partial q_n} \\
\end{matrix}\right)
\]

On appelle cette méthode ``principe des travaux virtuels'' car cela revient à effectuer des déplacements infinitésimaux (même si ces déplacements n'auront pas forcément lieu) pour regarder le travail engendré.

Dans le cadre d'une machine électrique (terme employé pour désigner les convertisseurs électro-magnéto-mécaniques tels les moteurs), l'énergie s'écrit \textcolor{red}{(justifier)} :

\[ 
W = \int_0^\Phi i_i(\varphi) d\varphi^i
\]

Mais dans le cadre de la modélisation linéaire qu'on s'est donné :

\[ 
W = \int_0^I i_i L^{i}_{\; j} di^j
\]

\[ 
W = \frac{1}{2} I_i L^{i}_{\; j} I^j
\]

Ainsi le principe des travaux virtuels nous donne :

\[ 
\vec{Q} = -\nabla \left( \frac{1}{2} I_i L^{i}_{\; j} I^j \right)
\]

\[ 
Q_k = -\frac{1}{2} I_i \partial_k L^{i}_{\; j} I^j
\]

Un tenseur de rang $3$ apparaissant au milieu de l'équation, nous allons  chercher à garder les pieds sur terre et simplifier le problême en ne considérant qu'un seul degré de liberté (par exemple une distance $x$) :

\[ 
F_x = -\frac{1}{2} I_i \frac{\partial L^{i}_{\; j}}{\partial x} I^j
\]

On voit alors qu'il suffit d'être capable de mesurer la variation de la matrice inductance selon un déplacement virtuel $\delta x$ pour connaître la force $F_x$ selon ce déplacement.

Autrement dit, dans le cas général connaître la matrice inductance dans toute situation mécanique possible revient à savoir écrire les équations du mouvement.

C'est la méthode des travaux virtuels, méthode que nous appliquerons à différents actionneurs et machines électriques dans le chapitre suivant.

\subsubsection{Système non-linéaire}

Dans le cas où les inductances ne dépendent pas seulement des coordonnées généralisées mais également de l'aimantation des matériaux, on repart de l'expression suivante de l'énergie :

\[ 
W = \int_0^\Phi i_i(\varphi) d\varphi^i
\]

Bien souvent, il est plus naturel de préférer travailler avec sa transformée de Legendre, qu'on appelle \emph{coénergie} :

\[ 
W = \Phi_i I^i - \int_0^I \varphi_i(i) di^i
\]

\[ 
W = \Phi_i I^i - CoW
\]

L'intéret de la coénergie sur l'énergie est purement pratique : on est plus souvent amené à travailler avec des flux fonctions des courants que l'inverse ; l'intégration est facilitée dans le sens de la coénergie.

Dans le cas non-linéaire, on aura néanmoins probablement recours aux éléments finis pour calculer la coénergie. Mais une fois ceci effectué, on peut calculer les forces généralisées aussi simplement qu'avec l'énergie :

\[ 
\vec{Q} = -\nabla W
\]

\[ 
\vec{Q} = -\nabla \left( \Phi_i I^i \right) + \nabla CoW
\]

Et parceque $\nabla \left( \Phi_i I^i \right) = 0$ \textcolor{red}{(oui mais pourquoi ?)} :

\[ 
\vec{Q} = \nabla CoW
\]

Toute la difficulté de cette méthode réside donc dans notre capacité à calculer l'énergie ou la coénergie du système, mais elle largement plus flexible qu'un calcul des forces volumiques locales et leur intégration par élément fini.

\textcolor{red}{(Ajouter calcul de force du problême d'induction)}

\chapter{Electromagnétisme appliqué}

\section{Réseaux électriques}

Le transport de l'énergie est un grand enjeu sociétal, relié à la problématique de l'épuisement des ressources minérales. Le réseau électrique est une solution qui cumule beaucoup d'avantage mais nécessite une importante quantité de métal.

\subsection{Ligne bifilaire}

En première approximation, un fil électrique est un équipotentiel, c'est-à-dire que la tension à ses bornes est nulle et parcouru en tout point par le même courant. Dans ce chapite, nous allons très souvent proposer un modèle idéalisé à l'extrême pour ensuite l'enrichir afin de prendre en compte des effets plus subtiles.

Ce modêle du fil équipotentiel ne nécessite cependant pas d'être discuté, alors présentons dès à présent le second modèle.

Pour qu'une transmission moyenne de l'énergie soit possible, une ligne électrique doit nécessairement comporter deux conducteurs isolés. L'ensemble de ces deux conducteurs forme un système plus complexe que le fil seul, car les deux potentiels et courants interagissent par le biais de capacités et inductances mutuelles.

\textcolor{red}{(compléter avec le calcul de C et L dans la ligne bifilaire)}

Le modèle sans perte qu'on considère est donc le suivant, pour une tranche infinitésimale de la ligne :

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
  \draw	(0,0) to[L, l=$L\; dx$, i>^=$I(x)$, *-] ++(4,0) node[shape=coordinate](A){} to[short] ++(0,-1) node[shape=coordinate](B){} to[C, l_=$C\; dx$] ++(0,-2) -- ++(0,-1) node[shape=coordinate](C){} to[short, -*] ++(-4,0);
  \draw (C) to[short, -*] ++(1,0);
  \draw (A) to[short, i=$I(x+dx)$, -*] ++(1,0);
  \draw (0,-4) to[open, v^>=$V(x)$, -*] ++(0,4);
  \draw (5,-4) to[open, v>=$V(x+dx)$, -*] ++(0,4);
\end{tikzpicture}
\end{center}
\shorthandon{:!}

La mise en équation se fait très simplement, nous adoptons immédiatement la notation complexe :

\[
\frac{\partial}{\partial x}\left(\begin{matrix}
\underline{I} \\
\underline{V}
\end{matrix}\right)
=
\left(\begin{matrix}
0 & -jC\omega \\
-jL\omega & 0
\end{matrix}\right)
\left(\begin{matrix}
\underline{I} \\
\underline{V}
\end{matrix}\right)
\]

Et par linéarité, on peut dériver de nouveau pour obtenir une matrice diagonale :

\[
\frac{\partial^2}{\partial x^2}\left(\begin{matrix}
\underline{I} \\
\underline{V}
\end{matrix}\right)
=
\left(\begin{matrix}
0 & -jC\omega \\
-jL\omega & 0
\end{matrix}\right)
\left(\begin{matrix}
0 & -jC\omega \\
-jL\omega & 0
\end{matrix}\right)
\left(\begin{matrix}
\underline{I} \\
\underline{V}
\end{matrix}\right)
\]

\[
\frac{\partial^2}{\partial x^2}\left(\begin{matrix}
\underline{I} \\
\underline{V}
\end{matrix}\right)
=
\left(\begin{matrix}
-LC\omega^2 & 0 \\
0 & -LC\omega^2
\end{matrix}\right)
\left(\begin{matrix}
\underline{I} \\
\underline{V}
\end{matrix}\right)
\]

\[
\frac{\partial^2}{\partial x^2}\left(\begin{matrix}
\underline{I} \\
\underline{V}
\end{matrix}\right)
=-LC\omega^2
\left(\begin{matrix}
\underline{I} \\
\underline{V}
\end{matrix}\right)
\]

Cette dernière équation est une équation d'onde \textcolor{red}{(pas parlé avant)}, ce qu'on conçoit mieux en abandonnant le formalisme complexe :

\[
\frac{\partial^2}{\partial x^2}\left(\begin{matrix}
I \\
V
\end{matrix}\right)
-LC
\frac{\partial^2}{\partial t^2}\left(\begin{matrix}
I \\
V
\end{matrix}\right)=0
\]

\textcolor{red}{(compléter avec transmission et réflexion)}

On peut modéliser les pertes dans les conducteurs par effet joule et effet couronne \textcolor{red}{(pas parlé avant)} par deux résistances disposées comme suit :

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
  \draw	(0,0) to[R, l=$R\; dx$, *-] ++(2,0) to[L, l=$L\; dx$, i>_=$I(x)$] ++(2,0) node[shape=coordinate](A){} to[short] ++(0,-1) node[shape=coordinate](B){} -- ++(-0.5,0) to[R, l_=$\frac{dx}{G}$] ++(0,-2) - - ++(0.5,0) -- ++(0,-1) node[shape=coordinate](C){} to[short, -*] ++(-4,0);
  \draw (B) -- ++(0.5,0) to[C, l=$C\; dx$] ++(0,-2) -- ++(-0.5,0);
  \draw (A) to[short, i=$I(x+dx)$, -*] ++(1,0);
  \draw (C) to[short, -*] ++(1,0);
\end{tikzpicture}
\end{center}
\shorthandon{:!}

Je vous laisse vous convaincre que l'équation devient :

\[
\frac{\partial^2}{\partial x^2}\left(\begin{matrix}
\underline{I} \\
\underline{V}
\end{matrix}\right)
=(R+jL\omega)(G+jC\omega)
\left(\begin{matrix}
\underline{I} \\
\underline{V}
\end{matrix}\right)
\]

\[
\frac{\partial^2}{\partial x^2}\left(\begin{matrix}
\underline{I} \\
\underline{V}
\end{matrix}\right)
=(RG+j(RC+LG)\omega-LC\omega^2)
\left(\begin{matrix}
\underline{I} \\
\underline{V}
\end{matrix}\right)
\]

En posant $\gamma = \sqrt{(R+jL\omega)(G+jC\omega)}$ :

\[
\frac{\partial^2}{\partial x^2}\left(\begin{matrix}
\underline{I} \\
\underline{V}
\end{matrix}\right)
=\gamma^2
\left(\begin{matrix}
\underline{I} \\
\underline{V}
\end{matrix}\right)
\]

Les solutions sont de la forme \textcolor{red}{(intervertion $V$ et $I$)} :

\[\boxed{
\left(\begin{matrix}
\underline{V} \\
\underline{I}
\end{matrix}\right)(x)
=\left(\begin{matrix}\cosh(\gamma x) & -\sqrt{\frac{G+jC\omega}{R+jL\omega}}\sinh(\gamma x) \\
-\sqrt{\frac{R+jL\omega}{G+jC\omega}}\sinh(\gamma x) & \cosh(\gamma x)\end{matrix}\right)
\left(\begin{matrix}
\underline{V} \\
\underline{I}
\end{matrix}\right)(0)
}\]

\textcolor{red}{(justifier les coefficients)}

Qu'on développe au second ordre, pour écarter les effets propagatifs :

\[
\left(\begin{matrix}
\underline{V} \\
\underline{I}
\end{matrix}\right)(x)
\sim \left(\begin{matrix}1+(R+jL\omega)(G+jC\omega)\frac{x^2}{2} & -(R+jL\omega)x \\
-(G+jC\omega)x & 1+(R+jL\omega)(G+jC\omega)\frac{x^2}{2}\end{matrix}\right)
\left(\begin{matrix}
\underline{V} \\
\underline{I}
\end{matrix}\right)(0)
\]

Pour une ligne de longueur $H$ petite devant la longueur d'onde, en posant $Z=(R+jL\omega)H$ et $Y=(G+jC\omega)H$ :

\[\boxed{
\left(\begin{matrix}
\underline{V} \\
\underline{I}
\end{matrix}\right)(H)
\sim \left(\begin{matrix}1+\frac{ZY}{2} & -Z \\
-Y & 1+\frac{ZY}{2}\end{matrix}\right)
\left(\begin{matrix}
\underline{V} \\
\underline{I}
\end{matrix}\right)(0)
}\]

\section{Dispositifs magnétiques}

\subsection{Transformateur monophasé}

Le transformateur monophasé est constitué d'un circuit magnétique muni de deux enroulements, désignés par \emph{primaire} et \emph{secondaire}.

\textcolor{red}{(figure)}

Les nombres de spires des enroulements sont notés respectivement $n_1$ et $n_2$. La perméabilité du fer est $\mu = 10000\mu_0$ qu'on supposera infinie.

Le calcul peut évidemment être mené avec d'autres matériaux, tant que la permabilité du circuit magnétique reste très grande devant celle du vide.

Cette très forte hypothèse ($\mu = \infty$) permet normalement de considérer un $\vec{H}$ nul partout dans le circuit, nécessaire pour que l'énergie magnétique soit finie. Néanmoins, cette façon de faire ne peut s'appliquer qu'à l'étude des machines électriques, moins à celle des transformateurs où il n'y a pas d'entrefer. En effet, puisque le circuit magnétique est fermé, on aurait sur tout un contour entourant des courants $\vec{H}=0$, ce qui est en contradiction avec Maxwell-Ampère.

Pour cette raison, on considèrera un $\mu$ fini mais suffisament élevé pour que le flux de fuite puisse être négligé (toutes les lignes de champs sont canalisées dans le matériaux).

Le théorème d'Ampère s'écrit :

\[
\oint  \vec{H} \cdot \vec{dl} = \iint \vec{j} \cdot \vec{dS}
\]

Dans ces conditions, on est amené à utiliser l'\emph{hypothèse de la fibre moyenne} qui revient à considérer un $\vec{H}$ constant sur toute section droite du circuit magnétique. Plus fort encore, si l'on suppose que ladite section est de surface $S$ constante et si ce circuit est de longueur totale $l$, on a partout :

\[
||\vec{H}|| = \frac{n_1 I_1 + n_2 I_2}{l}
\]

Puisque le matériaux est linéaire, on trouve très rapidement l'expression du flux magnétique $\Phi$ :

\[
\Phi = S||\vec{B}|| = \mu S||\vec{B}|| = \frac{\mu S}{l}(n_1 I_1 + n_2 I_2)
\]

Maxwell-Thomson implique que le flux est conservé le long du circuit magnétique. Maintenant, on effectue une transformation supplémentaire en introduisant les flux $\Phi_1$ et $\Phi_2$ vus par les bobines (ce qui permet de traiter les problême comme si chaque bobine ne comportait qu'une seule spire :

\[
\left( \begin{matrix} \Phi_1 \\ \Phi_2 \end{matrix} \right) = 
\left( \begin{matrix} n_1 \\ n_2 \end{matrix} \right)\Phi
\]

Nous faisons finalement apparaître la matrice inductance :

\[
\left( \begin{matrix} \Phi_1 \\ \Phi_2 \end{matrix} \right) = 
\left( \begin{matrix}  
\frac{\mu S}{l}n_1^2 & \frac{\mu S}{l}n_1 n_2 \\
\frac{\mu S}{l}n_1 n_2 & \frac{\mu S}{l}n_2^2
\end{matrix} \right)
\left( \begin{matrix} I_1 \\ I_2 \end{matrix} \right)
\]

Si ce raisonnement permet de très bien cerner ce qu'il advient dans un transformateur, il n'en reste pas moins excessivement simpliste ; négligeant notamment de par ses hypothèses le flux de fuite et les pertes fer (qu'on rappelle êtres dues conjointement aux courants de Foucault et à la saturation magnétique).

Pour aller plus loin, on peut utiliser la méthode des éléments finis dont la modélisation interne d'un transformateur est un des exercices les plus simples. On peut également adopter une attitude plus pédagogique et décomposer le circuit en réluctances, selon la géométrie du problême et les attentes qu'on sur le résultat pour prendre en compte les fuites.

En ne conservant que l'hypothèse de la linéarité (ce qui revient à modéliser les pertes par saturation sans vraiment considérer le phénomène physique en tant que tel) toutes ces méthodes permettent d'établir la matrice inductance du transformateur, dont nous redonnons la définition :

\[
\left( \begin{matrix} \Phi_1 \\ \Phi_2 \end{matrix} \right) = 
\left( \begin{matrix}  
L_1 & M \\
M & L_2
\end{matrix} \right)
\left( \begin{matrix} I_1 \\ I_2 \end{matrix} \right)
\]

Dans le transformateur, les mutuelles sont égales \textcolor{red}{(pourquoi ?)}. Et donc, en notations complexes :

\[
\left( \begin{matrix} \underline{V}_1 \\ \underline{V}_2 \end{matrix} \right) = 
j\left( \begin{matrix}  
L_1 & M \\
M & L_2
\end{matrix} \right)\omega
\left( \begin{matrix} \underline{I}_1 \\ \underline{I}_2 \end{matrix} \right)
\]

La première chose qu'on est en droit d'exiger est un modèle électrique moins formel permettant l'identification des paramêtres $L_1$, $L_2$ et $M$ par le biais de mesures. Le circuit suivant rempli cette tâche :

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
  \draw	(0,0) to[L, l=$l_1$, i=$I_1$, *-] ++(4,0) node[shape=coordinate](A){} to[short, i=$I_0$] ++(0,-1) node[shape=coordinate](B){} to[L, l=$L_\mu$] ++(0,-2) -- ++(0,-1) node[shape=coordinate](C){} to[short, -*] ++(-4,0);
  \draw (C) to[short] ++(1,0) -- ++(2,2) -- ++(0.5,0) -- ++(2,-2) to[short, -*] ++(4,0);
  \draw (A) to[short, i<=$mI_2$] ++(1,0) -- ++(2,-2) node[draw, fill=white, shape=circle, minimum size = 1cm, label={[shift={(0.25,0.2)}]$m$}](O){} ++(0.5,0) node[draw, fill=white, shape=circle, minimum size = 1cm](P){} -- ++(2,2) to[L, l=$l_2$, i<=$I_2$, -*] ++(4,0);
\end{tikzpicture}
\end{center}
\shorthandon{:!}

L'élément central est la représentation d'un rapport de transformation $m$ réel conservant l'énergie. Les trois paramètres internes sont remplacés par les paramètres externes $l_1$, $l_2$ et $L_\mu$. Il nous faut calculer les relations entre ces paramètres.

Pour ce faire, il suffit d'exprimer les tensions aux bornes en fonction des courants, par le biais de lois des mailles appropriés :

\[
\left( \begin{matrix} \underline{V}_1 \\ \underline{V}_2 \end{matrix} \right) = 
\left( \begin{matrix} 
jl_1\omega\underline{I}_1 + jL_\mu\omega\underline{I}_0 \\
jmL_\mu\omega\underline{I}_0 + jl_2\omega\underline{I}_2 \end{matrix} \right)
\]

\[
\left( \begin{matrix} \underline{V}_1 \\ \underline{V}_2 \end{matrix} \right) = 
\left( \begin{matrix} 
jl_1\omega\underline{I}_1 + jL_\mu\omega(\underline{I}_1 + m\underline{I}_2) \\
jmL_\mu\omega(\underline{I}_1 + m\underline{I}_2) + jl_2\omega\underline{I}_2 \end{matrix} \right)
\]

\[\boxed{
\left( \begin{matrix} \underline{V}_1 \\ \underline{V}_2 \end{matrix} \right) = 
j\left( \begin{matrix}  
l_1+L_\mu & mL_\mu \\
mL_\mu & l_2+m^2L_\mu
\end{matrix} \right)\omega
\left( \begin{matrix} \underline{I}_1 \\ \underline{I}_2 \end{matrix} \right)
}\]

Nous avons fait apparaître la matrice inductance, permettant de passer d'un modèle à l'autre.

Concrètement, l'inducance $L_\mu = \frac{M}{m}$ est appellée \emph{inductance magnétisante}, et $l_1 = L_1-L_\mu$ ainsi que $l_2= L_2-m^2 L_\mu$ sont les \emph{inductances de fuite} des deux enroulements, modélisants - comme leur nom l'indique - les fuites de flux qui ne contribuent pas à la conversion d'énergie.

L'important pour concevoir un transformateur efficace est que $L_\mu \gg l_1$ et $L_\mu \gg l_2$. Si ce n'est pas le cas, le courant $I_0$ (qui est un courant fictif, représentatif de ce qui est perdu à cause des fuites) sera alors élevé.

Cette description purement magnétique est cependant trop éloignée de la réalité. Pour cette raison, nous allons mettre en place un modèle externe du transformateur plus bavard en ce qui concerne les subtilités de son comportement. Nous introduisons trois résistances, qui dissiperont les pertes joules et fer :

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
  \draw	(0,0) to[R, l=$R_1$, i=$I_1$, *-] ++(2,0) to[L, l=$l_1$] ++(2,0) node[shape=coordinate](A){} to[short, i=$I_0$] ++(0,-1) node[shape=coordinate](B){} -- ++(-0.5,0) to[R, l=$R_f$] ++(0,-2) - - ++(0.5,0) -- ++(0,-1) node[shape=coordinate](C){} to[short, -*] ++(-4,0);
  \draw (B) -- ++(0.5,0) to[L, l=$L_\mu$] ++(0,-2) -- ++(-0.5,0);
  \draw (C) to[short] ++(1,0) -- ++(2,2) -- ++(0.5,0) -- ++(2,-2) to[short, -*] ++(4,0);
  \draw (A) to[short, i<=$mI_2$] ++(1,0) -- ++(2,-2) node[draw, fill=white, shape=circle, minimum size = 1cm, label={[shift={(0.25,0.2)}]$m$}](O){} ++(0.5,0) node[draw, fill=white, shape=circle, minimum size = 1cm](P){} -- ++(2,2) to[R, l=$R_2$, i<=$I_2$] ++(2,0) to[L, l=$l_2$, -*] ++(2,0);
\end{tikzpicture}
\end{center}
\shorthandon{:!}

L'identification des paramètres de ce circuit à partir d'un jeu de mesures de puissances actives et réactives s'effectue par deux essais : le premier avec un secondaire à vide ($I_2 = 0$) et le second en court-circuitant celui-ci $\underline{V}_2 = 0$.

La première chose à faire est de \emph{ramener le modèle du transformateur au primaire} :

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
  \draw	(0,0) to[R, l=$R_1$, i=$I_1$, *-] ++(2,0) to[L, l=$l_1$] ++(2,0) node[shape=coordinate](A){} to[short, i=$I_0$] ++(0,-1) node[shape=coordinate](B){} -- ++(-0.5,0) to[R, l=$R_f$] ++(0,-2) - - ++(0.5,0) -- ++(0,-1) node[shape=coordinate](C){} to[short, -*] ++(-4,0);
  \draw (B) -- ++(0.5,0) to[L, l=$L_\mu$] ++(0,-2) -- ++(-0.5,0);
  \draw (C) to[short] ++(4,0) -- ++(2,2) -- ++(0.5,0) -- ++(2,-2) to[short, -*] ++(1,0);
  \draw (A) to[R, l=$R'_2$, i<=$mI_2$] ++(2,0) to[L, l=$l'_2$] ++(2,0) -- ++(2,-2) node[draw, fill=white, shape=circle, minimum size = 1cm, label={[shift={(0.25,0.2)}]$m$}](O){} ++(0.5,0) node[draw, fill=white, shape=circle, minimum size = 1cm](P){} -- ++(2,2) to[short, i<=$I_2$, -*] ++(1,0);
\end{tikzpicture}
\end{center}
\shorthandon{:!}

Par conservation des puissances actives et réactives, on a les relations de passage $R_2 = m^2R'_2$ et $l_2 = m^2l'_2$.

Considérons en premier lieu un secondaire à vide. La branche dans laquelle circule $I_2$ peut-être ignorée :

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
  \draw	(0,0) to[R, l=$R_1$, i=$I_1$, *-] ++(2,0) to[L, l=$l_1$] ++(2,0) node[shape=coordinate](A){} to[short, i=$I_0$] ++(0,-1) node[shape=coordinate](B){} -- ++(-0.5,0) to[R, l=$R_f$] ++(0,-2) - - ++(0.5,0) -- ++(0,-1) node[shape=coordinate](C){} to[short, -*] ++(-4,0);
  \draw (B) -- ++(0.5,0) to[L, l=$L_\mu$] ++(0,-2) -- ++(-0.5,0);
\end{tikzpicture}
\end{center}
\shorthandon{:!}

Une mesure à l'ohmètre - en courant continu - permettra de déterminer $R_1$ puisque $R_f$ se trouve court-circuitée à l'équilibre

Une fois $R_1$ connue, un essai en régime alternatif permettra de calculer les autres paramètres en relevant simplement les valeurs efficaces de la tension $|\underline{V}_2|$, le courant $|\underline{I}_1|$ ainsi que les puissances actives et réactives :

\[
\left\{ \begin{matrix}
P = R_1|\underline{I}_1|^2+\frac{|\underline{V}_2|^2}{m^2R_f} \\
Q = l_1\omega|\underline{I}_1|^2+\frac{|\underline{V}_2|^2}{m^2L_\mu\omega}
\end{matrix}\right.
\]

$m$ nécessite d'être connu au préalable, et même si c'est inexact on pourra pour ce faire partir de l'hypothèse que $|\underline{V}_2|=m|\underline{V}_1|$ (ce qui revient à négliger $R_1$ et $l_1$ dans la détermination de $m$), auquel cas on aura :

\[
\left\{ \begin{matrix}
P = R_1|\underline{I}_1|^2+\frac{|\underline{V}_1|^2}{R_f} \\
Q = l_1\omega|\underline{I}_1|^2+\frac{|\underline{V}_1|^2}{L_\mu\omega}
\end{matrix}\right.
\]

Si nous n'avons pas de moyen immédiat pour mesurer la puissance réactive, on pourra utiliser la relation $Q = \sqrt{S^2-P^2}$ avec $S = |\underline{V}_1||\underline{I}_1|$.

L'essai en court-circuit va nous permettre de trouver les grandeurs $R'_2$ et $l'_2$ caractéristiques du secondaire (mais ramenées au primaire). Selon l'exigence qu'on a sur la précision du résultat on pourra négliger $I_0$ ou pas. Si l'on décide de le négliger :

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
  \draw	(0,0) to[R, l=$R_1$, i=$I_1$, *-] ++(2,0) to[L, l=$l_1$] ++(2,0) node[shape=coordinate](A){} to[open] ++(0,-4) node[shape=coordinate](C){} to[short, -*] ++(-4,0);
  \draw (C) -- ++(3,0);
  \draw (A) to[L, l=$l'_2$, i<=$mI_2$] ++(3,0) to[R, l=$R'_2$] ++(0,-4);
\end{tikzpicture}
\end{center}
\shorthandon{:!}

\[
\left\{ \begin{matrix}
P = (R_1+R'_2)|\underline{I}_1|^2 \\
Q = (l_1+l'_2)\omega|\underline{I}_1|^2
\end{matrix}\right.
\]

Cependant, étant données les puissances de calcul qu'on a à notre disposition aujourd'hui ce n'est pas un grand effort que de le considérer :

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
  \draw	(0,0) to[R, l=$R_1$, i=$I_1$, *-] ++(2,0) to[L, l=$l_1$] ++(2,0) node[shape=coordinate](A){} to[short, i=$I_0$] ++(0,-1) node[shape=coordinate](B){} -- ++(-0.5,0) to[R, l=$R_f$] ++(0,-2) - - ++(0.5,0) -- ++(0,-1) node[shape=coordinate](C){} to[short, -*] ++(-4,0);
  \draw (B) -- ++(0.5,0) to[L, l=$L_\mu$] ++(0,-2) -- ++(-0.5,0);
  \draw (C) -- ++(3,0);
  \draw (A) to[L, l=$l'_2$, i<=$mI_2$] ++(3,0) to[R, l=$R'_2$] ++(0,-4);
\end{tikzpicture}
\end{center}
\shorthandon{:!}

Il n'y a plus d'expression simple des puissances, ce qui complique légèrement l'établissement d'une expression analytique. Vous conviendrez que :

\[
R'_2+jl'_2\omega = \frac{\underline{V_1}-R_1\underline{I_1}}{\underline{I_1}-\underline{I_0}}
\]

Dont découle :

\[
R'_2+jl'_2\omega = \frac{\underline{V_1}-R_1\underline{I_1}}{\underline{I_1}-\left(\frac{1}{R_f}+\frac{1}{jL_\mu\omega}\right)(\underline{V_1}-R_1\underline{I_1})}
\]

Un logiciel de calcul scientifique conviendra pour séparer la partie réelle de la partie imaginaire. Cela demande néanmoins de connaitre le déphasage entre $V_1$ et $I_1$, ce qui se fait en considérant la convention qui veut que $\underline{V_1}$ fixe l'origine des temps et soit donc pris réel, et en utilisant la définition de la puissance apparente :

\[
\underline{S} = |\underline{V_1}|\underline{I_1}^*
\]

\[
\underline{I_1} = \frac{\underline{S}^*}{|\underline{V_1}|} = \frac{P-jQ}{|\underline{V_1}|}
\]

Ce qui achève notre discussion sur l'établissement des paramètres du transformateur monophasé.

Ce modèle est suffisament fin pour prendre en compte les pertes fer, mais il reste néanmoins intrinsèquement linéaire. Pour prendre en compte les effets de saturation magnétiques, on peut attribuer une valeur différente de chaque paramètre selon le point de fonctionnement.

\textcolor{red}{(cycle d'hystéresis)}

\subsection{Transformateur triphasé}

\subsubsection{Indice horaire}

Le paramètre le plus important du transformateur est son indice horaire, réel dans le cas monophasé et complexe en triphasé.

Par exemple, un couplage étoile-triangle direct aboutit à un indice horaire égal à $1$, du fait de :

\[
\underline{m} = |\underline{m}|e^{j\frac{\pi}{6}}
\]

Car en effet, l'indice horaire est défini comme l'argument de $\underline{m}$ divisé par $\frac{\pi}{6}$.

On désigne une transformateur triphasé par ses couplages (primaire et secondaire) ainsi que son indice horaire. On notera qu'il n'existe pas de couplage universel qui permettrait la réalisation de tous les indices horaires.

Par conservation de la puissance apparente, en régime équilibré :

\[
\underline{S} = 3 \underline{V_1}\underline{I_1}^* = -3 \underline{V_2}\underline{I_2}^*
\]

\[
\underline{I_2} = -\underline{m}^*\underline{I_1}
\]

\subsection{Machine à courant continu}

\subsubsection{Modèle interne de la machine à courant continu}

La machine à courant continu est constitué d'un stator muni d'enroulements, et d'un rotor également muni d'enroulements alimenté par un système de balais.

\begin{center}
\includegraphics[width=5\textwidth/6]{gmsh/mcc.pdf}
\end{center}

\textcolor{red}{(origine du couple, pression magnétique)}

\subsubsection{Modèle externe de la machine à courant continu}

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
  \draw (0,0) to[open, v=$U_e$] ++(0,-3.25);
  \draw (0,0) to[R, l_=$R_e$] ++(2,0)
  to[L, i=$Ie$, l=$L_e$] ++(0,-3.25)
  -- ++(-2,0);
  \draw	(5.5,0) to[R, l=$R_i$] ++(-2,0)
  to[L, i=$I_i$, l=$L_i$, mirror] ++(0,-2)
  to[european voltage source, v<=$E$] ++(0,-1)
  -- ++(0,-0.25)
  -- ++(2,0);
  \draw (5.5,0) to[open, v^=$U_i$] ++(0,-3.25);
\end{tikzpicture}
\end{center}
\shorthandon{:!}

Son équation de fonctionnement est donc :

\[
\left( \begin{matrix}
U_e \\
U_i
\end{matrix} \right)
=
\left( \begin{matrix}
R_e I_e \\
R_i I_i
\end{matrix} \right)
+
\left( \begin{matrix}
L_e & M \\
M & L_i
\end{matrix} \right)
\frac{d}{dt}
\left( \begin{matrix}
I_e \\
I_i
\end{matrix} \right)
+
\left( \begin{matrix}
0 \\
E
\end{matrix} \right)
\]

Nous allons chercher la valeur de $E$. Le principe des travaux virtuels s'écrit :

\[ 
\vec{Q} = \nabla CoW
\]

Qui devient pour un système linéaire :

\[ 
Q_k = -\frac{1}{2} I_i \partial_k L^{i}_{\; j} I^j
\]

Ici, le seul degré de liberté est l'angle $\theta$ que fait le rotor avec le stator ; et la force généralisée correspondante est un couple selon $z$ :

\[ 
\vec{\Gamma} = -\frac{1}{2} I_i \frac{\partial L^{i}_{\; j}}{\partial \theta} I^j \vec{e_z}
\]

Pour un \emph{rotor lisse}, les inductances propres ne varient pas avec $\theta$ \textcolor{red}{(Quid des rotors saillants ?)} :

\[ 
\vec{\Gamma} = -\frac{1}{2} I_i \frac{\partial}{\partial \theta}\left(\begin{matrix}
0 & M \\
M & 0
\end{matrix}\right) I^j \vec{e_z}
\]

Qui se calcule en :

\[ 
\vec{\Gamma} = - \frac{\partial M}{\partial \theta} I_e I_i \vec{e_z}
\]

L'hypothèse qui va être faite maintenant est propre à la machine à courant continu, et consiste à considérer les enroulements rotoriques comme suffisament rapprochés les uns des autres pour assimiler la dérivée de la mutuelle à sa valeur moyenne :

\[
\vec{\Gamma} = -\braket{\frac{\partial M}{\partial \theta}} I_e I_i \vec{e_z}
\]

Et par conservation de l'énergie :

\[
E I_i = \vec{\Omega} \cdot \vec{\Gamma} 
\]

\[
E = -\braket{\frac{\partial M}{\partial \theta}} I_e (\vec{\Omega} \cdot \vec{e_z})
\]

Le seul paramêtre intrinsèque du modèle externe idéalisé (au sens de l'hypothèse précédente) de la machine à courant continu est donc la constante $\braket{\frac{\partial M}{\partial \theta}}$, et on se mettra en quête d'elle quand on consultera une documentation, effectuera un essai sur machine réelle ou qu'on mettra en place une modélisation interne par éléments finis.

Notez que dans ce modèle, $I_e$ débitant sur une charge passive la puissance correspondante ne contribue pas au travail mécanique.

Dernière chose et non des moindres, regardons ce qu'il se passe si l'on fait chuter brutalement le courant d'excitation d'une machine en fonctionnement, c'est-à-dire en prenant $\Omega$ non-nul mais $I_e$ pas tout-à-fait nul mais très faible. En premier lieu, on en déduit que $E$ va diminuer également, ce qui implique que la tension aux bornes de l'induit débite presque intégralement sur l'impédance statorique, ce qui va rapidement faire exploser le courant d'induit $I_i$.

Or, le couple $\Gamma$ étant proportionnel au produit $I_eI_i$, selon les valeurs de résistance et de tension l'augmentation de $I_i$ peut largement contrebalancer la diminution de $I_e$, et faire s'emballer la machine.

Ce comportement n'est pas à prendre à la légère, l'emballement des machines à courant continu est la cause de nombreux accidents.

A noter qu'à cause des effets inductifs, l'emballement peut avoir lieu même en cas de coupure complète de $I_e$.

\subsubsection{Identification des paramètres}

On s'intéresse au problême de la paramétrisation du modèle pour un essai sur machine réelle.

La mesure de $R_e$ s'effectue sans souci à l'ohm-mètre à l'arrêt.

La détermination de $R_i$ s'effectue par des mesures conjointes de la tension et courant efficaces en régime permanent à vitesse constante (parce-que cette résistance varie avec l'angle à cause du système de balais on ne peut pas se contenter d'un ohm-mêtre à l'arrêt, on obtiendra ainsi sa valeur moyenne).

La régression linéaire donnera à la fois $R_i$ et $E$, le second permettant - connaissant $I_e$ et $\Omega$ - d'en déduire $\braket{\frac{\partial M}{\partial \theta}}$

Une autre façon pour trouver $\frac{\partial M}{\partial \theta}$ est de mesurer aussi le couple par le biais d'un capteur approprié, et de faire une régression sur $\Gamma(I_i)$

La mesure des inductances s'effectue en régime dynamique et donc en regardant les réponses transitoires (classiquement celle à un échelon), comme pour n'importe quel premier ordre. Coté induit, il faudra simplement faire attention à conserver $E$ constante et donc à se placer sur des intervalles d'étude petits devant la constante de temps mécanique du système.

\subsubsection{Comportements non-linéaires}

Néanmoins, ce modèle linéaire a ses limites et on peut-être amené à considérer que les grandeurs peuvent dépendre les unes des autres, ce qui revient à interpoler les mesures des caractéristiques par des courbes plus complexes que des droite.

En premier lieu, la saturation magnétique a évidemment des implications dans tout type de machine. Un courant trop élevé donnera lieu à un $\vec{B}$ n'augmentant plus $\vec{H}$, et donc une limitation en tension et vitesse associée à une chute du rendement (du fait d'un différentiel trop important entre la tension d'induit et la force électro-motrice).

Cette même saturation mérite d'être prise en considération pour choisir $I_e$ (qui est la première chose qu'un motoriste doit apprendre à faire), où la caractéristique $E(\Omega,I_e)$ (considérée auparavant linéaire) devient :

\textcolor{red}{(figure, page 19 cours mcc)}

En régime linéaire, le réglage de $I_e$ est aisé connaissant $\braket{\frac{\partial M}{\partial \theta}}$ ainsi qu'un couple $(E,\Omega)$ ou $(I_i,\Gamma)$ désiré ; en particulier pour un fonctionnement générateur débitant sur une charge $R_{ch}$ connue on choisira :

\[
I_e = \frac{V-R_sI_s}{\braket{\frac{\partial M}{\partial \theta}}\Omega}
\]

\[
I_e = \frac{V\left(1-\frac{R_s}{R_{ch}}\right)}{\braket{\frac{\partial M}{\partial \theta}}\Omega}
\]

En régime non-linéaire, on peut-être amené à devoir déterminer un point de fonctionnement par des méthodes numériques. Il existe également un optimum du rendement, qu'on peut-être amené à prendre en compte au moment du dimensionnement et dans la commande selon l'utilisation qu'on veut faire de la machine.

Enfin, l'\emph{induction rémanante} \textcolor{red}{(pas parlé avant)} intervient dans de nombreuses situations, notamment pour le démarrage des configurations à excitation liée à l'induit. Elle peut engendrer des phénomènes analogues à la coupure d'une excitation, mais cette-fois-ci simplement sur des transitoires dans certaines situations ; et c'est pourquoi la machine à courant continu doit-être manipulée avec précaution.

\subsubsection{Paires de pôles multiples}

L'introduction de paires de pôles supplémentaires à l'inducteur bipolaire précédemment considéré se traduit très simplement dans la machine à courant continu par l'introduction d'une pulsation électrique $\omega = p\Omega$. Les équations de la machine deviennent :

\[\boxed{
E = -p\braket{\frac{\partial M}{\partial \theta}} I_e (\vec{\Omega} \cdot \vec{e_z})
}\]

\[\boxed{ 
\vec{\Gamma} = -p\braket{\frac{\partial M}{\partial \theta}} I_e I_i \vec{e_z}
}\]

\textcolor{red}{(figure, éléments finis)}

\textcolor{red}{(discussion sur la puissance fluctuente peut-être ?)}

\subsection{Machine synchrone}

\subsubsection{Modèle interne de la machine synchrone}

Soit un stator consistué d'une armature en fer munie d'encoches dans lesquelles sont logées des conducteurs.

Pour l'exemple, nous travaillerons sur la machine suivante :

\textcolor{red}{(figure)}

L'induit de la machine est composée de trois bornes correspondant à trois courants pouvant parcourir les connecteurs internes. Ceux-ci sont raccordés en étoile ou triangle, comme n'importe quelle charge triphasée.

Pour faire l'étude analytique de ce problême (ce qui permet d'appréhender de la meilleure façon qui soit les concepts qui permettent à la machine synchrone d'exister en temps que machine électrique), nous allons devoir émettre plusieurs hypothèses :

\begin{itemize}
\item Le fer sera supposé de perméabilité magnétique très grande devant $\mu_0$.
\item L'entrefer sera supposé très petit devant le rayon du Rotor, ce justifie que le champ dans l'entrefer est radial \textcolor{red}{(pourquoi ? par minimisation de l'énergie ?)}.
\end{itemize}

Par finitude de l'énergie dont l'expression locale est $\frac{||\vec{B}||^2}{2\mu_0}$, $\vec{B}$, découle de la première hypothèse que dans le fer $\vec{H} = \frac{\vec{B}}{\mu} = 0$.

Commençons par simplifier le problême en ne s'intéressant qu'à une seule spire :

\textcolor{red}{(figure)}

Nous allons considérer les contours $C_1$ et $C_2$ sur lesquels nous allons intégrer Maxwell-Ampère :

\[
\nabla \times \vec{H} = \vec{j}
\] 

\[
\oint_{C_1}\vec{H} \cdot \vec{dl} = 0
\] 

En généralisant ça à tout contour de ce type n'entourant aucun conducteur et en se plaçant en coordonnées polaires, on constate que :

\[
\frac{\partial \vec{H}}{\partial \theta} = 0, \forall \theta \in ]0;\pi[\cup]\pi;2\pi[
\] 

On peut donc se contenter de chercher les expressions des champs $\vec{H_1}$ et $\vec{H_2}$. Pour ce faire, intégrons maintenant sur $C_2$ :

\[
\oint_{C_2}\vec{H} \cdot \vec{dl} = I
\] 

\[
\vec{H_1}e - \vec{H_2}e = I
\] 

\[
\vec{H_1} - \vec{H_2} = \frac{I}{e}
\]

Et puisqu'on se trouve dans l'entrefer :

\[
\vec{B_1} - \vec{B_2} = \frac{\mu_0 I}{e}
\]

Enfin, appliquons Maxwell-Thomson sur le contour $C_3$ dans l'entrefer :

\[
\nabla \cdot \vec{B} = 0
\]

\[
\oiint_{C_3} \vec{B} \cdot \vec{dS} = 0
\]

Le problême étant plan on ne fait qu'intégrer sur le contour.

\[
\vec{B_1}\pi R_{Rotor} + \vec{B_2}\pi R_{Rotor} = 0
\]

\[
\vec{B_1} + \vec{B_2} = 0
\]

On a donc le système suivant :

\[
\left( \begin{matrix}
1 & -1 \\
1 & 1
\end{matrix}\right)
\left( \begin{matrix}
B_1 \\
B_2
\end{matrix}\right)
=
\left( \begin{matrix}
\frac{\mu_0 I}{e} \\
0
\end{matrix}\right)
\]

Qu'on inverse en :

\[
\left( \begin{matrix}
B_1 \\
B_2
\end{matrix}\right)
=
\left( \begin{matrix}
\frac{\mu_0 I}{2e} \\
-\frac{\mu_0 I}{2e}
\end{matrix}\right)
\]

On peut donc traçer le champ dans l'entrefer en fonction de $\theta$ :

\textcolor{red}{(figure)}

Nous allons avoir besoin dans la suite de notre discussion de travailler avec les décompositions en séries de Fourier des grandeurs qu'on considère. Nous avons vu que les coefficients de Fourier sont donnés par :

\[
<f|e^{jk_n x}> = \frac{1}{\lambda} \int_{-\frac{\lambda}{2}}^{\frac{\lambda}{2}} f(x) e^{-j k_n x } dx
\]

Ou dans notre contexte, où l'abscisse curviligne est un angle :

\[
<B|e^{jn \theta}> = \frac{1}{2\pi} \int_{-\pi}^{\pi} B(\theta) e^{-j n \theta } d\theta
\]

Pour une seule spire, on a donc :

\[
<B|e^{jn \theta}> = \frac{1}{2\pi} \left( \int_0^{\pi} B_1 e^{-j n \theta } d\theta + \int_{-\pi}^0 B_2 e^{-j n \theta } d\theta \right)
\]

\[
<B|e^{jn \theta}> = \frac{\mu_0 I}{4e\pi} \left( \int_0^{\pi} e^{-j n \theta } d\theta - \int_{-\pi}^0 e^{-j n \theta } d\theta \right)
\]

\[
<B|e^{jn \theta}> = \frac{\mu_0 I}{-4je\pi n} \left( e^{-j n \pi } - e^{-j n 0} - e^{-j n 0} + e^{j n \pi } \right)
\]

Qu'on se doit de distinguer en deux cas :

\[
\left\{ \begin{matrix}
<B|e^{jn \theta}> = 0 , \forall n \; pair \\
<B|e^{jn \theta}> = \frac{\mu_0 I}{je\pi n} , \forall n \; impair
\end{matrix} \right.
\]

En se permettant d'omettre la notation des distributions, on en conclu que $B(\theta)$ se décompose en série de Fourier comme suit :

\[
B(\theta) = \sum_{n\in \mathbb{N}} \frac{\mu_0 I}{je\pi (2n+1)}e^{j(2n+1) \theta}
\]

Considérons maintenant la machine complète. La linéarité des équations de Maxwell permet d'additionner simplement les contributions de chacune des spires.

\textcolor{red}{(faux à partir de là)}

Pour ce faire, nous allons numéroter les champs $B_i$ par le numéro de la spire ($i$ de $0$ à $m-1$) qu'on lui attribue comme source, $m$ étant le nombre de spires que la machine comporte au stator :

\[
B_i(\theta) = \sum_{n\in \mathbb{N}} \frac{\mu_0 I_i}{je\pi (2n+1)}e^{j(2n+1) (\theta - \pi\frac{i}{m})}
\]

Les spires étant uniformément réparties.

Plaçons-nous en régime triphasé équilibré \textcolor{red}{(cad ?)} et permanent où $I_i = I_0 e^{j\frac{2\pi}{3}i}$ (le déphasage est ignoré car on ne s'intéresse pas aux tensions) :

\[
B_i(\theta) = \sum_{n\in \mathbb{N}} \frac{\mu_0 I_0}{je\pi (2n+1)}e^{j\frac{2\pi}{3}i}e^{-j(2n+1)\pi\frac{i}{m}}e^{j(2n+1)\theta}
\]

\[
B_i(\theta) = \sum_{n\in \mathbb{N}} \frac{\mu_0 I_0}{je\pi (2n+1)}e^{j(\frac{2}{3}-\frac{2n+1}{m})\pi i}e^{j(2n+1)\theta}
\]

Et le champ résultant s'écrit bien sûr par linéarité des équations de Maxwell dans le vide :

\[
B(\theta) = \sum_{i\in [0;m-1]} B_i(\theta)
\]

\[
B(\theta) = \sum_{i\in [0;m-1]} \sum_{n\in \mathbb{N}} \frac{\mu_0 I_0}{je\pi (2n+1)}e^{j(\frac{2}{3}-\frac{2n+1}{m})\pi i}e^{j(2n+1)\theta}
\]

Un coefficient de Fourier particulier :

\[
<B|e^{j(2n+1) \theta}> =  \frac{\mu_0 I_0}{je\pi (2n+1)} \sum_{i\in [0;m-1]} e^{j(\frac{2}{3}-\frac{2n+1}{m})\pi i}
\]

Plaçons-nous finalement dans le cas où $3|m$ (autrement écrit $\exists p : m = 3p$, et on appellera $p$ le \emph{nombre de paires de pôles}) :

\[
<B|e^{j(2n+1) \theta}> = \frac{\mu_0 I_0}{je\pi (2n+1)} \sum_{i\in [0;3p-1]} e^{j(\frac{2}{3}-\frac{2n+1}{3p})\pi i}
\]

\[
<B|e^{j(2n+1) \theta}> = \frac{\mu_0 I_0}{je\pi (2n+1)} \sum_{i\in [0;3p-1]} e^{j\frac{2(p-n)-1}{3p}\pi i}
\]

Intéressons-nous à l'harmonique de rang $3$, c'est-à-dire quand $n=1$ :

\[
<B|e^{j3\theta}> = \frac{\mu_0 I_0}{3je\pi} \sum_{i\in [0;3p-1]} e^{j\frac{2(p-1)-1}{3p}\pi i}
\]

\[
<B|e^{j3\theta}> = \frac{\mu_0 I_0}{3je\pi} \sum_{i\in [0;3p-1]} e^{j(\frac{2}{3} - \frac{1}{p}) \pi i}
\]

Effectuons maintenant un découpage particulier des sommes, qui permet de considérer des triplets de spires formant chacun une paire de pôle :

\textcolor{red}{(Calcul erroné car l'harmonique de rang 3 n'est pas nulle)}

\subsection{Modèle externe de la machine synchrone}

Une machine synchrone à rotor bobiné est constituée de quatre enroulements - trois statoriques et un rotorique - et nous lui appliquerons donc la méthode analytique sur les dispositifs magnétiques vue précédemment. Sa matrice inductance s'écrit :

\[
L_{\mu\nu} = \left(\begin{matrix}
L_s & M_s & M_s & M_{ar} \\
M_s & L_s & M_s & M_{br} \\
M_s & M_s & L_s & M_{cr} \\
M_{ar} & M_{br} & M_{cr} & L_r
\end{matrix}\right)
\]

Pour un câblage en étoile ou triangle équilibré, on a $i_1+i_2+i_3=0$, ce qui induit :

\[
\forall \mu \in \frac{\mathbb{Z}}{3\mathbb{Z}}, \; L_s i_\mu + M_s i_{\mu+1} + M_s i_{\mu+2} = L_s i_\mu - M_s i_\mu = (L_s - M_s )i_\mu
\]

Et on appelle $\mathcal{L}_s = L_s - M_s$ l'\emph{inductance cyclique}. Toujours sous l'hypothèse précédente, la matrice inductance devient :

\[
L_{\mu\nu} = \left(\begin{matrix}
\mathcal{L}_s & 0 & 0 & M_{ar} \\
0 & \mathcal{L}_s & 0 & M_{br} \\
0 & 0 & \mathcal{L}_s & M_{cr} \\
M_{ar} & M_{br} & M_{cr} & L_r
\end{matrix}\right)
\]

On écrit le principe des travaux virtuels :

\[ 
\vec{\Gamma} = -\frac{1}{2} I_i \frac{\partial L^{i}_{\; j}}{\partial \theta} I^j \vec{e_z}
\]

L'inductance cyclique ne varie pas avec $\theta$ \textcolor{red}{(uniquement pour un rotor lisse)} :

\[ 
\vec{\Gamma} = -\frac{1}{2} I_i \frac{\partial}{\partial \theta}\left(\begin{matrix}
0 & 0 & 0 & M_{ar} \\
0 & 0 & 0 & M_{br} \\
0 & 0 & 0 & M_{cr} \\
M_{ar} & M_{br} & M_{cr} & 0
\end{matrix}\right) I^j \vec{e_z}
\]

Qui se calcule en :

\[ 
\vec{\Gamma} = -I_r \left( \sum_{i \in (a,b,c)}\frac{\partial M_{ir}}{\partial \theta} I^i \right) \vec{e_z}
\]

Notre modèle consiste à imposer que les inductances mutuelles entre le stator et le rotor ont pour expressions :

\[ 
M_{ir} = M\cos\left(\theta-\frac{2\pi}{3}(i-1)+\theta_0\right)
\]

$\theta_0$ étant l'angle électrique que fait le rotor avec l'axe de la phase $a$ au passage du maximum de $V_a$ (origine des phases conventionnel). Le problême se prête bien au formalisme complexe :

\[ 
\underline{M_{ir}} = \frac{M}{\sqrt{2}} e^{j\left(-\frac{2\pi}{3}(i-1)+\theta_0\right)} = \frac{M}{\sqrt{2}}e^{j\theta_0} a^{1-i}
\]

On peut alors tout recouper :

\[ 
\vec{\Gamma} = -I_r \left( \sum_{i \in (a,b,c)}\frac{\partial M_{ir}}{\partial \theta} I^i \right) \vec{e_z}
\]

Au synchronisme :

\[ 
\vec{\Gamma} = -I_r \Re\left( \sum_{i \in (a,b,c)}j\sqrt{2}\underline{M_{ir}} e^{j\theta} \sqrt{2}\underline{I_i}^* e^{-j\theta} \right) \vec{e_z}
\]

$I_r$ est un courant continu. \textcolor{red}{(justifier la conjugaison, dans un espace hermitien l'inversion de $I$ revient à sa conjugaison)}

\[ 
\vec{\Gamma} = -I_r \Re\left( \sum_{i \in (a,b,c)}j2\underline{M_{ir}} \underline{I_i}^* \right) \vec{e_z}
\]

Avec un déphasage tension-courant noté $\varphi$ :

\[ 
\vec{\Gamma} = -\Re \left( I_r \sum_{i \in (a,b,c)}j2\frac{M}{\sqrt{2}} e^{j\left(-\frac{2\pi}{3}(i-1)+\theta_0\right)} |\underline{I_s}| e^{j\left(\frac{2\pi}{3}(i-1)-\varphi\right)} \right) \vec{e_z}
\]

\[ 
\vec{\Gamma} = - \sqrt{2} M |\underline{I_s}| I_r \left( \sum_{i \in (a,b,c)}e^{j\left(\frac{\pi}{2}+\theta_0-\varphi\right)} \right) \vec{e_z}
\]

Plus de dépendance en $i$ :

\[ 
\vec{\Gamma} = -3\sqrt{2}M |\underline{I_s}| I_r e^{j\left(\frac{\pi}{2}+\theta_0-\varphi \right)} \vec{e_z}
\]

Dans le domaine réel :

\[
\vec{\Gamma} = -3\sqrt{2}M |\underline{I_s}| I_r \sin\left(\varphi-\theta_0 \right) \vec{e_z}
\]

\textcolor{red}{(faux, le résultat attendu est :)}

\[
\vec{\Gamma} = -\frac{3}{\sqrt{2}}M |\underline{I_s}| I_r \sin\left(\theta_0+\varphi \right) \vec{e_z}
\]

Attention cependant à une chose : le courant $I_s$ introduit ici est l'amplitude des courants de phase, mais ce qu'on mesure est généralement leur valeur efficace qu'il faudra donc multiplier par $\sqrt{2}$. Pour un couplage triangle, une division par $\sqrt{3}$ interviendra également pour accéder aux courants de branches depuis les courants extérieurs.

\textcolor{red}{(faux à partir de là)}

Par conservation de la puissance active (on quitte le domaine complexe inapproprié aux considérations énergétiques) :

\[
\vec{\Gamma}\cdot \vec{\Omega} = 3EI
\]

\[
-3M I_s I_r \sin(\delta) (\Omega\cdot \vec{e_z}) = 3E_0\cos(\theta)I_s\cos(\theta+\varphi)
\]

\[
-M I_r \sin(\delta) (\Omega\cdot \vec{e_z}) = \frac{1}{2}E_0\left(\cos(\varphi)+\cos(2\theta+\varphi)\right)
\]

\[
E_0 = -\frac{2M \sin(\delta)}{\cos(\varphi)+\cos(2\theta+\varphi)}I_r (\Omega\cdot \vec{e_z})
\]

On voit que le 

\subsection{Machine asynchrone}

\subsubsection{Modèle interne de la machine asynchrone}

La machine asynchrone repose sur l'idée selon laquelle les courants rotoriques peuvent-êtres induits par des courants statoriques alternatifs, exactement à la manière d'un transformateur triphasé.

Cette machine est connue pour avoir un rendement un peu moins bon que la machine synchrone, mais cumule par ailleurs au moins trois avantages :

\begin{itemize}
\item Les courants d'excitation étant induits, Les balais ne sont pas indispensables pour un rotor bobiné.
\item Alors que la machine synchrone ne peut fonctionner qu'au synchronisme avec les grandeurs électriques, la vitesse $\Omega_s = \frac{\omega}{p}$ est pour la machine asynchrone précisément la seule à laquelle celle-ci ne délivre aucun couple. On peut ainsi faire varier la vitesse en connectant directement la machine au réseau ; bien que celui-ci soit à fréquence fixe.
\item Il existe une configuration particulière - dite en \emph{rotor à cage} - particulièrement simple à réaliser d'un point de vue technique.
\end{itemize}

Sa modélisation commence en premier lieu par une étude magnétodynamique d'une encoche rotorique :

\definecolor{cqcqcq}{rgb}{0.7529411764705882,0.7529411764705882,0.7529411764705882}\definecolor{ffxfqq}{rgb}{1,0.4980392156862745,0}
\begin{center}
\begin{tikzpicture}[scale=3]\clip(-1.7850752290856386,-1.3353557436942816) rectangle (1.8397269414313413,2.2769038587586192);\fill[line width=1pt,color=ffxfqq,fill=ffxfqq,fill opacity=1] (-0.5,0) -- (0.5,0) -- (0.5,1) -- (-0.5,1) -- cycle;\fill[line width=1pt,color=cqcqcq,fill=cqcqcq,fill opacity=1] (-1.5,-1) -- (1.5,-1) -- (1.5,1.5) -- (0.5,1.5) -- (0.5,0) -- (-0.5,0) -- (-0.5,1.5) -- (-1.5,1.5) -- cycle;\fill[line width=1pt,color=cqcqcq,fill=cqcqcq,fill opacity=1] (-1.5,1.6) -- (1.5,1.6) -- (1.5,2.1) -- (-1.5,2.1) -- cycle;\draw [line width=1pt,fill=black,fill opacity=1] (0.26636186926212374,0.2436184197828688) circle (0.01cm);\draw [line width=1pt,color=ffxfqq] (-0.5,0)-- (0.5,0);\draw [line width=1pt,color=ffxfqq] (0.5,0)-- (0.5,1);\draw [line width=1pt,color=ffxfqq] (0.5,1)-- (-0.5,1);\draw [line width=1pt,color=ffxfqq] (-0.5,1)-- (-0.5,0);\draw [line width=1pt,color=cqcqcq] (-1.5,-1)-- (1.5,-1);\draw [line width=1pt,color=cqcqcq] (1.5,-1)-- (1.5,1.5);\draw [line width=1pt,color=cqcqcq] (1.5,1.5)-- (0.5,1.5);\draw [line width=1pt,color=cqcqcq] (0.5,1.5)-- (0.5,0);\draw [line width=1pt,color=cqcqcq] (0.5,0)-- (-0.5,0);\draw [line width=1pt,color=cqcqcq] (-0.5,0)-- (-0.5,1.5);\draw [line width=1pt,color=cqcqcq] (-0.5,1.5)-- (-1.5,1.5);\draw [line width=2pt,color=cqcqcq] (-1.5,1.5)-- (-1.5,-1);\draw [line width=2pt,color=cqcqcq] (-1.5,1.6)-- (1.5,1.6);\draw [line width=2pt,color=cqcqcq] (1.5,1.6)-- (1.5,2.1);\draw [line width=2pt,color=cqcqcq] (1.5,2.1)-- (-1.5,2.1);\draw [line width=2pt,color=cqcqcq] (-1.5,2.1)-- (-1.5,1.6);\draw [line width=1pt,dashed] (-0.75,-0.25)-- (0.75,-0.25);\draw [line width=1pt,dashed] (0.75,-0.25)-- (0.75,0.6666666666666666);\draw [line width=1pt,dashed] (0.75,0.6666666666666666)-- (-0.75,0.6666666666666666);\draw [line width=1pt,dashed] (-0.75,0.6666666666666666)-- (-0.75,-0.25);\draw [line width=1pt,dashed] (-1,-0.5)-- (1,-0.5);\draw [line width=1pt,dashed] (1,-0.5)-- (1,1.25);\draw [line width=1pt,dashed] (1,1.25)-- (-1,1.25);\draw [line width=1pt,dashed] (-1,1.25)-- (-1,-0.5);\draw [line width=1pt,dashed] (-1.25,-0.75)-- (1.25,-0.75);\draw [line width=1pt,dashed] (1.25,-0.75)-- (1.25,1.85);\draw [line width=1pt,dashed] (1.25,1.85)-- (-1.25,1.85);\draw [line width=1pt,dashed] (-1.25,1.85)-- (-1.25,-0.75);\draw [->,line width=1pt] (1,0.22573728055670572) -- (1,0.7190551036541348);\draw [line width=1pt] (0.26636186926212374,0.2436184197828688) circle (0.1cm);\draw (0.046139708268960265,0.35196020543605997) node[anchor=north west] {$\vec{j}$};\draw (-0.14199881269212866,0.834849075902854) node[anchor=north west] {$\mathcal{C}_1$};\draw (0.10885254858932324,1.405535922818156) node[anchor=north west] {$\mathcal{C}_2$};\draw (0.3659751939028114,2.0263930419897483) node[anchor=north west] {$\mathcal{C}_3$};\draw (-0.8694677604083392,1.0229875968639427) node[anchor=north west] {$\mu, \sigma_f$};\draw (-0.4555630142939435,0.966546040575616) node[anchor=north west] {$\sigma_c$};\draw (1.0056461651705138,0.6212848743010397) node[anchor=north west] {$\vec{B}$};\draw (-0.054200836243620494,1.2487538220172488) node[anchor=north west] {$\vec{H}$};\draw [->,line width=1pt] (0.13393768471746847,1.25) -- (-0.20471165301249172,1.25);
\end{tikzpicture}
\end{center}

Plaçons-nous dans le cadre magnétostatique ($\vec{j}$ est continu) avec $\mu \sim \infty$, et appliquons Maxwell-Ampère successivement à chacun des contours :

\[
\oint  \vec{H} \cdot \vec{dl} = \iint \vec{j} \cdot \vec{dS}
\]

Le contour $\mathcal{C}_1$ permet de calculer le champ dans le cuivre :

\[
a||\vec{H}|| = ay||\vec{j}||
\]

\[
||\vec{H}|| = y||\vec{j}||
\]

En magnétostatique, le courant est uniformément réparti dans le conducteurs, et donc $I = ah_1||\vec{j}||$. Par conséquent :

\[
||\vec{H}|| = \frac{y}{ah_1}I
\]

Tandis que $\mathcal{C}_2$ nous amène au champ dans l'air de l'encoche :

\[
||\vec{H}|| = \frac{I}{a}
\]

Le champ dans l'encoche a donc pour allure :

\textcolor{red}{(figure)}

Enfin, $\mathcal{C}_3$ est le seul contour traversant à la fois le stator et le rotor. Il est seul contributeur de la conversion électro-mécanique, le reste constituant donc les fuites de flux \textcolor{red}{(compléter)}.

L'inductance de fuite de cette encoche peut se calculer par des considérations énergétiques. En effet, on sait d'une part que l'énergie dans une inductance est :

\[
W = \frac{1}{2}LI^2
\]

A mettre en relation avec l'expression de la densité d'énergie magnétique dans le vide \textcolor{red}{(pas fait)} ; le cuivre et l'air étant supposés de perméabilité égale à $\mu_0$ :

\[
w = \frac{1}{2}\mu_0||\vec{H}||^2
\]

Il ne reste plus qu'à dérouler le calcul. Le problême étant compris dans le plan, c'est à l'inductance linéique qu'on s'intéresse :

\[
\frac{1}{2}LI^2 = \iint \frac{1}{2}\mu_0||\vec{H}||^2 \; dx \; dy
\]

\[
LI^2 = a\left(\int_0^{h_1} \mu_0\frac{y^2}{a^2h_1^2}I^2 \; dy + \int_{h_1}^{h_1+h_2} \mu_0\frac{I^2}{a^2} \; dy\right)
\]

\[
L = \mu_0\left(\frac{h_1^3}{3ah_1^2} + \frac{h_2}{a}\right)
\]

\[
L = \frac{\mu_0}{a}\left(\frac{h_1}{3} + h_2\right)
\]

Qui est donc l'inductance de fuite linéique rotorique, qui donne ce qu'on note communément $l_2$ après multiplication par la longueur du rotor.

Les pertes joules sont évidemment modélisées par une résistance linéique :

\[
R = \frac{1}{\sigma a h_1}
\]

Qui correspond à $R_2$ dans le modèle transformateur quand on la multiplie par la longueur du rotor.

La magnétostatique est cependant un contexte d'étude inapproprié à la machine asynchrone. Essayons donc de trouver un résultat équivalent en magnétodynamique (prenant en compte la dépendance avec $\omega$, en négligeant les effets propagatifs).

La principale différence est que l'hypothèse selon laquelle $\vec{j}$ est uniforme dans le cuivre n'est plus valable. Néanmoins, le champ $\vec{H}$ dans l'air conserve la valeur précédemment calculée.

On effectue une étude magnéto-harmonique, ce qui revient à imposer un courant :

\[
I = |\underline{I}|\sqrt{2}	\cos(\omega t)
\]

Nous allons devoir manipuler l'équation aux dérivées partielles en $\vec{H}$ de la magnétodynamique pour calculer celui-ci dans le cuivre :

\[
\Delta \vec{H} -\mu_{0}\sigma\frac{\partial\vec{H}}{\partial t} = 0
\]

Le problême est unidimensionnel, et le cadre d'étude harmonique dans lequel on se place fait qu'on passe naturellement en complexe :

\[
\frac{\partial^2 \underline{H}_x}{\partial y^2} -j\omega\mu_{0}\sigma\underline{H}_x = 0
\]

Dont la solution générale s'écrit :

\[
\underline{H}_x = \lambda e^{\sqrt{j\omega\mu_{0}\sigma}y}
+ \mu e^{-\sqrt{j\omega\mu_{0}\sigma}y}
\]

Et parceque $\sqrt{j} = \sqrt{e^{j\frac{\pi}{2}}} = e^{j\frac{\pi}{4}} = \frac{1+j}{\sqrt{2}}$ :

\[
\underline{H}_x = \lambda e^{(1+j)\sqrt{\frac{\omega\mu_{0}\sigma}{2}}y}
+ \mu e^{-(1+j)\sqrt{\frac{\omega\mu_{0}\sigma}{2}}y}
\]

En introduisant $\delta = \sqrt{\frac{2}{\omega\mu_{0}\sigma}}$ l'\emph{épaisseur de peau} (notion très importante en magnétodynamique) :

\[
\underline{H}_x = \lambda e^{\frac{1+j}{\delta}y}
+ \mu e^{-\frac{1+j}{\delta}y}
\]

Reste à déterminer les constantes $\lambda$ et $\mu$. Pour ce faire, on doit formuler des conditions aux limites qui sont :

\[
\left\{\begin{matrix}
\underline{H}_x(0) = 0 \\
\underline{H}_x(h_1) = -\frac{\underline{I}}{a}
\end{matrix}\right.
\]

De la première condition découle immédiatement $\lambda + \mu =0$, et donc :

\[
\underline{H}_x = \lambda \left(e^{\frac{1+j}{\delta}y}
- e^{-\frac{1+j}{\delta}y}\right)
\]

De la seconde :

\[
\lambda \left(e^{\frac{1+j}{\delta}h_1}
- e^{-\frac{1+j}{\delta}h_1}\right) = -\frac{\underline{I}}{a}
\]

\[
\lambda = -\frac{\underline{I}}{a\left(e^{\frac{1+j}{\delta}h_1}
- e^{-\frac{1+j}{\delta}h_1}\right)}
\]

Et donc :

\[
\underline{H}_x = - \frac{e^{\frac{1+j}{\delta}y}
- e^{-\frac{1+j}{\delta}y}}{e^{\frac{1+j}{\delta}h_1}
- e^{-\frac{1+j}{\delta}h_1}}\frac{\underline{I}}{a}
\]

La densité d'énergie magnétique dans le cuivre est alors :

\[
w = \frac{1}{2}\mu_0||\vec{H}||^2
\]

Les complexes permettent d'obtenir directement la valeur moyenne \textcolor{red}{(justifier)} :

\[
\braket{w} = \frac{1}{2}\mu_0|\underline{H}_x|^2
\]

\[
\braket{w} = \frac{1}{2}\mu_0\underline{H}_x\underline{H}_x^*
\]

On se doute que les calculs sont amenés à devenir un peu inintelligibles :

\[
\braket{w} = \frac{1}{2}\mu_0
\frac{e^{\frac{1+j}{\delta}y}
- e^{-\frac{1+j}{\delta}y}}{e^{\frac{1+j}{\delta}h_1}
- e^{-\frac{1+j}{\delta}h_1}}
\frac{e^{\frac{1-j}{\delta}y}
- e^{-\frac{1-j}{\delta}y}}{e^{\frac{1-j}{\delta}h_1}
- e^{-\frac{1-j}{\delta}h_1}}
\frac{\underline{I}\underline{I}^*}{a^2}
\]

\[
\braket{w} = \frac{1}{2}\mu_0
\frac{e^{\frac{2}{\delta}y}
-e^{j\frac{2}{\delta}y}-e^{-j\frac{2}{\delta}y}+e^{-\frac{2}{\delta}y}}
{e^{\frac{2}{\delta}h_1}
-e^{j\frac{2}{\delta}h_1}-e^{-j\frac{2}{\delta}h_1}+e^{-\frac{2}{\delta}h_1}}
\frac{|\underline{I}|^2}{a^2}
\]

\[
\braket{w} = \frac{1}{2}\mu_0
\frac{\cosh\left(\frac{2}{\delta}y\right)-\cos\left(\frac{2}{\delta}y\right)}
{\cosh\left(\frac{2}{\delta}h_1\right)-\cos\left(\frac{2}{\delta}h_1\right)}
\frac{|\underline{I}|^2}{a^2}
\]

Nous ne sommes plus qu'à une étape de trouver l'inductance de fuite linéique :

\[
\frac{1}{2}L|\underline{I}|^2 = \iint \frac{1}{2}\mu_0|\underline{H}_x|^2 \; dx \; dy
\]

\[
L|\underline{I}|^2 = a\left(\int_0^{h_1}\mu_0
\frac{\cosh\left(\frac{2}{\delta}y\right)-\cos\left(\frac{2}{\delta}y\right)}
{\cosh\left(\frac{2}{\delta}h_1\right)-\cos\left(\frac{2}{\delta}h_1\right)}
\frac{|\underline{I}|^2}{a^2} \; dy 
+ \int_{h_1}^{h_1+h_2} \mu_0\frac{|\underline{I}|^2}{a^2} \; dy\right)
\]

\[
L = \frac{\mu_0}{a}\left(\frac{\delta}{2}\frac{\sinh\left(\frac{2}{\delta}h_1\right)-\sin\left(\frac{2}{\delta}h_1\right)}
{\cosh\left(\frac{2}{\delta}h_1\right)-\cos\left(\frac{2}{\delta}h_1\right)} + h_2\right)
\]

A travers l'épaisseur de peau, cette inductance dépend donc de la pulsation. On continu sur notre lancée avec le calcul de la résistance linéique, ce qui présuppose de connaître la répartition de $\vec{j}$. Maxwell-Ampère :

\[
\vec{j} = \nabla \times \vec{H}
\]

\[
\underline{\vec{j}} = -\frac{\partial \underline{H}_x}{\partial y} \vec{e_z}
\]

\[
\underline{\vec{j}} = -\frac{1+j}{\delta}\frac{e^{\frac{1+j}{\delta}y}
+ e^{-\frac{1+j}{\delta}y}}{e^{\frac{1+j}{\delta}h_1}
- e^{-\frac{1+j}{\delta}h_1}}\frac{\underline{I}}{a} \vec{e_z}
\]

Les pertes joules volumiques :

\[
\braket{w} = \frac{1}{\sigma}\underline{\vec{j}}\cdot\underline{\vec{j}}^*
\]

\[
\braket{w} = \frac{2}{\sigma\delta^2}
\frac{\cosh\left(\frac{2}{\delta}y\right)+\cos\left(\frac{2}{\delta}y\right)}
{\cosh\left(\frac{2}{\delta}h_1\right)-\cos\left(\frac{2}{\delta}h_1\right)}
\frac{|\underline{I}|^2}{a^2}
\]

Ce qui nous amène à une résistance linéique :

\[
R|\underline{I}|^2 = \iint \frac{2}{\sigma\delta^2}
\frac{\cosh\left(\frac{2}{\delta}y\right)+\cos\left(\frac{2}{\delta}y\right)}
{\cosh\left(\frac{2}{\delta}h_1\right)-\cos\left(\frac{2}{\delta}h_1\right)}
\frac{|\underline{I}|^2}{a^2} \; dx \; dy
\]

\[
R = \frac{2}{\sigma a\delta^2} \int_0^{h_1}
\frac{\cosh\left(\frac{2}{\delta}y\right)+\cos\left(\frac{2}{\delta}y\right)}
{\cosh\left(\frac{2}{\delta}h_1\right)-\cos\left(\frac{2}{\delta}h_1\right)} \; dy
\]

\[
R = \frac{1}{\sigma a\delta} \frac{\sinh\left(\frac{2}{\delta}h_1\right)+\sin\left(\frac{2}{\delta}h_1\right)}
{\cosh\left(\frac{2}{\delta}h_1\right)-\cos\left(\frac{2}{\delta}h_1\right)}
\]

Nous venons d'effectuer l'étude complète d'une encoche rotorique de machine asynchrone, de façon totalement analytique et en utilisant les méthodes de la magnétodynamique.

\textcolor{red}{(compléter avec l'étude du stator)}

\subsubsection{Modèle externe de la machine asynchrone}

Parce-que le fonctionnement de la machine asynchrone repose sur l'induction, le modèle qui la décrira sera basé sur celui du transformateur triphasé :

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
  \draw	(0,0) to[R, l=$R_1$, i=$I_1$, *-] ++(2,0) to[L, l=$l_1$] ++(2,0) node[shape=coordinate](A){} to[short, i=$I_0$] ++(0,-1) node[shape=coordinate](B){} -- ++(-0.5,0) to[R, l=$R_f$] ++(0,-2) - - ++(0.5,0) -- ++(0,-1) node[shape=coordinate](C){} to[short, -*] ++(-4,0);
  \draw (B) -- ++(0.5,0) to[L, l=$L_\mu$] ++(0,-2) -- ++(-0.5,0);
  \draw (C) to[short] ++(1,0) -- ++(2,2) -- ++(0.5,0) -- ++(2,-2) to[short, -*] ++(4,0);
  \draw (A) to[short, i<=$mI_2$] ++(1,0) -- ++(2,-2) node[draw, fill=white, shape=circle, minimum size = 1cm](O){} ++(0.5,0) node[draw, fill=white, shape=circle, minimum size = 1cm](P){} -- ++(2,2) to[R, l=$l_2$, i<=$I_2$] ++(2,0) to[L, l=$R_2$, -*] ++(2,0);
\end{tikzpicture}
\end{center}
\shorthandon{:!}

Le rotor étant court-circuité en fonctionnement normal. La connaissance de l'inductance primaire $l_1$ n'ayant pas grand intéret et la détermination de ce paramètre étant problématique sans accès aux grandeurs rotoriques, on la néglige. Il en va de même pour le rapport de transformation pur $m$, qui ne mérite pas d'être identifié étant donné l'effort que ça demande. On utilisera donc le modèle en court-circuit ramené au rotor d'une phase :

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
  \draw	(0,0) to[R, l=$R_1$, i=$I_1$, *-] ++(4,0) node[shape=coordinate](A){} to[short, i=$I_0$] ++(0,-1) node[shape=coordinate](B){} -- ++(-0.5,0) to[R, l=$R_f$] ++(0,-2) - - ++(0.5,0) -- ++(0,-1) node[shape=coordinate](C){} to[short, -*] ++(-4,0);
  \draw (B) -- ++(0.5,0) to[L, l=$L_\mu$] ++(0,-2) -- ++(-0.5,0);
  \draw (C) -- ++(3,0);
  \draw (A) to[L, l=$l'_2$, i<=$mI_2$] ++(3,0) to[R, l=$\frac{R'_2}{g}$] ++(0,-4);
\end{tikzpicture}
\end{center}
\shorthandon{:!}

\textcolor{red}{(justifier la division par le glissement)}

Nous verrons que toutes les grandeurs importantes dépendent du glissement $g=1-\frac{\omega}{\omega_s}$. La détermination des paramètres du s'effectuera de façon tout à fait analogue à celle adoptée pour le transformateur.

En premier lieu, $R_1$ est déterminé par à l'ohm-mètre (en mesurant la moyenne sur les trois phase dans le cas d'une machine triphasé)).

L'essai en court-circuit correspond au fonctionnement en transformateur simple, c'est-à dire à vitesse nulle (on parle de \emph{rotor bloqué}). On a alors $g=1$ :

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
  \draw	(0,0) to[R, l=$R_1$, i=$I_1$, *-] ++(4,0) node[shape=coordinate](A){} to[short, i=$I_0$] ++(0,-1) node[shape=coordinate](B){} -- ++(-0.5,0) to[R, l=$R_f$] ++(0,-2) - - ++(0.5,0) -- ++(0,-1) node[shape=coordinate](C){} to[short, -*] ++(-4,0);
  \draw (B) -- ++(0.5,0) to[L, l=$L_\mu$] ++(0,-2) -- ++(-0.5,0);
  \draw (C) -- ++(3,0);
  \draw (A) to[L, l=$l'_2$, i<=$mI_2$] ++(3,0) to[R, l=$R'_2$] ++(0,-4);
\end{tikzpicture}
\end{center}
\shorthandon{:!}

En négligeant l'influence de la branche magnétisante, sous l'hypothèse où $I_0$ est très petit devant $mI_2$ aux tensions auxquelles s'effectuent l'essai :

\[
\left\{ \begin{matrix}
P = (R_1+R'_2)|\underline{I}_1|^2 \\
Q = l'_2\omega|\underline{I}_1|^2
\end{matrix}\right.
\]

L'essai à vide correspond au synchronisme, cas dans lequel on n'a pas d'induction car le flux est en phase avec le rotor, et aucune force-électromotrice n'apparaît dans les enroulements rotoriques. On trouve qu'alors $g=0$ et $I_2=0$ ; et c'est bien à un transformateur à vide qu'on a affaire. 

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
  \draw	(0,0) to[R, l=$R_1$, i=$I_1$, *-] ++(4,0) node[shape=coordinate](A){} to[short, i=$I_0$] ++(0,-1) node[shape=coordinate](B){} -- ++(-0.5,0) to[R, l=$R_f$] ++(0,-2) - - ++(0.5,0) -- ++(0,-1) node[shape=coordinate](C){} to[short, -*] ++(-4,0);
  \draw (B) -- ++(0.5,0) to[L, l=$L_\mu$] ++(0,-2) -- ++(-0.5,0);
\end{tikzpicture}
\end{center}
\shorthandon{:!}

Cet état est difficile à atteindre car le couple y est nul puisque les courants rotoriques sont absents. La solution consiste à se placer à puissance mécanique constante, ce qui est le cas en fonctionnement moteur à fréquence constante pour des raisons que nous verrons plus loin.

A l'exception de cette subtilité, la méthode vue dans la partie sur les tranformateurs peut ainsi être directement appliquée pour identifier les paramètres électriques de la machine asynchrone.

La formule permettant d'identifier les paramètres au primaire (d'après celle établie pour l'essai à vide du transformateur) devient :

\[
\left\{ \begin{matrix}
P = R_1|\underline{I}_1|^2+\frac{|\underline{V}_1-R_1\underline{I}_1|^2}{R_f} + \vec{\Gamma}\cdot\vec{\Omega} \\
Q = \frac{|\underline{V}_1-R_1\underline{I}_1|^2}{L_\mu\omega}
\end{matrix}\right.
\]

En outre, cette expression demande à être adapté pour la machine asynchrone triphasée qui est la machine la plus répandue de toutes à ce jour à moyenne puissance :

\[
\left\{ \begin{matrix}
P = 3R_1|\underline{I}_1|^2+3\frac{|\underline{V}_1-R_1\underline{I}_1|^2}{R_f} + \vec{\Gamma}\cdot\vec{\Omega} \\
Q = 3\frac{|\underline{V}_1-R_1\underline{I}_1|^2}{L_\mu\omega}
\end{matrix}\right.
\]

En pratique, si l'on n'est pas trop exigeant sur la précision du résultat, on pourra souvent négliger l'influence de $R_1$ sur la tension aux bornes de la branche magnétisante.

On remarquera que le couple nécessitant un courant rotorique, la résistance $R_2$ existe même dans le modèle idéal. Les pertes $R_2|\underline{I}_2|^2$ sont donc inévitables, et même nécessaires au fonctionnement de la machine.

Le glissement étant la grandeur centrale de la machine asynchrone, on écrira sa définition sous la forme :

\[\boxed{
\Omega = \frac{\omega_s}{p}(1-g)
}\]

Qui constitue notre première équation mécanique. La seconde sera plus difficile à établir :


\textcolor{red}{(compléter)}

\[\boxed{
\Gamma = \frac{2\Gamma_{max}}{\frac{g}{g_{max}}+\frac{g_{max}}{g}}
}\]

Qui graphiquement a pour allure :

\begin{sagesilent}
Gamma=var('Gamma');
g = var('g');
g_coords = [g for g in srange(-2,2,0.01)];
g_max = 0.2;
Gamma_coords = [(2/(g/g_max+g_max/g)).n() for g in g_coords];
output = "";
for i in range(0,len(g_coords)-1):
    output += "\draw[red, thick] ("+str(g_coords[i])+","+str(Gamma_coords[i])+")--("+str(g_coords[i+1])+","+str(Gamma_coords[i+1])+");\n";
\end{sagesilent}

\begin{center}
\begin{tikzpicture}[x=5\textwidth/6/(4)), y=\textwidth/3/2]
	// Axes
	\draw[->] (-2.2,0) -- (2.2,0) node[right] {$g$};
	\draw[->] (0,-1.2) -- (0,1.2) node[above] {$\Gamma(g)$};
	//Graduation angulaire
	\foreach \g in {-2,-1,1,2}
     	\draw (\g,0.08) -- (\g,-0.08)
		node[anchor=north] {$\g$};
		
	\draw (0,1) node[left] {$\Gamma_{max}$};
	\draw[dotted] (0,1) -- ++(0.2,0);
	\draw (0.2,0) node[below] {$g_{max}$};
	\draw[dotted] (0.2,0) -- ++(0,1);
	\if\releaseversion1
	\sagestr{output}
	\fi
\end{tikzpicture}
\end{center}

De la combinaison des deux équations précédentes résulte la \emph{caractéristique couple-vitesse} \textcolor{red}{(expression)} :

\begin{sagesilent}
Gamma=var('Gamma');
g = var('g');
g_coords = [g for g in srange(-2,2,0.01)];
g_max = 0.2;
Gamma_coords = [(2/(g/g_max+g_max/g)).n() for g in g_coords];
omega_s = 2*pi*30;
Omega_coords = [(omega_s*(1-g)).n() for g in g_coords];
output = "";
for i in range(0,len(g_coords)-1):
    output += "\draw[red, thick] ("+str(Omega_coords[i])+","+str(Gamma_coords[i])+")--("+str(Omega_coords[i+1])+","+str(Gamma_coords[i+1])+");\n";
\end{sagesilent}

\begin{center}
\begin{tikzpicture}[x=5\textwidth/6/(2*pi*30*4)), y=\textwidth/3/(2)]
	// Axes
	\draw[->] (-1.2*2*pi*30,0) -- (3.2*2*pi*30,0) node[right] {$\omega$};
	\draw[->] (0,-1.2) -- (0,1.2) node[above] {$\Gamma(g)$};
	//Graduation angulaire
	\foreach \omega in {-1,1,2,3}
     	\draw (\omega*2*pi*30,-0.08) -- (\omega*2*pi*30,0.08)
		node[above] {$\omega \frac{w_s}{p}$};
		
	\draw (0,1) node[left] {$\Gamma_{max}$};
	\draw[dotted] (0,1) -- ++(0.8*2*pi*30,0);
	\draw (0.8*2*pi*30,0) node[below] {$\omega_{max}$};
	\draw[dotted] (0.8*2*pi*30,0) -- ++(0,1);
	\if\releaseversion1
	\sagestr{output}
	\fi
\end{tikzpicture}
\end{center}

On remarque une chose : la zone de fonctionnement stable (pour laquelle $\frac{\partial \Gamma}{\partial \Omega}<0$) est comprise entre $\omega_{max}$ et $\omega_{min}$. Or, cette zone est relativement étroite ; ce qui implique qu'à fréquence constante, si une variation de la tension induit une forte variation du couple, la vitesse de rotation reste relativement proche de la vitesse de synchronisme.

Pour un modèle mécanique dépendant continuement de la vitesse, on aura donc un couple quasiment constant et donc une puissance mécanique constante. C'est la raison pour laquelle lors de l'essai à vide, on peut considérer que $\vec{\Gamma}\cdot\vec{\Omega}$ ne varie pas (hypothèse indispensable à la détermination de $R_f$, comme nous l'avons vu).

Par ailleurs, le rendement d'une machine asynchrone est limité :

\[
\eta \leq \frac{\vec{\Gamma}\cdot\vec{\Omega}}{\frac{R_2}{g}|\underline{I}_2|^2}
\]

\textcolor{red}{(compléter)}

\[\boxed{
\eta \leq 1-g
}\]

\section{Electronique pour le signal}

\subsection{Amplificateur opérationnel}

\subsection{Filtrage \& oscillateurs analogique}

\section{Electronique de puissance}

\subsection{Conversion DC-DC}
La conversion statique de l'électricité répond au besoin souvent rencontré consistant à devoir adapter le niveau de tension d'une source continue avec conservation de la puissance (aux pertes près).

On utilisera à cet effet des interrupteurs commandés à fréquence élevée, et des composants passifs servants de petits réservoirs d'énergie permettants de lisser les grandeurs.

\subsubsection{Modélisation moyenne}

Le montage suivant est la forme la plus simple du hacheur Buck :

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
  \coordinate (A) at (3,2);
  \coordinate (B) at (6,2);
  \draw	(0,0) to[european voltage source, v=$E$, *-*] (0,2) 
  to[switch, l=$K$] (A);
  \draw (3,0) to[Do, l=$D$] ++(0,2);
  \draw (A) -- ++(2,0) to[european current source, i=$I$, *-*] ++(0,-2) -- ++(-2,0);
  \draw (0,0) -- (4,0); 
\end{tikzpicture}
\end{center}
\shorthandon{:!}

Il permet de maîtriser le transfert d'énergie entre les deux sources. En effet, l'interrupteur étant ouvert on se réduit au schéma suivant :

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
  \coordinate (A) at (3,2);
  \coordinate (B) at (6,2);
  \draw	(0,0) to[european voltage source, v=$E$, *-*] (0,2);
  \draw (3,0) -- ++(0,2);
  \draw (A) -- ++(2,0) to[european current source, i=$I$, *-*] ++(0,-2) -- ++(-2,0);
  \draw (3,0) -- (4,0); 
\end{tikzpicture}
\end{center}
\shorthandon{:!}

Dans lequel aucune puissance n'est transmise d'aucune sorte. Cependant, une fois la fermeture effectuée il devient :

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
  \coordinate (A) at (3,2);
  \coordinate (B) at (6,2);
  \draw	(0,0) to[european voltage source, v=$E$, *-*] (0,2) -- ++(2,0) to[european current source, i=$I$, *-*] ++(0,-2) -- ++(-2,0);
\end{tikzpicture}
\end{center}
\shorthandon{:!}

Qui illustre un transfert de puissance parfait. En théorie, en commandant correctement ce montage (en ouvrant et fermant $K$ quand il le faut), on devrait pouvoir transférer l'énergie entre deux sources de façon tout à fait maitrisée.

Cependant, ce montage a plusieurs inconvénients en pratique :

\begin{itemize}
\item Les sources ne sont jamais idéales et ont des comportements qui leurs sont propres. C'est pourquoi $E$ et $I$ peuvent varier, et on devra veiller à que ces variations se fassent avec un temps caractéristique long devant la période de commutation de  l'interrupteur. En outre, ces imperfections des sources peuvent engendrer des non-linéarités.
\item Les sources peuvent posséder des courants et tensions maximales qui peuvent être plus petites que la valeur nominale de l'autre source. Ce montage provoquerait alors des surtensions et surintensités.
\item Le transfert de puissance ne peut se faire que dans un seul sens, c'est à dire que s'il advenait que $I$ devienne négatif, ouvrir l'interrupteur détruirait la diode. De la même façon, si $E$ se trouvait négative, la source de tension serait court-circuitée. On dit que ce montage ne travaille que dans un seul \emph{quadran}.
\item Les commutations ne sont pas maitrisées, dans le sens où dans notre modèle on ne sait pas ce qu'il se passe pendant une commutation. Dans les faits, les transistors qui assurent le rôle d'interrupteur sont susceptibles de passer par des états où le produit $UI$ n'est pas nul ; ce qui engendre une dissipation d'énergie non-désirée.
\end{itemize}

Nous allons progressivement discuter des solutions qu'on peut trouver à chacun de ces points.

Le premier constitue toute la problématique de l'électronique de commande et du contrôle des systèmes que nous aborderons ultérieurement.

Attaquons-nous au second point. Voici le schéma complet du hacheur Buck :

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
  \coordinate (A) at (3,2);
  \coordinate (B) at (6,2);
  \draw	(0,0) to[V, v=$E$, *-*] (0,2) 
  to[switch, l=$K$] (A);
  \draw (3,0) to[Do, l=$D$] ++(0,2);
  \draw (A) to[L, l=$L$] (B)
  to[C, l=$C$] ++(0,-2);
  \draw (B) -- ++(2,0) to[I, i=$I$, *-*] ++(0,-2) -- ++(-2,0);
  \draw (0,0) -- (6,0); 
\end{tikzpicture}
\end{center}
\shorthandon{:!}

Pour toute la discussion qui va suivre, $\sqrt{LC}$ sera choisie très grand devant la période de commutation notée $T$, de sorte que les effets oscillatoires ne se fassent pas ressentirs.

Dans un premier temps, nous allons faire l'analyse qualitative d'un modèle moyen, qui est ce vers quoi tend en régime permanant le système réel quand on commute à vitesse infinie. Souvenez-vous que $L$ et $C$ s'opposent respectivement aux variations de courant et de tension. L'idée est donc de remplacer l'inductance et le condensateur par des sources :

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
  \coordinate (A) at (3,2);
  \coordinate (B) at (6,2);
  \draw	(0,0) to[V, v=$E$, *-*] (0,2) 
  to[switch, l=$K$] (A);
  \draw (3,0) to[Do, l=$D$] ++(0,2);
  \draw (A) to[I, i=$\overline{I_L}$] (B)
  to[V, v<=$\overline{V_C}$] ++(0,-2);
  \draw (B) -- ++(2,0) to[I, i=$I$, *-*] ++(0,-2) -- ++(-2,0);
  \draw (0,0) -- (6,0); 
\end{tikzpicture}
\end{center}
\shorthandon{:!}

Si l'interrupteur est fermé, on a le schéma équivalent :

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
  \coordinate (B) at (3,2);
  \draw	(0,0) to[V, v=$E$, *-*] (0,2)
  to[I, i=$\overline{I_L}$] (B)
  to[V, v<=$\overline{V_C}$] ++(0,-2);
  \draw (B) -- ++(2,0) to[I, i=$I$, *-*] ++(0,-2) -- ++(-2,0);
  \draw (0,0) -- (4,0); 
\end{tikzpicture}
\end{center}
\shorthandon{:!}

Pas question d'entreprendre le calcul des grandeurs $\overline{V_L}$ et $\overline{I_L}$, mais on observe néanmoins que le courant $\overline{I_L}$ est réparti entre le condensateur et la charge. La tension aux bornes du condensateur est égale à celle aux bornes de la charge. 

Et s'il est ouvert :

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
  \coordinate (A) at (3,2);
  \coordinate (B) at (5,2);
  \draw	(0,0) to[V, v=$E$, *-*] (0,2);
  \draw (3,0) to[I, i=$\overline{I_L}$] ++(0,2);
  \draw (A) -- (B)
  to[V, v<=$\overline{V_C}$] ++(0,-2);
  \draw (B) -- ++(2,0) to[I, i=$I$, *-*] ++(0,-2) -- ++(-4,0);
\end{tikzpicture}
\end{center}
\shorthandon{:!}

Ce qui a été dit juste avant reste vrai. En fait, la situation de la charge n'a pas été modifiée lors de l'ouverture.

Mieux encore, en commandant $K$ en créneaux de rapport cyclique $\alpha$,  nous pouvons imaginer un montage équivalent global :

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
  \coordinate (A) at (3,2);
  \coordinate (B) at (5,2);
  \draw	(0,0) to[V, v=$E$, *-*] (0,2) 
  to[I, i=$\alpha I_L$] (A);
  \draw (3,0) to[I, i=$(1-\alpha) \overline{I_L}$] ++(0,2);
  \draw (A) -- (B)
  to[V, v<=$\overline{V_C}$] ++(0,-2);
  \draw (B) -- ++(2,0) to[I, i=$I$, *-*] ++(0,-2) -- ++(-2,0);
  \draw (0,0) -- (6,0); 
\end{tikzpicture}
\end{center}
\shorthandon{:!}

Nous sommes toujours dans l'incapacité de calculer $V_C$ et $I_L$, par contre nous pouvons écrire une relation entre les deux par conservation de l'énergie (le régime permanant étant établi, les sources intermédiaires de travaillent pas) :

\[
\alpha E \overline{I_L} = \overline{V_C} I
\]

Cette première approche sera formalisée plus tard \textcolor{red}{(pas fait)}. Elle permet déjà néanmoins de mettre en évidence l'objet principal de la conversion statique : il existe dans ce modèle moyen un rapport $\frac{\overline{V_c}}{E} = \frac{\alpha \overline{I_L}}{I}$ entre la tension d'entrée et celle de sortie.

\subsubsection{Mode de conduction continu}

Revenons maintenant au schéma réel, mais restons en régime permanant :

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
  \coordinate (A) at (3,2);
  \coordinate (B) at (6,2);
  \draw	(0,0) to[V, v=$E$, *-*] (0,2) 
  to[switch, l=$K$] (A);
  \draw (3,0) to[Do, l=$D$] ++(0,2);
  \draw (A) to[L, l=$L$, i=$I_L$] (B)
  to[C, l=$C$, v<=$V_C$] ++(0,-2);
  \draw (B) -- ++(2,0) to[I, i=$I$, *-*] ++(0,-2) -- ++(-2,0);
  \draw (0,0) -- (6,0); 
\end{tikzpicture}
\end{center}
\shorthandon{:!}

L'idée va maintenant être de proposer une modélisation plus correcte afin d'affecter au rapport des tensions une valeur ne dépendant que du rapport cyclique et des composants.

En premier lieu, le hacheur Buck admet deux modes de fonctionnement qualifiés de \emph{conduction continue} et \emph{conduction discontinue}. Commençons par nous intéresser au mode de conduction continu dans lequel la diode et l'interrupteur ne conduisent jamais simultanément.

Je vous laisse vous convaincre que durant la \emph{séquence 1} pendant laquelle l'interrupteur est fermé on peut écrire :

\[
\left\{\begin{matrix}
\frac{dI_L}{dt}+\frac{V_C}{L} = \frac{E}{L} \\
\frac{dV_C}{dt}-\frac{I_L}{C} = -\frac{I}{C} 
\end{matrix}\right.
\]

Et pendant la \emph{séquence 2}, pendant laquelle l'interrupteur est ouvert :

\[
\left\{\begin{matrix}
\frac{dI_L}{dt}+\frac{V_C}{L} = 0 \\
\frac{dV_C}{dt}-\frac{I_L}{C} = -\frac{I}{C} 
\end{matrix}\right.
\]

Puisque la séquence $1$ s'étend de $t=0$ à $t=\alpha T$ (avec $T$ la période de la commande), on peut réunir ce système différentiel linéaire par morceaux en un système non linéaire équivalent au premier ordre mais que les mathématiciens qualifient d'\emph{affine sur l'entrée} \textcolor{red}{(?)} :

\[
\left\{\begin{matrix}
\frac{dI_L}{dt}+\frac{V_C}{L} = \frac{\alpha E}{L} \\
\frac{dV_C}{dt}-\frac{I_L}{C} = -\frac{I}{C} 
\end{matrix}\right.
\]

La situation décrite par ce nouveau système n'est pas rigoureusement identique à la précédente, mais l'hypothèse $\sqrt{LC}\gg T$ permet de conclure que leurs réponses seront très semblables aux ondulations près.

Passons ce système sous forme de représentation d'état :

\[
\frac{d}{dt}\left(\begin{matrix}
I_L\\ V_C
\end{matrix}\right)
=
\left(\begin{matrix}
\frac{-1}{L} & 0 \\
0 & \frac{1}{C}
\end{matrix}\right)
\left(\begin{matrix}
I_L\\ V_C
\end{matrix}\right) +
\left(\begin{matrix}
\frac{E}{L} & -\frac{I}{L}
\end{matrix}\right)
\left(\begin{matrix}
\alpha\\ 1
\end{matrix}\right)
\]

On trouve sans surprise un $1$ dans le vecteur commande puisqu'il n'y qu'une entrée $\alpha$ pour deux variables d'état $I_L$ et $V_c$.

\textcolor{red}{(résolution, puis ajouter partie sur le Buck réversible)}

\subsection{Interrupteurs résonnants}

Nous prendrons dans cette section l'exemple du hacheur Buck, mais la notion d'interrupteur résonnant peut-être adaptée à toute sorte de montage d'électronique de puissance.

Dans la section précédente, nous avons commencé par 
présenter une première version simplifiée du hacheur Buck. Nous avons ensuite listé un certain nombre de défauts de ce montage, et nous avons montré qu'introduire l'inductance et la capacité en résolvait certains ; mais pas tous.

Notamment, nous avions dis que lors des commutations nous ne maîtrisions pas la puissance dans l'interrupteur, celle-ci dépendant du fonctionnement propre de l'interrupteur lui-même. Cela peut-être source de perte d'énergie dans le composant, nuisant au rendement du système et exigeant un refroidissement dimensionné plus largement.

Pour palier à ce problême, la première idée que nous pourrions avoir serait d'introduire une petite inductance en série avec l'interrupteur, celle-ci venant alors empécher le courant de croître brusquement au cours de la fermeture :

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
  \coordinate (A) at (4,2);
  \coordinate (B) at (6,2);
  \draw	(0,0) to[V, v=$E$, *-*] (0,2) -- ++(1,0)
  to[switch, l=$K$] ++(1,0)
  to[L, l=$L_K$] ++(1,0) -- (A);
  \draw (A)++(0,-2) to[Do, l=$D$, mirror] (A);
  \draw (A) to[L, l=$L$] (B)
  to[C, l=$C$] ++(0,-2);
  \draw (B) -- ++(2,0) to[I, i=$I$, *-*] ++(0,-2) -- ++(-2,0);
  \draw (0,0) -- (6,0);
\end{tikzpicture}
\end{center}
\shorthandon{:!}

Cependant un nouveau problême se pose. Lors de l'ouverture, $L_K$ impose pour un court moment un courant non-nul dans l'interrupteur, engendrant sa destruction.

La solution consiste à remplacer l'interrupteur par un thyristor à blocage spontané.

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
  \coordinate (A) at (4.5,2);
  \coordinate (B) at (6.5,2);
  \draw	(0,0) to[V, v=$E$, *-*] (0,2) -- ++(1,0)
  to[Ty, l_=$K$] ++(1.5,0)
  to[L, l=$L_K$] ++(1,0) -- (A);
  \draw (A)++(0,-2) to[Do, l=$D$, mirror] (A);
  \draw (A) to[L, l=$L$] (B)
  to[C, l=$C$] ++(0,-2);
  \draw (B) -- ++(2,0) to[I, i=$I$, *-*] ++(0,-2) -- ++(-2,0);
  \draw (0,0) -- (6.5,0);
  \draw (2.5,2.8) node[below left] {$a$};
\end{tikzpicture}
\end{center}
\shorthandon{:!}

Ce montage est moins facilement maîtrisable que le précédent. En effet, si on contrôle l'amorçage par la gâchette, il faut attendre que le courant devienne nul dans $L_k$ pour que le thyristor se bloque.

Par contre, l'avantage certain est que lors de l'amorçage, le courant ne peut pas croître aussi vite que la tension ne chute, ce qui permet de limiter très fortement la dissipation de chaleur dans le composant.

Il reste cependant un dernier problême à régler : l'inertie introduite par $L_k$ nuit aux performances du montage de par un autre aspect. En effet, on cherche toujours à maximiser la fréquence de découpage à laquelle on fait commuter l'interrupteur afin de limiter les ondulations ; et ici le courant dans l'interrupteur ne peut plus varier rapidement, et le transfert de puissance est donc freiné.

Une solution simple serait de choisir $C$ plus gros, mais il en existe une beaucoup plus économique. En ajoutant une capacité $C_K$ comme suit, on a un petit réservoir d'énergie qui se recharge quand $K$ est bloqué, et vient forcer le courant à croître rapidement dans $L_K$  (de par la tension qu'il impose à ses bornes) dès que $K$ s'amorçe.

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
  \coordinate (A) at (4.5,2);
  \coordinate (B) at (6.5,2);
  \draw	(0,0) to[V, v=$E$, *-*] (0,2) -- ++(1,0)
  to[Ty, l_=$K$, mirror] ++(1.5,0)
  to[L, l=$L_K$] ++(1,0) -- (A);
  \draw (A)++(0,-2) to[Do, l=$D$, mirror] (A);
  \draw (A) to[L, l=$L$] (B)
  to[C, l=$C$] ++(0,-2);
  \draw (B) -- ++(2,0) to[I, i=$I$, *-*] ++(0,-2) -- ++(-2,0);
  \draw (0,0) -- (6.5,0);
  \draw (2.5,1.6) node[below left] {$a$};
  \draw (A)++(-0.5,0) -- ++(0,1)
  to[C, l_=$C_K$] ++(-3,0) -- ++(0,-1);
\end{tikzpicture}
\end{center}
\shorthandon{:!}

Concrètement, la constante de temps $\sqrt{L_K C_K}$ introduite ici peut-être choisie arbitrairement petite, et c'est tout l'intéret de cette architecture. On parle d'\emph{interrupteur résonnant}.

\textcolor{red}{(Peut-être expliquer un peu plus ici...)}

Il existe en fait quatre interrupteurs quasi-résonnants usuels, un pour chaque type de thyristor et thyristor-dual :

\textcolor{red}{(Les dessiner, page 37 du poly de davat)}

Le montage ainsi constitué est dit \emph{quasi-résonnant}. Il n'est cependant pas un montage intéressant, car spécifiquement pour ce montage précis, nous allons voir que le mauvais interrupteur résonnant a été choisi.

\textcolor{red}{(Compléter à partir du cours de Davat)}

\subsection{Conversion AC-DC}

\subsubsection{Onduleur triphasé}

\textcolor{red}{(introduire)}

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
  \coordinate (A) at (2,5);
  \coordinate (B) at (3.5,5);
  \coordinate (C) at (5,5);
  \coordinate (D) at (2,3.5);
  \coordinate (E) at (3.5,2.5);
  \coordinate (F) at (5,1.5);
  \coordinate (G) at (6,3.5);
  \coordinate (H) at (6,2.5);
  \coordinate (I) at (6,1.5);
  \coordinate (N) at (10,2.5);
  \draw	(0,0) to[V, v=$E$, *-*] (0,5) -- (A)
  to[switch, l=$K_1$] ++(0,-2) -- ++(0,-1)
  to[switch, l=$K_2$] ++(0,-2) -- ++(-2,0);
  \draw	(A) -- ++(1.5,0) to[switch, l=$K_3$] ++(0,-2) -- ++(0,-1)
  to[switch, l=$K_4$] ++(0,-2) -- ++(-1.5,0);
  \draw	(B) -- ++(1.5,0) to[switch, l=$K_3$] ++(0,-2) -- ++(0,-1)
  to[switch, l=$K_4$] ++(0,-2) -- ++(-1.5,0);
 \draw (D) -- (G) to[R, v=$V_1$, *-] ++(4,0) -- (N);
 \draw (E) -- (H) to[R, v=$V_2$, *-] ++(4,0);
 \draw (F) -- (I) to[R, v=$V_3$, *-] ++(4,0) -- (N);
 \draw (G) to[open, v>=$U_{21}$] (H);
 \draw (H) to[open, v>=$U_{32}$] (I);
 \draw (I) to[open, v>=$U_{13}$] (G);
\end{tikzpicture}
\end{center}
\shorthandon{:!}

Sur le même bras, on veillera toujours à ne pas fermer les deux interrupteurs (ce qui court-circuiterait la source de tension. On considérera également qu'au moins un des deux interrupteurs doit-être fermé, de façon à ce que le courant puisse circuler (indispensable dans le cas d'une charge inductive). En pratique, on met une diode en antiparallèle de chacun des interrupteurs.

Les différents vecteurs commande des bras d'onduleur sont énumérés ci-dessous :

\[
\left( \begin{matrix}
0 \\ 0 \\ 0
\end{matrix} \right)
,
\left( \begin{matrix}
0 \\ 0 \\ 1
\end{matrix} \right)
,
\left( \begin{matrix}
0 \\ 1 \\ 0
\end{matrix} \right)
,
\left( \begin{matrix}
0 \\ 1 \\ 1
\end{matrix} \right)
,
\left( \begin{matrix}
1 \\ 0 \\ 0
\end{matrix} \right)
,
\left( \begin{matrix}
1 \\ 0 \\ 1
\end{matrix} \right)
,
\left( \begin{matrix}
1 \\ 1 \\ 0
\end{matrix} \right)
,
\left( \begin{matrix}
1 \\ 1 \\ 1
\end{matrix} \right)
\]

Chacun étant associé à une valeurs possible pour $U_i$ (le vecteur nul peut-être obtenu de deux manières différentes) :

\[
\left( \begin{matrix}
0 \\ 0 \\ 0
\end{matrix} \right)
,
\left( \begin{matrix}
0 \\ E \\ -E
\end{matrix} \right)
,
\left( \begin{matrix}
E \\ -E \\ 0
\end{matrix} \right)
,
\left( \begin{matrix}
E \\ 0 \\ -E
\end{matrix} \right)
,
\left( \begin{matrix}
-E \\ 0 \\ E
\end{matrix} \right)
,
\left( \begin{matrix}
-E \\ E \\ 0
\end{matrix} \right)
,
\left( \begin{matrix}
0 \\ -E \\ E
\end{matrix} \right)
\]

De composantes directes selon la transformée de Fortescue :

\[
0 , 
jE\sqrt{3}, 
E\sqrt{3}e^{-j\frac{\pi}{6}},
E\sqrt{3}e^{j\frac{\pi}{6}},
-E\sqrt{3}e^{j\frac{\pi}{6}},
-E\sqrt{3}e^{-j\frac{\pi}{6}},
-jE\sqrt{3}
\]

Ce qu'on peut représenter dans le plan complexe comme suit :

\begin{center}
\begin{tikzpicture}
// Axes
\draw[->] (-3,0) -- (3,0);
\draw (3,0) node[right] {$\Re U_d$};
\draw [->] (0,-3) -- (0,3);
\draw (0,3) node[above] {$j\Im U_d$};

// Contenu
\node at (0,0) {\textbullet};
\node at (30:2.6) {\textbullet};
\node at (90:2.6) {\textbullet};
\node at (150:2.6) {\textbullet};
\node at (210:2.6) {\textbullet};
\node at (270:2.6) {\textbullet};
\node at (330:2.6) {\textbullet};
\draw[dashed] (0,0) circle (2.6);
\draw[<->,,>=latex] (0,0) -- (-70:2.6) node[midway, above right] {$E\sqrt{3}$};

\end{tikzpicture}
\end{center}

Un résultat précédent à propos du couplage en étoile était :

\[
\left( \begin{matrix}
U_d \\
U_d^* \\
U_h
\end{matrix} \right)
=
\left( \begin{matrix}
\sqrt{3}e^{-j\frac{\pi}{6}} & 0 & 0 \\
0 & \sqrt{3}e^{j\frac{\pi}{6}} & 0 \\
0 & 0 & 0
\end{matrix} \right)
\left( \begin{matrix}
V_d \\
V_d^* \\
V_h
\end{matrix} \right)
\]

Dont on déduit :

\[
V_d = \frac{U_d}{\sqrt{3}}e^{j\frac{\pi}{6}}
\]

Dans le plan complexe :

\begin{center}
\begin{tikzpicture}
// Axes
\draw[->] (-3,0) -- (3,0);
\draw (3,0) node[right] {$\Re V_d$};
\draw [->] (0,-3) -- (0,3);
\draw (0,3) node[above] {$j\Im V_d$};

// Contenu
\node at (0,0) {\textbullet};
\node at (0:1.5) {\textbullet};
\node at (60:1.5) {\textbullet};
\node at (120:1.5) {\textbullet};
\node at (180:1.5) {\textbullet};
\node at (240:1.5) {\textbullet};
\node at (300:1.5) {\textbullet};
\draw[dashed] (0,0) circle (1.5);
\draw[<->,,>=latex] (0,0) -- (-70:1.5) node[midway, above right] {$E$};

\end{tikzpicture}
\end{center}

\textcolor{red}{(ajouter onduleur pleine onde)}

\subsubsection{Redresseur triphasé}

Observons le montage suivant :

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
  \coordinate (A) at (-2,-5);
  \coordinate (B) at (-3.5,-5);
  \coordinate (C) at (-5,-5);
  \coordinate (D) at (-2,-3.5);
  \coordinate (E) at (-3.5,-2.5);
  \coordinate (F) at (-5,-1.5);
  \coordinate (G) at (-6,-3.5);
  \coordinate (H) at (-6,-2.5);
  \coordinate (I) at (-6,-1.5);
  \coordinate (N) at (-10,-2.5);
  \draw	(0,0) to[R, i_=$I_c$, v^=$U_c$, *-*] (0,-5) -- (A)
  to[Ty] ++(0,2) -- ++(0,1)
  to[Ty] ++(0,2) -- ++(2,0);
  \draw	(A) -- ++(-1.5,0) to[Ty] ++(0,2) -- ++(0,1)
  to[Ty] ++(0,2) -- ++(1.5,0);
  \draw	(B) -- ++(-1.5,0) to[Ty] ++(0,2) -- ++(0,1)
  to[Ty] ++(0,2) -- ++(1.5,0);
 \draw (D) -- (G) to[V, v<=$V_3$, *-] ++(-4,0) -- (N);
 \draw (E) -- (H) to[V, v<=$V_2$, *-] ++(-4,0);
 \draw (F) -- (I) to[V, v<=$V_1$, *-] ++(-4,0) -- (N);
\end{tikzpicture}
\end{center}
\shorthandon{:!}

Le fait est que pour l'application qui va être la sienne, ce montage est classiquement obtenu avec six thyristor, mais il est possible d'en remplacer tout ou une partie par d'autres jonctions semi-conductrices comme la diode ou le transistor (faisant office d'interrupteur bicommandé). On peut ainsi imaginer un certain nombre de variantes à se montage (et à son homologue monophasé), tout en conservant cette topologie - dite en \emph{Pont de Graetz} - à trois bras.

Ce montage est un redresseur, qui a vocation à assurer la conversion AC-DC. En fait, il n'est qu'une reconfiguration de l'onduleur, lequel peut d'ailleurs lui-même fonctionner en redresseur si l'on utilise les bons composants.

Ces deux montages n'en sont en fait qu'un unique, la seule différence étant l'environnement dans lequel on l'intègre.

Ici, on suppose que les tensions à gauche sont celles d'un réseau électrique idéal, et donc tout à fait sinusoïdales. Les tensions entre phases s'écrivent :

\[
\left( \begin{matrix}
U_{13} \\
U_{21} \\
U_{32}
\end{matrix} \right)
= V_0 \sqrt{3}
\left( \begin{matrix}
\sin(\omega t - \frac{\pi}{6}) \\
\sin(\omega t + \frac{\pi}{2}) \\
\sin(\omega t + \frac{7\pi}{6})
\end{matrix} \right)
\]

\textcolor{red}{(pourquoi sin ?)}

Pour rappel, les phases initiales sont choisies de sorte que $V_1$ soit un sinus.

Nous allons nous placer en régime équilibré et permanent. Alors, le régime alternatif triphasé réduit l'intervalle d'étude à un sixième de période, du fait de la parité des grandeurs du réseau.

La technique qui va être présentée prend le nom de \emph{commande pleine onde} et consiste à découper au synchronisme avec le réseau (ce qui justifie en partie la pertinence de l'usage de thyristors).

Supposons qu'en $\frac{\pi}{6}$ les thyristors \emph{nord} et \emph{sud-ouest} soient amorçés :
\[
U_c = U_{21}
\]

Puis, ce n'est qu'en $\frac{\pi}{6}+\Psi$ avec $\Psi \in [0;\frac{\pi}{3}]$ l'\emph{angle de retard à l'amorçage} qu'on amorçera le thyristors \emph{sud-est} ; bloquant inévitablementaprès un court temps de commutation le thyristort \emph{sud-ouest} du fait que $U_{13}$ soit positif sur cet intervalle. On a alors :

\[
U_i = -U_{32} 
\]

Et ce jusqu'en $\frac{\pi}{2}$, où s'arrète notre intervalle d'étude. $U_c$ sera ensuite répliqué si l'on garde $\Psi$ constant :

\begin{sagesilent}
x=var('x');
t = var('t');
x_coords = [t for t in srange(0,2*pi,0.01)];
y1_coords = [sin(x-pi/6).n() for x in x_coords];
y2_coords = [sin(x+2*pi/3-pi/6).n() for x in x_coords];
y3_coords = [sin(x+4*pi/3-pi/6).n() for x in x_coords];
Psi = var('Psi');
Psi = 0.4;
xc_coords = [t for t in srange((pi/6).n(),(pi/2).n(),0.01)];
yc_coords = [t for t in srange((pi/6).n(),(2*pi).n(),0.01)];
n = len(xc_coords); 
for t in range(0,n):
    if xc_coords[t]<pi/6+Psi :
        yc_coords[t] = sin(xc_coords[t]+2*pi/3-pi/6).n();
    else:
        yc_coords[t] = (-sin(xc_coords[t]+4*pi/3-pi/6)).n();
xc_coords = [t for t in srange((pi/6).n(),(2*pi).n(),0.01)];
for t in range(0,len(xc_coords)):
    u=t%n;
    yc_coords[t] = yc_coords[u];
output = "";
for i in range(0,len(x_coords)-1):
    output += "\draw[red, thick] ("+str(x_coords[i])+","+str(y1_coords[i])+")--("+str(x_coords[i+1])+","+str(y1_coords[i+1])+");\n";
    output += "\draw[blue, thick] ("+str(x_coords[i])+","+str(y2_coords[i])+")--("+str(x_coords[i+1])+","+str(y2_coords[i+1])+");\n";
    output += "\draw[green, thick] ("+str(x_coords[i])+","+str(y3_coords[i])+")--("+str(x_coords[i+1])+","+str(y3_coords[i+1])+");\n";
for i in range(0,len(xc_coords)-1):
    output += "\draw[black, thick] ("+str(xc_coords[i])+","+str(yc_coords[i])+")--("+str(xc_coords[i+1])+","+str(yc_coords[i+1])+");\n";
\end{sagesilent}

\begin{center}
\begin{tikzpicture}[x=5\textwidth/6/(2*pi)), y=\textwidth/3/2]
	// Axes
	\draw[->] (0,0) -- (2*pi+0.2,0) node[right] {$\alpha \; [rad]$};
	\draw[->] (0,-1.1) -- (0,1.2) node[above] {$U_i(\alpha)$};
	//Graduation angulaire
	\foreach \x in {1,2,...,12}
     	\draw (pi/6*\x,0.08) -- (pi/6*\x,-0.08)
		node[anchor=north] {$\frac{\x \pi}{6}$};
		
	\draw (0,1) node[left] {$U_0$};
	\draw[dotted] (3.1416/6,0) -- ++(0,0.89);
	\draw[dotted] (3.1416/6+0.4,0) -- ++(0,0.6);
	\draw[dotted] (3.1416/2,0) -- ++(0/2,0.89);
	
	\draw (5*3.1416/6,0.8) node[anchor=north east] {\textcolor{red}{$U_{13}$}};
	\draw (10.5*3.1416/6,0.7) node[anchor=north west] {\textcolor{blue}{$U_{21}$}};
	\draw (9*3.1416/6,0.8) node[anchor=north east] {\textcolor{green}{$U_{32}$}};
	\draw (5*3.1416/6,0.86) node[anchor=south west] {\textcolor{black}{$U_c$}};
	\if\releaseversion1
	\sagestr{output}
	\fi
\end{tikzpicture}
\end{center}

Le plus simple est donc de reconsidérer l'intervalle d'étude, au profit de $[\frac{\pi}{2}+\Psi;\frac{5\pi}{6}+\Psi]$ dans lequel on a simplement :

\[
U_i = U_{13} 
\]

Sur ce sixième de période (et donc sur la période entière), on a pour valeur moyenne de la tension de charge :

\[
\braket{U_i} = \frac{3}{\pi}\int_{\frac{\pi}{2}+\Psi}^{\frac{5\pi}{6}+\Psi} U_{13}(\alpha) \; d\alpha
\]

\[
\braket{U_i} = \frac{3}{\pi}\int_{\frac{\pi}{2}+\Psi}^{\frac{5\pi}{6}+\Psi} U_0 \sin(\alpha-\frac{\pi}{6}) \; d\alpha
\]

\[
\braket{U_i} = \frac{3U_0}{\pi}\left( \cos\left(\frac{\pi}{3}+\Psi\right) - \cos\left(\frac{2\pi}{3}+\Psi\right) \right)
\]

\[
\braket{U_i} = \frac{3U_0}{\pi}\left( \cos\left(\frac{\pi}{3}+\Psi\right) + \cos\left(-\frac{\pi}{3}+\Psi\right) \right)
\]

\[
\braket{U_i} = \frac{6U_0}{\pi}
\cos\left(\frac{1}{2}\left(\frac{\pi}{3}-\frac{\pi}{3}\right)+\Psi\right)
\cos\left(\frac{1}{2}\left(\frac{\pi}{3}+\frac{\pi}{3}\right)\right)
\]

\[
\braket{U_i} = \frac{6U_0}{\pi}
\cos(\Psi)
\cos\left(\frac{\pi}{3}\right)
\]

\[
\braket{U_i} = \frac{3U_0}{\pi}
\cos(\Psi)
\]

On notera que remplacer tous les thyristors par des diodes revient à imposer $\Psi = 0$.

Rien n'interdit de choisir $\cos(\Psi)$ négatif, auquel cas on peut affecter une tension négative aux bornes de la charge et fonctionner. Cependant, il n'est pas question d'inverser le sens du courant de charge, et l'emploi des thyristors limite ainsi le fonctionnement du montage à seulement deux quadrants.

Pour fonctionner sur les quatre quadrants, l'emploi d'interrupteurs bicommandés devient nécessaire.

On voit que la forme de la tension aux bornes de la charge peut s'avérer très éloignées de celle d'une tension continue. Cela va altérer la forme du courant $I_c$ et de la puissance $U_c I_c$. Cela implique de devoir mettre en place des méthodes adaptées pour effectuer l'étude coté réseau et coté charge.

C'est pourquoi vous trouverez dans le chapitre d'analyse numérique une étude de cas sur la simulation d'un moteur à courant continu redressé par un pont de Graetz. Une étude harmonique aurait également pû s'avérer pertinente.

\textcolor{red}{(pont mixte, puissance réactive)}

\part{Physique statistique}
\setcounter{chapter}{0}

\chapter{Mouvement brownien}

\chapter{Transferts thermiques}

Soit un système macroscopique, constitué d'un ensemble de particules de masses $m$ et de vitesse moyenne $v$. On défini une grandeure appellée \emph{température} par $T = \frac{mv^2}{3k_B}$ avec $k_B$ la constante de Boltzmann.

Soit $u$ la densité d'énergie volumique d'agitation dans un milieu et $\vec{\varphi}$ la densité de flux de puissance. La conservation de l'énergie s'écrit en l'absence de source :

\[\boxed{
\frac{\partial u}{\partial t} + \nabla \cdot \vec{\varphi} = 0
}\]

Dans un milieu, on introduira la \emph{capacité thermique à pression constante} $c$ émanante des interaction entre particules, et qui donne une relation de proportionnalité entre la densité d'énergie locale d'agitation $u$ dans un milieu et la température en ce point :

\[\boxed{
u = \rho c T
}\]

La présence de la masse volumique $\rho$ se justifie par des raisons historiques \textcolor{red}{(pourquoi proportionnalité ?)}.

La conservation de l'énergie se réecrit donc :

\[\boxed{
\rho c\frac{\partial T}{\partial t} + \nabla \cdot \vec{\varphi} = 0
}\]

On a parfois plusieurs \emph{capacités thermiques} pour un même milieu, suivant les contraintes thermodynamiques qu'on se donne (volume constant, pression constante, etc...). Pour toute la suite on considèrera un milieu à pression constante et donc son $c_P$ \textcolor{red}{(pourquoi ?)}.

\section{Conduction}

Au sein d'un milieu continu homogène, un champ de température $T({x},t_0)$ sera la source d'une diffusion de l'énergie $\vec{\varphi}$ selon :

\[\boxed{
\vec{\varphi} = -\lambda \nabla T
}\]

Ce résultat est issu de la théorie du mouvement brownien appliquée au phonon.

Le couplage de ces deux équations permet d'établir l'équation de la chaleur :

\[
\rho c\frac{\partial T}{\partial t} -\lambda \nabla \cdot \nabla T = 0
\]

\[\boxed{
\rho c\frac{\partial T}{\partial t} -\lambda \Delta T = 0
}\]

En présence de source (dissipation par effet joule notamment), le terme de droite est non-nul.

\section{Convection}

Les phonons qui se meuvent dans le cadre de la conduction ne sont pas des particules de matières à proprement parler, mais des entités virtuelles qui véhiculent l'énergie de proche en proche.

La conduction n'est pas prépondérante dans les lieux où il y a réellement déplacement de matière ; dans un fluide par exemple. On parle alors de convection.

Le modèle usuel pour modéliser la convection consiste à considérer une paroi de température $T_p$, et à dire que cette paroi perd de l'énergie par convection vers le fluide, de sorte que la composante normale de son flux s'écrive :

\[\boxed{
\vec{\varphi} \cdot \vec{n} = h(T_p-T_{\infty})
}\]

Avec $h$ le coefficient de convection. Les autres composantes du flux peuvent-êtres non-nulles, mais relèvent de la conduction.

\section{Dipôles thermiques}

\subsection{Résistance thermique}
En régime permanent, le premier principe de la thermodynamique devient :

\[
\nabla \cdot \vec{\varphi} = 0
\]

Qui traduit la conservation du flux de puissance en l'absence de source.

On a donc une densité de flux $\varphi$ qui dérive d'un potentiel du fait que :

\[
\vec{\varphi} = -\lambda \nabla T
\]

Ces équations correspondent très exactement à celles de l'électrocinétique dans un milieu résistif, développées précédemment. Cette analogie permet d'introduire la notion de \emph{résistance thermique}.

En toute généralité, tous les résultats que nous avons vu pour les résistances électriques pourront s'appliquer aux résistances thermiques, puisque les formalismes sont identiques. Les courants deviendront les flux, et les potentiels les températures.

Soit un corps n'échangeant de l'énergie avec l'extérieur qu'à travers deux surfaces particulières à températures respectives $T_1$ et $T_2$ homogènes.

Ce corps est un dipôle thermique, auquel on pourra attribuer une résistance thermique $R$ de sorte que :

\[\boxed{
T_2 - T_1 = R \Phi
}\]

\textcolor{red}{(Trouver une expression générale de $R$)}

Deux milieux peuvent se combiner en série ou en parallèle de la même façon que pour les résistances électriques. A l'interface entre deux milieux, on trouve bien souvent un contact imparfait qu'on pourra modéliser par une résistance thermique supplémentaire mise en série avec les autres.

Toujours en régime permanent, l'intégrale de l'expression du transfert thermique convectif sur une paroi de surface $S$ prend naturellement la forme d'une loi d'Ohm :

\[
T_2-T_1 = \frac{\Phi}{hS}
\]

Les résistances thermiques de conduction et de convection peuvent cohabiter car elles se définissent à partir des mêmes grandeurs $T$ et $\Phi$.

On défini le \emph{Nombre de Biot} comme :

\[\boxed{
B = \frac{R_{cond}}{R_{conv}}
}\]

Si ce nombre est très éloigné de 1 on peut négliger l'une ou l'autre des différences de températures, et ne considérer qu'un seul type de transfert thermique.

\subsection{Capacité thermique}

\section{Rayonnement}

\subsection{Corps noir}

Il existe un troisième mode d'échange thermique qui caractérise le transfert d'énergie entre deux corps par véhiculé par un rayonnement électromagnétique. En effet, un résultat de mécanique quantique (le tout premier qui ait été découvert, en fait) est qu'un \emph{corps noir} à la température $T$ émet une densité surfacique de puissance sous la forme de photons de longueur d'onde $\lambda$ :

\[\boxed{
\varphi^0(\lambda, T) = \frac{2 \pi h c^2}{\lambda^5} \frac{1}{e^\frac{hc}{\lambda k_B T} - 1}
}\]

Dans le cadre du rayonnement, on rebaptise $\varphi$ l'\emph{émittance} et on la note usuellement $\mathcal{M}$. Le $0$ en exposant sert à indiquer qu'on se réfère au spectre du corps noir ; un corps noir étant un objet physique qui ne réfléchirait ni ne transmettrait la lumière d'aucune façon.

\begin{sagesilent}
x=var('x');
t = var('t');
x_coords = [t for t in srange(1,3000,10)];
y1_coords = [0.000000000001*3.7e-16/(t*10^(-9))^5/(e^(0.0144/(t*10^(-9)*2000))-1) for t in srange(1,3000,10)];
output = "";
y2_coords = [0.000000000001*3.7e-16/(t*10^(-9))^5/(e^(0.0144/(t*10^(-9)*2500))-1) for t in srange(1,3000,10)];
output = "";
y3_coords = [0.000000000001*3.7e-16/(t*10^(-9))^5/(e^(0.0144/(t*10^(-9)*3000))-1) for t in srange(1,3000,10)];
output = "";
y4_coords = [0.000000000001*3.7e-16/(t*10^(-9))^5/(e^(0.0144/(t*10^(-9)*4000))-1) for t in srange(1,3000,10)];
output = "";
y5_coords = [0.000000000001*3.7e-16/(t*10^(-9))^5/(e^(0.0144/(t*10^(-9)*5000))-1) for t in srange(1,3000,10)];
output = "";
for i in range(0,len(x_coords)-1):
    output += "\draw[red, thick] ("+str(x_coords[i])+","+str(y1_coords[i])+")--("+str(x_coords[i+1])+","+str(y1_coords[i+1])+");\n";
    output += "\draw[blue, thick] ("+str(x_coords[i])+","+str(y2_coords[i])+")--("+str(x_coords[i+1])+","+str(y2_coords[i+1])+");\n";
    output += "\draw[green, thick] ("+str(x_coords[i])+","+str(y3_coords[i])+")--("+str(x_coords[i+1])+","+str(y3_coords[i+1])+");\n";
    output += "\draw[orange, thick] ("+str(x_coords[i])+","+str(y4_coords[i])+")--("+str(x_coords[i+1])+","+str(y4_coords[i+1])+");\n";
    output += "\draw[purple, thick] ("+str(x_coords[i])+","+str(y5_coords[i])+")--("+str(x_coords[i+1])+","+str(y5_coords[i+1])+");\n";
\end{sagesilent}

\begin{center}
\begin{tikzpicture}[x=5*\textwidth/6/3000), y=\textwidth/3/50]
\pgfplotsset{width=15*\textwidth/16,height=5*\textheight/16,compat=newest}
	// Axes
	\draw[->] (0,0) -- (3100,0) node[right] {$\lambda \; [nm]$};
	\foreach \x in {0,500,...,3000}
     	\draw (\x,1) -- (\x,-1)
		node[anchor=north] {\x};
	\draw[->] (0,0) -- (0,50) node[above] {$\mathcal{M}^0(\lambda,T)$};
	\foreach \y in {0,10,...,40}
     	\draw (25,\y) -- (-25,\y)
		node[anchor=east] {\y};
	
	\draw (650,37) node[anchor=south west] {\textcolor{purple}{$T = 5000K$}};
	\draw (700,13) node[anchor=south] {\textcolor{orange}{$T = 4000K$}};
	\draw (850,2.5) node[anchor=south] {\textcolor{green}{$T = 3000K$}};
	\if\releaseversion1
	\sagestr{output}
	\fi
\end{tikzpicture}
\end{center}

\textcolor{red}{(modifier l'axe des ordonnés)}

Nous prendrons pour acquise cette distribution du \emph{spectre électromagnétique} du corps noir.

\subsubsection{Loi de Wien}

Un premier calcul qui peut-être mené consiste à chercher la longueur d'onde du maximum d'émission $\lambda_{max}$ pour une température donnée :

\[
\frac{\partial \mathcal{M}}{\partial \lambda}(\lambda_{max}, T) = 0
\]

\[
\left. \frac{\partial}{\partial \lambda}\frac{1}{\lambda^5\left(e^\frac{hc}{\lambda k_B T} - 1\right)}\right|_{\lambda = \lambda_{max}} = 0
\]

Le logiciel de calcul formel \emph{Sagemath} calcule la dérivée pour nous :

\[
-\frac{5}{\lambda_m^{6} {\left(e^\frac{hc}{\lambda_m k_B T} - 1\right)}} + \frac{\frac{hc}{k_B T}
e^\frac{hc}{\lambda_m k_B T}}{\lambda_m^{7} {\left(e^\frac{hc}{\lambda_m k_B T} - 1\right)}^{2}} = 0
\]

\[
-5\left(e^\frac{hc}{\lambda_m k_B T} - 1\right) + \frac{hc}{\lambda_m k_B T}e^\frac{hc}{\lambda_m k_B T} = 0
\]

\[
e^{-\frac{hc}{\lambda_m k_B T}} + \frac{hc}{5\lambda_m k_B T} - 1 = 0
\]

Equation non-linéaire, qu'on résoudra numériquement après avoir posé $x = \frac{hc}{\lambda_m k_B T}$ :

\[
e^{-x} + \frac{x}{5} - 1 = 0
\]

Sagemath (qui est évidemment un outil ridiculement puissant pour résoudre une équation aussi simple) retourne $x \sim 4,965$. On en déduit finalement la loi de Wien :

\[
\lambda_m T \sim \frac{hc}{4,965 k_B}
\]

\[\boxed{
\lambda_m T \sim 2889 K \mu m
}\]

\subsubsection{Loi de Stefan-Boltzmann}

On défini l'émittance totale en toute généralité par :

\[\boxed{
\mathcal{M} (T) = \int_0^\infty \mathcal{M} (\lambda, T) \; d\lambda
}\]

Qui n'est bien entendu autre que la puissance émise. Dans le cas d'un corps noir :

\[
\mathcal{M}^0 (T) = 2 \pi h c^2\int_0^\infty \frac{1}{\lambda^5 \left(e^\frac{hc}{\lambda k_B T} - 1\right)} \; d\lambda
\]

Que Sagemath calcule comme :

\[
\mathcal{M}^0 (T) = \frac{2 \pi ^5 k_B^4}{15 h^3 c^2} T^4
\]

Connue comme la \emph{loi de Stefan-Boltzmann} :

\[\boxed{
\mathcal{M}^0 (T) = \sigma T^4
}\]

Avec $\sigma = \frac{2 \pi ^5 k_B^4}{15 h^3 c^2} \sim 5.67 \; 10^{-8} W.m^{-2}.K^{-4}$ la constante de Stefan-Boltzmann.

La démonstration est laborieuse, mais on peut ajouter que $95\%$ de la puissance émise l'est entre $\frac{\lambda_m}{2}$ et $5\lambda_m$ :

\[\boxed{
F_{\frac{\lambda_{max}}{2}T \rightarrow 5\lambda_{max}T} = \frac{\int_\frac{\lambda_m}{2}^{5\lambda_{max}} \mathcal{M}^0 (\lambda, T) \; d\lambda}{\sigma T^4} = 0.95
}\]

\subsubsection{Absorption}

Pour achever cette discussion sur le corps noir, on précisera que celui-ci se comporte de façon tout à fait symétrique pour ce qui est de son émittance et de son absorbance. En effet, si on l'expose à un rayonnement de densité de flux monochromatique $\mathcal{H}(\lambda)$, le corps noir absorbera toute la puissance contenue dans le flux laquelle vaudra :

\[\boxed{
\alpha\mathcal{H} = \mathcal{H} = \int_0^\infty \mathcal{H}(\lambda) \; d\lambda
}\]

A l'équilibre thermique, on aura donc :

\[
\mathcal{H} = \mathcal{M}^0 (T)
\]

Pour chacune des surfaces noires en présence, ce qui doit permettre de déterminer leurs températures individuelles.

\subsection{Dépendance géométrique}

\subsubsection{Luminance}

Tout ce qui précède permet de calculer en tout point de la surface d'un corps noir la distribution spectrale d'émittance $\mathcal{M}^0 (\lambda,T)$ ; et pour une distribution spectrale de densité de flux reçu $\mathcal{H}(\lambda)$ donnée, d'en déduire la puissance captée $\mathcal{H}$ par le biais d'une simple intégration.

Ces densités surfaciques de puissance ne sont cependant pas émise de façon omnidirectionnelle. De fait, un calcul d'échange d'énergie entre plusieurs surfaces peut s'avérer complexe.

\textcolor{red}{(figure)}

L'idée étant de déterminer la densité de flux monochromatique $\mathcal{H}_i(\lambda)$ reçue par une surface identifiée par un indice $i$ de la part de n'importe quelle surface d'indice $j$ rayonnant une émittance monochromatique $\mathcal{M}_j (\lambda,T)$.

Autrement dit, on cherchera à remplir le tableau $f_i^j$ des \emph{facteurs de forme} émanant des dispositions géométriques des surface, de sorte qu'on puisse écrire :

\[\boxed{
\mathcal{H}_i = f_i^j \mathcal{M}_j
}\]

Pour ce faire, on usera du modèle de dépendance angulaire le plus simple et le plus commun : celui de la source \emph{Lambertienne} ; également qualifiée d'\emph{orthotrope}.

\textcolor{red}{(figure)}

L'introduction de considérations géométriques dans le rayonnement commence par dire qu'on localise chaque point de l'espace dans un système de coordonnées sphériques. Ce système de coordonnées est un peu particulier car - comme nous l'avons vu \textcolor{red}{(pas fait)} - sa métrique est telle que son angle solide élémentaire $d\Omega$ s'écrive :

\[
d\Omega = \sin(\theta)\;d\theta \; d\phi
\]

La relation de dépendance directionnelle de l'émittance est propre à la surface, donnant la densité directionnelle d'émittance par angle solide élémentaire :

\[
d\mathcal{M}(\theta,\phi,\lambda,T) = f(\lambda,T,\theta,\phi) \; d\Omega
\]

Une surface lambertienne est définie par sa relation particulière :

\[\boxed{
d\mathcal{M}(\theta,\phi,\lambda,T) = L(\lambda,T)\cos(\theta) \; d\Omega
}\]

Pour comprendre la simplicité qu'apporte l'étude de surfaces lambertiennes par rapport à d'autres surfaces, il nous faut introduire une nouvelle grandeur ; la luminance, ayant pour raison d'être le calcul du flux reçu par une autre surface du fait de l'émittance de $dS$ :

\textcolor{red}{(figure)}

\[\boxed{
\mathcal{L}(\theta,\phi,\lambda,T) = r^2\frac{d^2\Phi_{S \rightarrow S'}}{\vec{dS}\cdot \vec{dS'}}(\theta,\phi,\lambda,T)
}\]

Qui est homogène à une densité de flux, tout comme l'émittance. Le produit scalaire se calcul simplement comme :

\[
\mathcal{L}(\theta,\phi,\lambda,T) = \frac{r^2}{\cos{\theta}}\frac{d^2\Phi_{S \rightarrow S'}}{dS \; dS'}(\theta,\phi,\lambda,T)
\]

On reconnait l'émittance ainsi que l'angle solide : 

\[
\mathcal{L}(\theta,\phi,\lambda,T) = \frac{1}{\cos{\theta}}\frac{d\mathcal{M}}{d\Omega}(\theta,\phi,\lambda,T)
\]

Et pour une surface lambertienne :

\[
\mathcal{L}(\theta,\phi,\lambda,T) = L(\lambda,T)
\]

Ainsi, le flux transmit d'une surface lambertienne $S$ à une portion d'hémisphére $S'$ sera simplement :

\[
\Phi(\lambda,T) = \iint_{S'} \iint_{S} \frac{L(\lambda,T)}{r^2} \vec{dS}\cdot \; \vec{dS'}
\]

Qui reste une intégrale quadruple, donc pas si facile à calculer. Si $S'$ n'est pas une portion d'hémisphère, on l'assimilera à la portion d'hémisphère décrivant le même angle solide du point de vue de $dS$ :

\[
\Phi(\lambda,T) = \iint_{\Omega(S')} \iint_{S} L(\lambda,T) \cos(\theta)\; dS\; d\Omega
\]

\subsubsection{Loi de Lambert}

Dans le cas de surface lambertienne, il existe une relation simple entre la luminance $L(\lambda,T)$ et l'émittance $\mathcal{M}(\lambda,T)$. En effet, nous avions la dépendance directionnelle de l'émittance donnée par :

\[
d\mathcal{M}(\theta,\phi,\lambda,T) = L(\lambda,T)\cos(\theta) \; d\Omega
\]

De sorte qu'en intégrant sur l'hémisphère, laquelle décrit un angle solide valant $2\pi$ steradians \textcolor{red}{(angle solide non présenté auparavant)} :

\[
\mathcal{M}(\lambda,T) = \iint_{2\pi} L(\lambda,T)\cos(\theta) \; d\Omega
\]

En se souvenant de la forme que donne la métrique cylindrique à $d\Omega$, et en décrivant un découpage en anneaux partants du pôle \textcolor{red}{(figure)} :

\[
\mathcal{M}(\lambda,T) = \int_0^{\frac{\pi}{2}} 2\pi L(\lambda,T)\cos(\theta)\sin(\theta) \; d\theta \; d\phi
\]

\[
\mathcal{M}(\lambda,T) = 2\pi L(\lambda,T) \int_0^{\frac{\pi}{2}} \frac{\sin(2\theta)}{2} \; d\theta \; d\phi
\]

\[
\mathcal{M}(\lambda,T) = 2\pi L(\lambda,T) \frac{\cos(0)-\cos(\pi)}{4}
\]

\[\boxed{
\mathcal{M}(\lambda,T) = \pi L(\lambda,T)
}\]

Connue comme la \emph{loi de Lambert}.

\subsubsection{Facteurs de forme}

Considérons un ensemble de surfaces non-réfléchissantes désignées par leurs indices. Soit $\mathcal{H}_i(\lambda)$ la densité de flux monochromatique reçue par la surface d'indice $i$. Cette grandeur est reliée aux émittances $\mathcal{M}_j(\lambda,T)$ des autres surface à travers les facteurs de formes :

\[
\mathcal{H}_i = f_i^j \mathcal{M}_j
\]

\textcolor{red}{(démonstration)}

Une première propriété est que par défition de l'émittance :

\[
\mathcal{M}_i = f_i^j \mathcal{M}_i
\]

\textcolor{red}{(wtf les notations, à revoir)}

\[\boxed{
\forall j, \; \sum_i f_i^j = 1
}\]

Une seconde est qu'à l'équilibre thermique, on aura :

\[\boxed{
\forall (i,j), \; S_i f_i^j = S_j f_j^i
}\]

\subsection{Surfaces réelles}

\subsubsection{Emissivité}

Le modèle du corps noir ne s'applique pas aux surfaces réelles, où les imperfections (couche diélectrique de peinture, rugosité, etc...) viennent modifier le spectre d'émission.

En particulier, les milieux peuvent-être partiellement transparents et réflechissants. Pour formaliser ce comportement, on intruit l'\emph{émissivité monochromatique} $\epsilon(\lambda)$ supposée indépendante de la température et qui vient corriger le spectre du corps noir de sorte que :

\[\boxed{
\mathcal{M}(\lambda, T) = \epsilon(\lambda) \mathcal{M}^0 (\lambda, T)
}\]

Avec $\mathcal{M}^0$ la densité de flux du corps noir, préalablement présentée.

Si en outre $\epsilon$ ne dépend pas de $\lambda$, on dit que la surface est $grise$.

L'émissivité monochromatique permet de définir l'\emph{émissivité totale} $\epsilon$ qui est le rapport de la puissance émise par la puissance qu'aurait émis un corps noir :

\[
\mathcal{M}(T) = \epsilon \mathcal{M}^0 (T)
\]

\[
\epsilon = \frac{\int_0^\infty \mathcal{M}(\lambda, T) \; d\lambda}{\sigma T^4}
\]

\[
\epsilon = \frac{\int_0^\infty \epsilon(\lambda) \mathcal{M}^0 (\lambda, T) \; d\lambda}{\sigma T^4}
\]

\subsubsection{Absorbance}

L'absorbance est donnée par :

\[\boxed{
\alpha(\lambda) = \epsilon(\lambda)
}\]

\textcolor{red}{(démonstration, cf loi de Kirchhoff)}

Et l'absorbance totale :

\[\boxed{
\alpha = \frac{\int_0^\infty \alpha(\lambda) \mathcal{H} (\lambda, T) \; d\lambda}{\mathcal{H}} = \frac{\int_0^\infty \epsilon(\lambda) \mathcal{H} (\lambda, T) \; d\lambda}{\mathcal{H}}
}\]

On notera que pour un corps non-gris, absorbance et émissivité totales ne sont plus nécessairement égales puisque le spectre du flux incident n'est pas nécessairement celui du rayonnement de corps noir \textcolor{red}{(vérifier l'argument)}.

\subsubsection{Méthode des radiosités}

Soit $\mathcal{H}(\lambda)$ la densité de flux monochromatique reçue par une surface, alors la conservation de l'énergie se traduit par une relation entre $\alpha(\lambda)$ l'absorbance, $\rho(\lambda)$ la reflexivité et $\tau(\lambda)$ la transparence :

\[\boxed{
\alpha(\lambda) \mathcal{H}(\lambda) + \rho(\lambda) \mathcal{H}(\lambda) + \tau(\lambda) \mathcal{H}(\lambda) = \mathcal{H}(\lambda)
}\]

\[\boxed{
\alpha(\lambda) + \rho(\lambda) + \tau(\lambda) = 1
}\]

Pour un corps opaque par exemple, on aura $\rho(\lambda) = 1-\alpha(\lambda)$.

On défini maintenant la \emph{radiosité} comme étant la grandeure représentative du flux quittant le corps, qui n'est autre que la somme de l'émittance et du flux réfléchi :

\[\boxed{
\mathcal{R}(\lambda,T) = \mathcal{M}(\lambda,T)+\rho(\lambda)\mathcal{H}(\lambda)
}\]

C'est ainsi la radiosité qui apparaîtra dans la relation des facteurs de forme pour une surface réelle :

\[\boxed{
\mathcal{H}_i = f_i^j \mathcal{R}_j
}\]

\section{Thermique appliquée}

\subsection{Echangeur de chaleur}

Dans cet partie nous allons mettre en application les concepts qui viennent d'être présentés sur un système utilisé entre autres dans les centrales nucléaires, dans lesquelles le fluide radioactif du circuit primaire est en contact direct avec la matière en fission, et dont la chaleur doit donc passer dans un circuit secondaire sans échange de matière.

\textcolor{red}{(figure)}

Commençons par considérer une tranche de ce dispositif. Le fluide interne est à une température $T_0$. 

\textcolor{red}{(figure)}

On se plaçera en régime permanent, ce qui permet tout de suite de mettre en place un modèle électrique (linéique dans ce cas-ci) :

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
\centering
\draw node[rground]{} (0,0) to[V, v=$T_0$] ++(0,2) to[R, l=$\frac{1}{2\pi h_1 r_1}$] ++(2,0) to[R, l=$R_c$] ++(2,0) to[R, l=$\frac{1}{2\pi h_2 r_2}$] ++(2,0) to[R, l_=$R_i$, v^=$T_1$] ++(0,-2) node[rground]{};
\end{tikzpicture}
\end{center}
\shorthandon{:!}

En coordonées polaires, pour un système isotrope et en régime permanent, la loi de Fourier prend la forme :

\[
\frac{\partial}{\partial r}\left(r\frac{\partial T}{\partial r}\right) = 0
\]

Qu'on intègre dans le conducteur thermique :

\[
T = \int \frac{1}{r} \int 0 \; dr \; dr
\]

\[
T = \int \frac{A}{r} \; dr
\]

\[
T = A \ln(r) + B
\]

Les conditions de Dirichlet sont :

\[
\left\{ \begin{matrix}
T(r_1) = T_0 \\
T(r_2) = T_1
\end{matrix} \right.
\]

\[
\left( \begin{matrix}
\ln(r_1) & 1 \\
\ln(r_2) & 1
\end{matrix} \right)
\left( \begin{matrix}
A \\
B
\end{matrix} \right)
=
\left( \begin{matrix}
T_0 \\
T_1
\end{matrix} \right)
\]

Qu'on inverse en (calcul effectué sous \emph{sagemath}) :

\[
\left( \begin{matrix}
A \\
B
\end{matrix} \right)
=
\left( \begin{matrix}
-\frac{T_0-T_1}{\ln\left(\frac{r_2}{r_1}\right)} \\
\frac{T_1 \ln(r_1)-T_0 \ln(r_2)}{\ln\left(\frac{r_2}{r_1}\right)}
\end{matrix} \right)
\]

Ce qui permet d'écrire :

\[
T = -\frac{T_0-T_1}{\ln\left(\frac{r_2}{r_1}\right)} \ln(r) + \frac{T_1 \ln(r_1)-T_0 \ln(r_2)}{\ln\left(\frac{r_2}{r_1}\right)}
\]

Et le flux linéique :

\[
\Phi = 2\pi r ||\vec{\varphi(r_1)}||
\]

\[
\Phi = - 2\pi r \lambda \nabla T
\]

\[
\Phi = (T_0-T_1) \frac{2\pi \lambda}{\ln\left(\frac{r_2}{r_1}\right)}
\]

On trouve la résistance thermique linéique du conducteur:

\[
R_c = \frac{T_0-T_1}{\Phi}
\]

\[
R_c = \frac{\ln\left(\frac{r_2}{r_1}\right)}{2\pi \lambda}
\]

Ce raisonnement s'applique également à la détermination de $R_i$, que nous supposerons donc connu pour la suite.

\textcolor{red}{(Discuter des résistances convectives)}

Le modèle $2D$ qui vient d'être développé en régime permanent est transposable en $3D$ à un échangeur de chaleur linéaire dans lequel les fluides ne s'écouleraient pas ; ce qui a un intéret très limité.

Pour prendre en compte l'écoulement des fluides, il va être nécessaire de développer un modèle transitoire. Cela peut sembler inapproprié dans la mesure où l'on devine que les variations de température sont spatiales et non temporelles, mais l'idée consiste à suivre une tranche de fluide pendant son trajet.

Sur une tranche de l'échangeur d'épaisseur $dz$, on aura donc le bilan de puissance :

\[
\left\{ \begin{matrix}
\rho c_p S_0 \frac{dT_0}{dt} = - \frac{T_0-T_1}{R_i} \\
\rho c_p S_1 \frac{dT_1}{dt} = \frac{T_0-T_1}{R_i} - \frac{T_1-T_\infty}{R_e}
\end{matrix}\right.
\]

\[
\frac{d}{dt}\left(\begin{matrix} T_0 \\ T_1
\end{matrix}\right) - \frac{1}{\rho c_p} \left(\begin{matrix} -\frac{1}{S_0 R_i} & \frac{1}{ S_0 R_i} \\ \frac{1}{S_1 R_i} & -\left(\frac{1}{S_1 R_i}+\frac{1}{S_1 R_e}\right)
\end{matrix}\right)\left(\begin{matrix} T_0 \\ T_1
\end{matrix}\right) =
\frac{1}{\rho c_p}\left(\begin{matrix} 0 \\ \frac{T_\infty}{S_1 R_e}
\end{matrix}\right)
\]

Qui correspond en fait au circuit équivalent suivant :

\textcolor{red}{(circuit)}

Pour passer dans le domaine spatial, on impose les débits volumiques des fluides $Q_{0} = S_0\frac{dx_0}{dt}$ et $Q_{1} = S_1\frac{dx_1}{dt}$, auquel cas ce système différentiel devient :

\[
\left(\begin{matrix} \frac{dT_0}{dx_0}\\ \frac{dT_1}{dx_1}
\end{matrix}\right) - \frac{1}{\rho c_p} \left(\begin{matrix} -\frac{Q_0}{R_i} & \frac{Q_0}{R_i} \\ \frac{Q_1}{R_i} & -Q_1\left(\frac{1}{R_i}+\frac{1}{R_e}\right)
\end{matrix}\right)\left(\begin{matrix} T_0 \\ T_1
\end{matrix}\right) =
\frac{1}{\rho c_p}\left(\begin{matrix} 0 \\ \frac{Q_1 T_\infty}{R_e}
\end{matrix}\right)
\]

Pour se placer dans un système de coordonnées uniques, on remarque que 

\[
dx_1 = \frac{\partial x_1}{\partial x_0}dx_0
\]

\[
dx_1 = \frac{Q_1 S_0}{S_1 Q_0}dx_0
\]

Et donc :

\[
\frac{d}{dx_0}\left(\begin{matrix} T_0 \\ T_1
\end{matrix}\right) - \frac{1}{\rho c_p} \left(\begin{matrix} -\frac{Q_0}{R_i} & \frac{Q_0}{R_i} \\ \frac{Q_1^2 S_0}{S_1 Q_0 R_i} & -\frac{Q_1^2 S_0}{S_1 Q_0}\left(\frac{1}{R_i}+\frac{1}{R_e}\right)
\end{matrix}\right)\left(\begin{matrix} T_0 \\ T_1
\end{matrix}\right) =
\frac{1}{\rho c_p}\left(\begin{matrix} 0 \\ \frac{Q_1^2 S_0}{S_1 Q_0}\frac{T_\infty}{R_e}
\end{matrix}\right)
\]

\textcolor{red}{(résolution, puis discussion du contre-courant)}

\subsection{Ailette de refroidissement}

Une ailette est un dispositif thermique destiné à améliorer l'évacuation de chaleur de composants électroniques ou mécaniques.

On considère un cylindre (à base quelconque) conducteur thermique dont toutes les faces sont soumises à la convection (condition de Neumann) sauf une sur laquelle la température est fixée (condition de Dirichlet).

\textcolor{red}{(figure)}

Le modèle de l'ailette consiste à se placer en régime permanant, et surtout à négliger les variations transverses de température dans le conducteur thermique (là où seul la conduction opère). Cela permet de trouver une solution analytique comme nous allons le voir , mais n'est vrai que si le nombre de Biot est suffisament petit.

L'équation de la chaleur prend alors la forme :

\[
\frac{\partial^2 T}{\partial x^2} = 0
\]

Cependant, ce modèle n'est physiquement pas acceptable car il y une singularité au niveau des parois, où le flux convectif change de direction au contact du matériaux.

Il est cependant possible de s'en sortir en revenant un instant à la conservation de l'énergie tridimensionnelle (en régime permanent), pour appliquer le théorème de Green-Ostrogradski sur une petite couche de conducteur :

\[
\nabla \cdot \varphi = 0
\]

\[
\iiint_{\delta V} \nabla \cdot \varphi \; dV = 0
\]

\[
\iiint_{\delta V} \varphi \cdot \vec{dS} = 0
\]

Et à partir du dessin, si le contour de l'ailette est un lacet de longueur $l$, on se laissera convaincre que :

\[
\Phi(x+\delta x) - \Phi(x) + h (T - T_{\infty}) ldx  = 0
\]

Dont on trouve que :

\[
\frac{\partial \Phi}{\partial x} + hl(T - T_{\infty})  = 0
\]

D'après la loi de Fourier mono-dimensionnelle :

\[
- \lambda S \frac{\partial^2 T}{\partial x^2} + hl(T - T_{\infty})  = 0
\]

\[
\frac{\partial^2 T}{\partial x^2} - \frac{hl}{\lambda S}T  = - \frac{hl}{\lambda S}T_{\infty}
\]

Je vous laisse vérifier que cette équation - dite \emph{équation de l'ailette} a pour solution générale :

\[
T(x) = A e^{\sqrt{\frac{hl}{\lambda S}}x} + B e^{-\sqrt{\frac{hl}{\lambda S}}x} + T_{\infty}
\]

N'ayant pas encore fixé les constantes, on peut lui donner une forme hyperbolique :

\[
T(x) = A \cosh \left(\sqrt{\frac{hl}{\lambda S}}x \right) + B \sinh \left(\sqrt{\frac{hl}{\lambda S}}x \right) + T_{\infty}
\]

On peut maintenant utiliser les conditions aux limites restantes. Celle de Dirichelet en $x=0$ nous donne $A = T_0 - T_{\infty}$ tandis que $B$ peut-être trouvé du fait que :

\[
-\lambda \frac{\partial T}{\partial x}(L) = \vec{\varphi}(L) \cdot \vec{e_x} = h(T(L)-T_{\infty})
\]

Mais comme le calcul est pénible à mener, nous nous contenterons de supposer $L$ suffisament grand pour que le flux en bout d'ailette soit négligeable. L'expression précédente se réduit alors à :

\[
h(T(L)-T_{\infty}) = 0
\]

\[
(T_0-T_{\infty}) \cosh \left(\sqrt{\frac{hl}{\lambda S}}L \right) + B \sinh \left(\sqrt{\frac{hl}{\lambda S}}L \right) = 0
\]

\[
B = -(T_0-T_{\infty}) \coth \left(\sqrt{\frac{hl}{\lambda S}}L \right)
\]

Et la solution devient :

\[
T(x) = (T_0-T_{\infty}) \left( \cosh \left(\sqrt{\frac{hl}{\lambda S}}x \right) - \coth \left(\sqrt{\frac{hl}{\lambda S}}L \right) \sinh \left(\sqrt{\frac{hl}{\lambda S}}x \right) \right) + T_{\infty}
\]

Le flux dissipé se calcule en $x = 0$ :

\[
\Phi = S\varphi
\]

\[
\Phi = -S\lambda\frac{\partial T}{\partial x}(0)
\]

\[
\Phi = -S\lambda(T_0-T_{\infty})\sqrt{\frac{hl}{\lambda S}} \left( \sinh \left(\sqrt{\frac{hl}{\lambda S}}0 \right) - \coth \left(\sqrt{\frac{hl}{\lambda S}}L \right) \cosh \left(\sqrt{\frac{hl}{\lambda S}}0 \right) \right)
\]

\[
\Phi = (T_0-T_{\infty})\sqrt{S\lambda hl}\coth \left(\sqrt{\frac{hl}{\lambda S}}L \right)
\]

Ce qui permet finalement de trouver la résistance thermique :

\[
R_{th} = \frac{T_0-T_{\infty}}{\Phi}
\]

\[
R_{th} = \frac{\tanh \left(\sqrt{\frac{hl}{\lambda S}}L \right)}{\sqrt{S\lambda hl}}
\]

\textcolor{red}{(on doit trouver une $\coth$...)}

Une ailette bien conçue est une ailette dont la tangente hyperbolique est tout juste proche de $1$ (sa valeur maximale), auquel cas :

\[
R_{th} = \frac{1}{\sqrt{S\lambda hl}}
\]

A comparer avec la résistance thermique purement convective d'une surface d'aire $S$ :

\[
R_{th} = \frac{1}{hS}
\]

\subsection{Méthode Flash}

La méthode flash permet de mesurer la conductivité thermique d'un matériaux. L'idée est d'en prendre un cylindre, d'éclairer l'une de ses faces par un rayonnement bref mais puissant et de regarder la réponse de la température sur l'autre face.

Soit un cylindre soumis sur ses deux bases à la convection, et dont l'une de ses deux bases reçoit le rayonnement $\Phi_s(t) = Q \delta(t)$

Les conditions aux limites s'écrivent :

\begin{itemize}
\item La condition initiale : $\forall \vec{x} \in \Omega, T(\vec{x}, 0) = T_\infty$
\item Les surfaces latérales $\partial \Omega_l$ ainsi que la surface haute $\partial \Omega_h$ sont isolées : $\forall \vec{x} \in \partial \Omega_l \cup \partial \Omega_h, \nabla T \cdot \vec{n} = 0$
\item La surface basse $\partial \Omega_b$ reçoit le rayonnement : $\forall \vec{x} \in \partial \Omega_b, -\lambda \frac{\partial T}{\partial x} = \vec{\varphi_s} \cdot \vec{n} = -\varphi_s(t)$
\end{itemize}

L'isolation de la surface latérale permet d'approximer le problême comme mono-dimensionnel. L'équation de la chaleur s'écrit donc (en introduisant la diffusivité $a = \frac{\lambda}{\rho c_P}$) :

\[
\frac{1}{a}\frac{\partial T}{\partial t} = \frac{\partial^2 T}{\partial x^2}
\]

L'impulsion initiale contient une énergie $Q$ qui va venir immédiatement prendre la forme d'une augmentation de température dans la couche infinitésimale de la surface exposée :

\[
T(0,0) - T_\infty = \frac{Q}{\rho c} 
\]

Toutes les grandeurs étant exprimées de façon mono-dimensionnelles.

Une fois ce second membre sur condition de Neumann levée le problême devient :

\[ 
\left\{ \begin{matrix}  
\frac{1}{a}\frac{\partial T}{\partial t} = \frac{\partial^2 T}{\partial x^2} \\
\forall \vec{x} \in \partial \Omega_b, T(\vec{x}, 0) = T_\infty + \frac{Q}{\rho c_P}\delta(x)dx \\
\forall \vec{x} \in \partial \Omega, \frac{\partial T}{\partial x} = 0
\end{matrix} \right.
\]

La condition de Neumann étant sans second membre, on peut utiliser la méthode de séparation des variables. Ainsi, on postule que $T$ est le produit d'une fonction temporelle par une fonction spatiale :

\[ 
T(x,t) = \Gamma(t)\Psi(x)
\]

L'équation de la chaleur prend la forme :

\[ 
\frac{1}{a}\frac{1}{\Gamma}\frac{\partial \Gamma}{\partial t} = \frac{1}{\Psi}\frac{\partial^2 \Psi}{\partial x^2}
\]

On pose $-\alpha^2 = \frac{1}{a}\frac{1}{\Gamma}\frac{\partial \Gamma}{\partial t} = \frac{1}{\Psi}\frac{\partial^2 \Psi}{\partial x^2}$ :

\[ 
\frac{\partial \Gamma}{\partial t} + a\alpha^2\Gamma = 0
\]

\[ 
\Gamma(t) = \lambda e^{-a\alpha^2 t}
\]

De la même façon :

\[ 
\frac{\partial^2 \Psi}{\partial x^2} + \alpha^2\Psi = 0
\]

\[ 
\Psi(x) = \mu e^{j\alpha x} + \nu e^{-j\alpha x}
\]

Le changement de base $\mu = \frac{A}{2}+\frac{B}{2j}$ et $\nu = \frac{A}{2}-\frac{B}{2j}$ permet de se limiter à l'axe des réels, seul ayant un sens physique :

\[ 
\Psi(x) = \left(\frac{A}{2}+\frac{B}{2j}\right) e^{j\alpha x} + \left(\frac{A}{2}-\frac{B}{2j}\right) e^{-j\alpha x}
\]

\[ 
\Psi(x) = A\cos(\alpha x) + B\sin(\alpha x)
\]

La nullité de la condition de Neumann du nouveau problême sur la face inférieure impose $B = 0$ :

\[ 
\Psi(x) = A\cos(\alpha x)
\]

Et donc (en unifiant les constantes) :

\[ 
T(x,t) = \lambda \cos(\alpha x)e^{-a\alpha^2 t}
\]

On détermine le lambda par le biais de la condition de Dirichlet en $x = 0$ :

\[ 
T(0,0) = \lambda \cos(0)e^{0}
\]

\[ 
\lambda = T_\infty + \frac{Q}{\rho c_P}\delta(0)dx
\]

\[ 
\lambda = T_\infty + \frac{Q}{\rho c_P}
\]

\[ 
T(x,t) = (T_\infty + \frac{Q}{\rho c_P})\cos(\alpha x)e^{-a\alpha^2 t}
\]

Reste la dernière constante $\alpha$ à identifier, par le biais de la condition de Neumann sur la face haute, c'est-à-dire en $x = L$ :

\[ 
\frac{\partial T}{\partial x}(L,t) = 0
\]

\[ 
- (T_\infty + \frac{Q}{\rho c_P}) \alpha \sin(\alpha L)e^{-a\alpha^2 t} = 0
\]

Parce-que l'exponentielle est définie positive :

\[ 
\sin(\alpha L) = 0
\]

Qui laisse une infinité de valeurs possibles pour $\alpha$ :

\[ 
\alpha_k = \frac{\pi k}{L}
\]

Chacune de ces valeurs est associée à une solution possible du problême qu'on s'est posé. L'équation de Fourier étant linéaire, toute combinaison linéaire de celles-ci est une solution :

\[ 
T(x,t) = \sum_{k \in \mathrm{N}} \lambda_k \cos(\alpha_k x)e^{-a\alpha_k^2 t}
\]

Avec la condition que :

\[ 
T(x,t) = \sum_{k \in \mathrm{N}} \lambda_k = T_\infty + \frac{Q}{\rho c_P}
\]

Nous venons de passer d'une seule constante à une infinité... Mais c'est parcequ'il reste une condition aux limites ! Celle qui défini l'état initiale n'a été que partiellement utilisé, notre solution ignore encore que partout ailleurs que sur la face inférieure, $T = T_\infty$ :

\[
T(\vec{x}, 0) = \sum_{k \in \mathrm{N}} \lambda_k \cos(\alpha_k x)
\]

\[
T_\infty + \frac{Q}{\rho c_P}\delta(x)dx = \sum_{k \in \mathrm{N}} \lambda_k \cos(\alpha_k x)
\]

On a déjà inévitablement $\lambda_0 = T_\infty$ :

\[
\frac{Q}{\rho c_P}\delta(x)dx = \sum_{k \in \mathrm{N}/{0}} \lambda_k \cos(\alpha_k x)
\]

C'est alors qu'on meure d'envie de s'essayer à une décomposition au sens des distributions :

\[
\braket{\sum_{i \in \mathrm{N}/{0}} \lambda_i \cos(\alpha_i x) | \cos(\alpha_j x)} = \braket{\frac{Q}{\rho c_P}\delta(x)dx | \cos(\alpha_j x)}
\]

Commençons par ne toucher qu'au terme de gauche :

\[
\sum_{i \in \mathrm{N}/{0}} \lambda_i \braket{\cos(\alpha_i x) | \cos(\alpha_j x)} = \braket{\frac{Q}{\rho c_P}\delta(x)dx | \cos(\alpha_j x)}
\]

\textcolor{red}{(démontrer l'orthogonalité)}

\[
\delta_j^i \lambda_i = \braket{\frac{Q}{\rho c_P}\delta(x)dx | \cos(\alpha_j x)}
\]

\[
\lambda_j = \braket{\frac{Q}{\rho c_P}\delta(x)dx | \cos(\alpha_j x)}
\]

Attaquons-nous maintenant au terme de droite :

\[
\lambda_j = \int_\mathbb{R} \frac{Q}{\rho c_P}\delta(x)dx \cos(\alpha_j x) dx
\]

\textcolor{red}{(..., obtenir le résultat qui apparaît dans l'énoncé de TP thermique)}

\subsection{Capteur thermique}

\textcolor{red}{(cf TD3 rayonnement)}

\chapter{Thermodynamique}

\chapter{Semi-conducteurs}

\section{Théorie générale}

\subsection{Statistique de Fermi-Dirac}

Dans toute notre discussion, on considèrera que le terme d'\emph{équilibre thermodynamique} désigne la situation où le régime permanant est atteint, mais où le système est également isolé et qu'aucun champ électrique externe ne s'applique à lui. Ces hypothèses induisent l'électroneutralité locale si le semi-conducteur auquel on s'intéresse est homogène.

Un résultat de la physique quantique - dans l'approximation classique et à l'équilibre thermodynamique - est donné par la \emph{statistique de Fermi-Dirac} :

\[\boxed{
\mathbb{P}(E)\;dE = \frac{1}{e^{\frac{ E - E_f }{ k_{B} T }} + 1 }
}\]

Qui décrit la distribution des niveaux d'énergie des électrons dans un semi-conducteur à une température $T$. $E_f$ est l'\emph{énergie de Fermi} du matériau, celle à laquelle se regroupent tous les électrons à $T=0K$.

\textcolor{red}{(figure)}

Un autre résultat de la mécanique quantique est que dans un semi-conducteur, un électron peut échapper au puit de potentiel du noyau seulement au delà d'une énergie $E_c$ (qui est proche de $0$ pour un conducteur, et très grand pour un isolant) ; et on parlera alors d'\emph{électron libre}.

Leur densité volumique dans un semi-conducteur est donnée par :

\[\boxed{
n_e = \int_{E_c}^\infty g(E) \mathbb{P}(E) dE
}\]

Avec $g(E)$ la densité volumique d'électrons à l'énergie $E$, à laquelle la physique quantique attribue une dépendance en $\sqrt{E}$. On posera donc :

\[
n_e = \int_{E_c}^\infty \frac{A\sqrt{E}}{e^{\frac{E-E_f }{k_{B}T}}+1 } dE
\]

Pour laquelle on pourrait trouver une expression analytique trop générale vis-à-vis de nos besoins pratiques. En effet, l'\emph{approximation de Boltzmann} s'applique aux électrons où $Ec-2E_F \gg k_B T$ - tous dans les milieux avec lesquels on travaille - et consiste supposer l'exponentielle grande devant 1 :

\[
n_e = \int_{E_c}^\infty A\sqrt{E} e^{-\frac{E-E_f }{k_{B}T}} dE
\]

\[
n_e =  Ae^{\frac{E_f}{k_{B}T}} \int_{E_c}^\infty\sqrt{E} e^{\frac{-E}{k_{B}T}} dE
\]

Qu'on intègre par le changement de variable $u = \frac{E}{k_{B}T}$ et:

\[
n_e =  Ae^{\frac{E_f}{k_{B}T}}\sqrt{k_{B}T} \int_{\frac{E_c}{k_{B}T}}^\infty\sqrt{u} e^u dE
\]

Qu'un étrange résultat sur les intégrales de Lebesgue \textcolor{red}{(pas parlé avant)} que je n'ai pas la moindre envie d'aller consulter nous dit égal à :

\[
n_e =  Ae^{\frac{E_f}{k_{B}T}}\sqrt{k_{B}T} \frac{\sqrt{\pi}}{2}e^{\frac{-Ec }{k_{B}T}}
\]

Ou encore, en faisant rentrer les constantes dans la constante :

\[
n_e =  Ae^{-\frac{Ec-E_f}{k_{B}T}}
\]

De la même façon, on peut calculer la densité volumique d'absence d'électron sur la \emph{bande de Valence} (les niveaux d'énergies liès à l'atome, par opposition à la \emph{bande de conduction}) aussi appelés \emph{trous} :

\[
n_h = \int_0^{E_v} g(E) \mathbb{P}(E) dE
\]

\[
n_h =  Be^{-\frac{E_f-E_v}{k_{B}T}}
\]

Avec $E_v$ la borne supérieure de la bande de Valence.

Le résultat important est le suivant, et prend le nom de \emph{loi d'action de masse}. Quelque soit l'énergie de Fermi du semi-conducteur, qu'il soit \emph{dopé} ou non, à l'équilibre thermodynamique (condition nécessaire pour que la statistique de Fermi-Dirac s'applique) on aura :

\[\boxed{
n_e n_h =  ABe^{-\frac{Ec-E_v}{k_{B}T}}
}\]

\subsection{Régime quasi-statique}

Hors équilibre thermodynamique, la loi d'action de masse est prise en défaut. Les transitoires dans les semi-conducteurs sont gérés par le modèle de Drude présenté dans la section sur la magnétodynamique \textcolor{red}{(fait mais Drude ?)}.

Cette section est donc dédiée à sa version semi-conductrice.

Nous partons d'une relation fondamentale :

\[
\vec{E} = -\frac{\partial\vec{A}}{\partial t} - \nabla\phi
\]

En régime quasi-statique, on considère des transitoires tout en négligeant les dérivées temporelles dans les équations :

\[
\vec{E} = - \nabla\phi
\]

Le comportement électrique particulier des semi-conducteurs nous amène à distinguer deux conductivités, une pour chaque type de porteur :

\[
\left\{\begin{matrix}
\vec{j_h} = \sigma_h \vec{E}\\
\vec{j_e} = \sigma_e \vec{E}
\end{matrix}\right.
\]

Celle des électrons étant plus importante que celle des trous dans les matériaux car il faut moins d'énergie pour déplacer un électron d'un atome à son voisin s'il est préalablement libre que s'il ne l'est pas.

\textcolor{red}{(figure, illustrer un déplacement de trou et un déplacement d'électron)}

En outre, dans le matériau lui-même la température est source de diffusion. Les expressions prenant en compte celle-ci deviennent :

\[\boxed{
\left\{\begin{matrix}
\vec{j_h} = \sigma_h \vec{E} - q_+ D_h \nabla n_h\\
\vec{j_e} = \sigma_e \vec{E} + q_+ D_e \nabla n_e
\end{matrix}\right.
}\]

A noter que la question du signe peut se poser au passage du courant au sens de la thermique au courant au sens de l'électricité. Les expressions ci-dessus sont électriques, et les électrons se déplacent dans le sens opposé à celui pointé par $\vec{j_e}$ du fait de leur charge négative.

Le $\vec{j}$ résultant s'écrira donc :

\[
\vec{j} = (\sigma_h + \sigma_e) \vec{E} - q_+ D_h \nabla n_h + q_+ D_e \nabla n_e + \frac{\partial \vec{D}}{\partial t}
\]

En introduisante les densités de charge partielle :

\[\boxed{
\vec{j} = (\sigma_h + \sigma_e) \vec{E} - D_h \nabla \rho_h - D_e \nabla \rho_e + \frac{\partial \vec{D}}{\partial t}
}\]

Le terme propagatif n'est pas à négliger pour qui s'intéresse à la conception des semi-conducteurs \textcolor{red}{(mais doit-il apparaître explicitement dans $\vec{j}$ ? Probablement pas)}, car il est à l'origine de tout un pan de la discipline ayant trait à la compatibilité électromagnétique qui théorise et tente de proposer des solutions pour contenir les perturbations électromagnétiques en électronique de puissance.

Nous le négligerons néanmoins pour le moment.

La conservation de la charge électrique s'écrit :

\[
\frac{\partial \rho}{\partial t} + \nabla \cdot \vec{j} = 0
\]

Mais envisager les strictes analogues de cette équation pour les électrons et les trous reviendrait à ignorer le phénomène de \emph{recombinaison-dissociation} qui résulte des annhilations entre les électrons libres et les trous. En introduisant les \emph{taux nets} notés $u_e$ et $u_h$ :

\[
\left\{\begin{matrix}
\frac{\partial \rho_h}{\partial t} + \nabla \cdot \vec{j_h} + q_+ u_h = 0 \\
\frac{\partial \rho_e}{\partial t} + \nabla \cdot \vec{j_e} + q_+ u_e = 0
\end{matrix}\right.
\]

\textcolor{red}{(cf. version papier de janvier 2017)}

Pour les régions désertes ($n_h = n_e = 0$), les seules causes de recombinaison-dissociation sont :

\begin{itemize}
\item La thermique : \textcolor{red}{(compléter)}
\item Le phénomène d'avalanche : \textcolor{red}{(compléter)}
\end{itemize}

Pour aboutir aux équations :

\[
\frac{\partial \rho_h}{\partial t} + \nabla \cdot \vec{j_h} = \frac{\partial \rho_e}{\partial t} + \nabla \cdot \vec{j_e} = q_+ k_B n_i^2 + \alpha_h ||\vec{j_h}|| + \alpha_e ||\vec{j_e}||
\]

Pour les régions quasi-neutres, on mettra en place un modèle perturbatif linéaire de recombinaison-dissociation :

\[
\left\{\begin{matrix}
\frac{\partial \rho_h}{\partial t} + \nabla \cdot \vec{j_h} + \frac{q_+}{\tau_h} \tilde{u}_h = 0 \\
\frac{\partial \rho_e}{\partial t} + \nabla \cdot \vec{j_e} + \frac{q_+}{\tau_e} \tilde{u}_e = 0
\end{matrix}\right.
\]

Evidemment, l'étape suivante est de mettre en place des pseudo-équations de Fourier à partir des expressions de $\vec{j_h}$ et $\vec{j_e}$, mais nous avons déjà un pied hors du cadre des régimes quasi-statiques en laissant apparaître les $\frac{\partial \rho}{\partial t}$ (l'important pour moi était d'évoquer les recombinaisons-dissociations) donc gardons ça pour plus tard \textcolor{red}{(quand ? La CEM ?)}.

\subsection{Dopage}

En temps normal, un semi-conducteur est un réseau cristallin d'un seul atome. On parle de \emph{semi-conducteur intrinsèque}. Il y a alors la même densité d'électrons libres que de trous, et :

\[\boxed{
n_e = n_h =  \sqrt{AB}e^{-\frac{Ec-E_v}{2k_{B}T}}
}\]

Il faut savoir que dans cette situation et dans la plupart des semi-conducteurs - le Silicium en premier lieu - le \emph{gap} d'énergie $Ec-E_v$ est si grand que le matériau a le comportement d'un isolant à température ambiante.

L'intéret des semi-conducteur réside dans la technique de \emph{dopage} qui consiste à adjoindre au réseau cristallin monoatomique des atomes supplémentaires, voisins dans le tableau périodique \textcolor{red}{(pas parlé avant)}. Cela revient à introduire un déficit de protons mais surtout d'électrons (pour le dopage $P$, par exemple le Silicium-Bore) ou des protons et électrons excédentaire (pour le dopage $N$, par exemple le Silicium-Phosphore).

Cette altération va amener à un déséquilibre dans le comportement électrique du semi-conducteur en jouant sur son gap \textcolor{red}{(sur le niveau de Fermi plutôt)}. Le dopage $P$ va créer des trous dans le cristal et on aura $n_h\gg n_e$, tandis que les électrons supplémentaires du dopage $N$ feront que $n_e\gg n_h$.

Si on appelle $N = N_p - N_n$ la densité volumique algébrique de protons excédentaires (négative en cas de défaut de protons), l'électroneutralité (seulement en l'absence de champ électrique et pour un dopage homogène) s'écrira :

\[\boxed{
n_e - n_h = N
}\]

\textcolor{red}{(expliquer)}

Qui revient simplement à dire qu'un ion dopé - bien qu'ayant perdu un électron - n'est pas un trou dans le contexte du réseau cristallin puisqu'il s'y intègre parfaitement du point de vue électronique.

La relation $n_e = n_h$ n'est donc plus vraie dans un semi-conducteur extrinsèque (dopé), mais la loi d'action de masse le reste néanmoins à l'équilibre thermodynamique. Si on désigne par $n_i = \sqrt{AB}e^{-\frac{Ec-E_v}{2k_{B}T}}$ la \emph{concentration intrinsèque} (celle qu'auraient chacun des porteurs s'il n'était pas dopé), on écrira :

\[\boxed{
n_e n_h = n_i^2
}\]

En fait, le \emph{potentiel thermodynamique} qui apparaissait tout à l'heure dans les formules \textcolor{red}{($\mu$ ou $\psi$ ?)} est relié à $N$ \textcolor{red}{(démonstration ? lien avec les niveaux de fermi)} par :

\[\boxed{
\psi=\frac{kT}{q_+}\asinh \frac{N}{2n_i}
}\]

\textcolor{red}{($E_f$ ?)}

Avec $q_+$ la charge du proton. Il est positif pour un dopage $N$ et négatif pour $P$ ; et nul pour un semi-conducteur intrinsèque.

On conclu de ce qui précède les relations :

\[
\left\{\begin{matrix}
n_h^2+Nn_h-n_i^2 = 0 \\
n_e^2-Nn_e-n_i^2 = 0
\end{matrix}\right.
\]

Dont les racines sont :

\[\boxed{
\left\{\begin{matrix}
n_h = \frac{N+\sqrt{N^2+4n_i^2}}{2} \\
n_e = \frac{-N+\sqrt{N^2+4n_i^2}}{2}
\end{matrix}\right.
}\]

Si le dopage est suffisament \emph{fort} et si l'on désigne par $n_M$ et $n_m$ les concentrations des \emph{porteurs majoritaire} ($n_e$ pour un dopage $N$ et $n_p$ pour un dopage $P$) et \emph{minoritaires}, on peut négliger $n_m$ devant $n_M$ et l'électronégativité devient :

\[
n_M = \pm N
\]

Selon le signe du dopage. Dont découle :

\[
\left\{\begin{matrix}
n_h = \frac{\pm n_M+\sqrt{n_M^2+4n_i^2}}{2} \\
n_e = \frac{- (\pm n_M)+\sqrt{n_M^2+4n_i^2}}{2}
\end{matrix}\right.
\]

Par la loi d'action de masse :

\[
\left\{\begin{matrix}
n_h = \frac{\pm n_M+\sqrt{n_M(n_M+4n_m)}}{2} \\
n_e = \frac{-(\pm n_M)+\sqrt{n_M(n_M+4n_m)}}{2}
\end{matrix}\right.
\]

Et toujours parceque l'on néglige les concentrations en porteurs minoritaires :

\[
\left\{\begin{matrix}
n_h = \frac{(1\pm 1)}{2}n_M \\
n_e = \frac{(1-(\pm 1))}{2}n_M
\end{matrix}\right.
\]

Cependant, ce dernier résultat viole la loi d'action de masse puisque la concentration du porteur minoritaire est nulle. On recollera donc les morceaux en écrivant simplement :

\[
\left\{\begin{matrix}
n_M = n_M \\
n_m = \frac{n_i^2}{n_M}
\end{matrix}\right.
\]

La première ligne est une tautologie, et permet simplement de vérifier le cohérence de ce qui précède, tandis la seconde constitue un résultat important sur $n_m$ approximativement valable uniquement à l'équilibre et pour un semi-conducteur fortement dopé, puisqu'on en déduit d'après notre hypothèse initiale :

\[\boxed{
\left\{\begin{matrix}
n_M = N \\
n_m = \frac{n_i^2}{N}
\end{matrix}\right.
}\]

\subsubsection{Jonction PN à l'équilibre thermodynamique}

Une jonction est la surface à l'interface entre deux semi-conducteurs.

Dans le cas de la jonction $PN$, on a affaire à deux semi-conducteurs de même nature mais de dopages opposés et qu'on supposera fortement dopés :

\textcolor{red}{(figure)}

Se passe alors la chose suivante (accrochez-vous) :

\begin{itemize}
\item Les \emph{porteurs majoritaires} (les trous pour $P$ et les électrons pour $N$) dans chacun des semi-conducteurs vont migrer à travers la jonction par diffusion (les ordres de grandeurs sont tels qu'on peut généralement négliger l'influence des porteurs minoritaires en premier lieu).
\textcolor{red}{(figure)}
\item A proximité de la jonction, les porteurs majoritaires des deux types devraient cohabiter, ce qui est impossible à cause des recombinaisons. Cette zone est donc déserte en trous tout comme en électrons libres.
\textcolor{red}{(figure)}
\item Cette déplétion viole l'électroneutralité locale, autrement dit les charges transportées sont la source d'un désequilibre dans la répartition de la charge.
\textcolor{red}{(figure)}
\item Ce qui est la source d'un champ électrique venant s'opposer à la diffusion des porteurs majoritaires et entretenir le déplacement des porteurs minoritaires, lesquels deviennent par ailleurs des porteurs majoritaires au passage de la jonction (qui est facilité par la zone de déplétion) :
\textcolor{red}{(figure)}
\item L'équilibre n'est réellement atteint que quand ces courants se compensent, de sorte que $\vec{j} = 0$
\end{itemize}

Plutôt que de s'essayer à mettre cela en équation, partons de la fin de l'histoire. On écrit à l'équilibre thermodynamique :

\[
\vec{j} = 0
\]

Dans ce qui suit, nous allons négliger les courants de trous qui contribuent peu à $\vec{j}$ du fait de la plus faible mobilité de ceux-ci :

\[
\sigma \vec{E} - q_+ D \nabla n_e = 0
\]

Dont on extrait :

\[
\vec{E} = \frac{q_+ D}{\sigma} \nabla n_e
\]

Mais il faut savoir que la conductivité a une dépendance linéaire avec $n$ \textcolor{red}{(pourquoi ?)}. En introduisant la \emph{mobilité} $\mu = \frac{\sigma}{q_+ n}$ \textcolor{red}{(quid de la relation d'Einstein ? Expliquer)} :

\[
\vec{E} = \frac{D}{\mu} \frac{\nabla n_e}{n_e}
\]

Ce qui permet de calculer la tension aux bornes du composant faisant intervenir la jonction ; à l'équilibre toujours :

\[
U = \int \vec{E} \cdot \vec{dl} = \frac{D}{\mu} \int \frac{\nabla n_e}{n_e} \cdot \vec{dl}
\]

En prenant des bornes de part et d'autre de la jonction et suffisament éloignées :

\[
U = \frac{D}{\mu} \ln\frac{N_n}{\frac{n_i^2}{N_p}}
\]

Le terme au numérateur étant la concentration en porteurs majoritaires dans la zone $N$, et le dénominateur celle des porteurs minoritaires dans la zone $P$. Finalement :

\[\boxed{
U = \frac{D}{\mu} \ln\frac{N_nN_p}{n_i^2}
}\]

\textcolor{red}{(faire le lien avec la constance de l'énergie de Fermi et l'homogénéité de l'énergie à l'équilibre thermodynamique)}

Si vous faites l'application numérique avec les valeurs standards de dopage du Silicium, vous trouverez $U\sim 0.7 V$ ce qui parlera certainement aux électroniciens qui penseront immédiatement à la Diode.

\section{Composants semi-conducteurs}

Les semi-conducteurs sont utilisés pour remplir certaines fonctions élémentaires de l'électronique de commande et de puissance, notamment celles d'interrupteur, de diode et de thyristor.

Parmi celles-ci, la fonction d'interrupteur est essentielle puisqu'elle permet de faire le lien entre information et électricité, et donc de réaliser les meilleurs technologies de traitement de l'information qu'on ait à notre disposition, en attendant l'ordinateur quantique.

\subsection{Diode}

\subsubsection{Jonction PN hors équilibre}

Cette section est la suite directe de la précédente, mais ayant déterminé cette tension résiduelle à l'équilibre thermodynamique nous allons nous intéresser aux autres comportements statiques et dynamiques qui peuvent apparaître dans une diode. 

Evidemment, nous ne saurons aller très loin dans le détail de la physique sous-tendant les effets observés car vous réalisez sûrement que celle-ci est particulièrement complexe, mais l'ingénieur qui souhaite faire de l'implantation sur puce (auquel cas ce n'est pas le dernier des crétins) ou de l'électronique de puissance voudra savoir comment s'en servir.

Nous allons néanmoins continuer un peu de manipuler la physique sur l'exemple de la diode, quand une tension lui est appliquée par une source extérieur (donc quand le champ $\vec{E}$ n'est plus seulement d'origine thermodynamique).

\subsection{Transistor bipolaire}

\subsection{Transistor à effet de champ}

\subsection{Architectures composites}

\subsubsection{Thyristor}

\subsubsection{IGBT}

\part{Traitement de l'information}
\setcounter{chapter}{0}

\chapter{Electronique numérique}

\section{Portes logiques et bascules}

\section{Calcul numérique}

\section{Modulation en largeur d'impulsion}

\textcolor{red}{(+ Single vector modulation)}

\chapter{Analyse numérique}

\section{Calcul numérique sur les espaces vectoriels}

\subsection{Résolution numérique des systèmes linéaires}

\subsection{Diagonalisation numérique}

\section{Calcul numérique sur les espaces fonctionnels}

\subsection{Résolution numérique des équations algébriques}

En passant simplement le second membre à gauche, toute équation peut-être ramenée à une équation de la forme :

\[\boxed{
f(x) = 0
}\]

Le problême de cette section étant la recherche des valeurs de $x$ qui satisfont - si elles existent - cette équation.

Par exemple, quand nous avons présenté la loi de Wien en rayonnement thermique, une équation non-linéaire a émergée :

\[
e^{-x} + \frac{x}{5} - 1 = 0
\]

Equation qui n'a pas de solution analytique. Les techniques qui vont être présentées maintenant sont celles qui permettent aux programmes informatiques d'en trouver toutefois la solution.

La difficulté toute particulière de ce problême dans le cas général est la possible non-unicité de la solution. Si $f$ a plusieurs racines et qu'on souhaite les trouver toutes, le problême doit-être considéré de façon globale ; ce qu'on ne sait que très approximativement faire, même avec un ordinateur.

\subsection{Méthode du point fixe}

On cherche les racines d'une fonction continue $f$.

On considère la méthode itérative avec $x_{n+1} = x_n-f(x_n)$. Si la fonction $g : x \rightarrow x-f(x)$ est Lipzitchienne \textcolor{red}{(pas parle avant)} avec $k<1$, on a le théorème du point fixe qui stipule que cette méthode converge vers une racine de $f$.

Notamment, si $g$ est à dérivée comprise entre $0$ et $1$, on peut utiliser le \emph{théorème des valeurs intermédiaires} \textcolor{red}{(pas parlé avant)} pour écrire que :

\[
\exists c : f'(c)\leq \frac{g(b)-g(a)}{b-a}
\]

Et l'hypothèse principale est vérifiée.

\textcolor{red}{(compléter)} 

\subsection{Dérivation numérique}

La dérivation numérique est la pierre angulaire de la théorie de l'optimisation que nous étudierons prochainement, puisque la recherche d'extremum est équivalente à la recherche de l'annulation de la dérivée.

On peut donc penser que l'optimisation d'une fonction se réduit au couplage de la résolution numérique d'équation algébriques à une méthode de dérivation numérique, et c'est effectivement le cas en optimisation statique sans contrainte, mais nous verrons que la théorie de l'optimisation couvre un champ d'application plus large que ça.

\subsubsection{Interpolation}

L'idée de la dérivation numérique va être d'établir une reconstitution locale analytique mais approximative d'une fonction discrétisée pour en déduire sa dérivée.

C'est cet effort de reconstitution qu'on appelle \emph{interpolation}.

Il existe un certain nombre de méthodes d'interpolations, et nous ne présentons ici que l'interpolation polynomiale, elle-même divisée en plusieurs sous-domaines. Nous allons nous limiter à l'étude d'une seule d'entre elles - la plus connue d'entre toutes - qui porte le nom d'\emph{interpolation de Lagrange}.

On supposera la façon $t_i$ dont sont répartis les $n$ points $u_i$ de la fonction discrétisée connue. Il est clair que bien souvent (dans un signal par exemple), ceux-ci sont équirépartis ($t_i = T_e i$) mais ce n'est pas toujours vrai, notamment quand on cherche à résoudre une équation différentielle sur un domaine qui contient une singularité auquel cas on met en place un \emph{maillage} particulier.

On montre que le polynôme :

\[\boxed{
L : x \rightarrow \sum_{i=0}^n u_i \left( \prod_{j=0, i\neq j}^n\frac{x-t_j}{t_i-t_j} \right)
}\]

Est le seul polynôme de degré $\deg L \leq n$ qui passe par tous les points $(t_k, u_k)$.

L'existence est claire puisqu'on vient de proposer un polynôme qui satisfait cette propriété. En effet :

\[
\forall k,\; L(t_k) = \sum_{i=0}^n u_i \left( \prod_{j=0, i\neq j}^n\frac{t_k-t_j}{t_i-t_j} \right) 
\]

\[
\forall k,\; L(t_k) = \sum_{i=0, i\neq k}^n u_i \left( \prod_{j=0, i\neq j}^n\frac{t_k-t_j}{t_i-t_j}\right)  + u_k \prod_{j=0, i\neq j}^n\frac{t_i-t_j}{t_i-t_j}
\]

\[
\forall k,\; L(t_k) = \sum_{i=0, i\neq k}^n u_i \left( 0 \times \prod_{j=0, i\neq j\cap j\neq k}^n\frac{t_k-t_j}{t_i-t_j}\right)  + u_k \prod_{j=0, i\neq j}^n 1
\]

\[
\forall k,\; L(t_k) = u_k
\]

L'unicité se démontre par des considérations mathématiques sur les polynômes, pas si complexes mais sans grand intéret.

L'expression générale de $L$ a cependant ceci d'inconfortable qu'elle n'est pas définie récursivement, et se prête ainsi mal à une implémentation algorithmique flexible. On lui trouve néanmoins une autre forme (dite \emph{de Newton}) plus adaptée au calcul numérique :

\[\boxed{
L : x \rightarrow \sum_{i=0}^n d^0 u_i \left( \prod_{j=0}^{i-1}(x-t_j) \right)
}\]

Avec $d^i u_j$ une notation adoptée pour désigner la suite bidimensionnelle des \emph{différences divisées}, initialisée sur sa diagonale et défini récursivement par :

\[\boxed{
\left\{\begin{matrix}
d^i u_i = u_i \\
d^k u_i = \frac{d^{k+1} u_i - d^k u_{i-1}}{t_{i}-t_{k}}
\end{matrix}\right.
}\]

La démonstration de l'égalité entre ces deux formes se fait par récurrence :

\begin{itemize}
\item Initialisation : Pour $n=1$, la forme de Newton s'écrit :

\[
\sum_{i=0}^1 d^0 u_i \left( \prod_{j=0}^{i-1}(x-t_j) \right)
\]

\[
= d^0 u_0 + d^0 u_1(x-t_0)
\]

\[
= u_0 + \frac{u_1 - u_0}{t_1-t_0}(x-t_0)
\]

\[
= u_0\frac{t_1-t_0-x+t_0}{t_1-t_0} + u_1\frac{x-t_0}{t_1-t_0}
\]

\[
= u_0\frac{x-t_1}{t_0-t_1} + u_1\frac{x-t_0}{t_1-t_0}
\]

\[
= \sum_{i=0}^1 u_i \left( \prod_{j=0, i\neq j}^1\frac{x-t_j}{t_i-t_j} \right)
\]

\[
= L(x)
\]

\item Récurrence : Supposons l'égalité des deux formes vraie pour $n=m$, alors en $n=m+1$ on a :

\textcolor{red}{(compléter)}

\end{itemize}

Dans le cas où le maillage est uniforme (les points sont équirépartis), l'expression de la différence divisée devient :

\[\boxed{
d^k u_i = \frac{d^{k+1} u_i - d^k u_{i-1}}{T_e(i-k)}
}\]

Avec $T_e$ le \emph{pas} entre deux valeurs de $t$. C'est alors qu'il devient pertinent d'introduire la \emph{différence finie} :

\[\boxed{
\left\{\begin{matrix}
\Delta^0 u_i = u_i \\
\Delta^k u_i = \Delta^{k-1} u_{i+1} - \Delta^{k-1} u_{i}
\end{matrix}\right.
}\]

Commençons par montrer par récurrence que :

\[\boxed{
d^k u_i = \frac{\Delta^{i-k} u_k}{(i-k)!T_e^{i-k}}
}\]

\begin{itemize}
\item Initialisation : Pour $i=k$, on a :

\[
\frac{\Delta^0 u_k}{0!T_e^0} = u_k = u_i = d^0 u_i
\]

Auquel cas :

\[
d^{k+1} u_{i+1} = \frac{\Delta^{i-k} u_{k+1}}{(i-k)!T_e^{i-k}}
\]

Est également trivialement vraie puisque $i=k \Rightarrow i+1=k+1$

\item Récurrence : Supposons qu'on ai pour $i=j$ :

\[
\left(d^k u_j = \frac{\Delta^{j-k} u_k}{(j-k)!T_e^{j-k}}\right) \cap \left( d^{k+1} u_{j+1} = \frac{\Delta^{j-k} u_{k+1}}{(j-k)!T_e^{j -k}}\right)
\]

Alors en $i=j+1$ on a :

\[
d^k u_{j+1} = \frac{d^{k+1} u_{j+1} - d^k u_j}{T_e(j+1-k)}
\]

\[
d^k u_{j+1} = \frac{\Delta^{j-k} u_{k+1} - \Delta^{j-k} u_k}{(j+1-k)!T_e^{j+1-k}}
\]

\[
d^k u_{j+1} = \frac{\Delta^{j+1-k} u_{k}}{(j+1-k)!T_e^{j+1-k}}
\]

\textcolor{red}{(ne devrait-on pas montrer également que $d^{k+1} u_{j+2} = \frac{\Delta^{j+1-k} u_{k+1}}{(j+1-k)!T_e^{j+1-k}}$ ?)}
\end{itemize}

On peut finalement réecrire le polynôme de Lagrange en terme de différences finies :

\[
L(x) = \sum_{i=0}^n d^0 u_i \left( \prod_{j=0}^{i-1}(x-jT_e) \right)
\]

\[\boxed{
L(x) = \sum_{i=0}^n \Delta^{i} u_0 \left( \prod_{j=0}^{i-1} \frac{x-jT_e}{i!T_e^{i}} \right)
}\]

Par un tour de passe-passe polynomial (cf. polynôme symétrique et binôme de Newton) :

\textcolor{red}{(compléter)}

\subsubsection{Dérivation}

La plus simple des méthodes de dérivation numérique consiste à approcher localement la fonction discrétisée par le polynôme de Lagrange d'ordre 1 généré par deux points adjacents $(t_i,u_i)$ et $(t_{i+1},u_{i+1})$.

Dans l'hypothèse d'une discrétisation homogène et en se limitant à l'ordre 1 - chose qu'on fait la plupart du temps - la dérivation devient excessivement simple puisque le polynôme prend la forme :

\[\boxed{
L(x) = \Delta^0 u_i + \Delta^1 u_i \frac{x}{T_e}
}\]

Le polynôme de Lagrange n'approxime alors la fonction qu'au proche voisinage de $t_0$. La dérivée est donnée directement par :

\[\boxed{
u'_i = \frac{\Delta^1 u_i}{T_e} = \frac{u_{i+1} - u_i}{T_e}
}\]

Résultat que nous aurions pû intuiter sans avoir à évoquer une seule fois la notion d'interpolation. Cependant, celle-ci va nous permettre d'aller plus loin en développant à l'ordre 2 :

\[
L(x) = \Delta^0 u_i + \Delta^1 u_i \frac{x}{T_e} + \Delta^2 u_i \frac{x}{2 T_e}\left(\frac{x}{T_e}-1\right)
\]

\[\boxed{
L(x) = \Delta^0 u_i + \left(\frac{\Delta^1 u_i}{T_e}-\frac{\Delta^2 u_i}{2 T_e}\right)x + \frac{\Delta^2 u_i}{2 T_e^2}x²
}\]

Si la fonction avant discrétisation est suffisament régulière sur $[i T_e;(i+2)T_e]$, sa dérivée première sera donc souvent mieux approximée par :

\[
u'_i = \frac{\Delta^1 u_i}{T_e}-\frac{\Delta^2 u_i}{2 T_e}
\]

\[
u'_i = \frac{u_{i+1} - u_i}{T_e} - \frac{\Delta^1 u_{i+1} - \Delta^1 u_{i}}{2T_e}
\]

\[
u'_i = \frac{u_{i+1} - u_i}{T_e} - \frac{u_{i+2}-2u_{i+1}+u_i}{2T_e}
\]

\[\boxed{
u'_i = \frac{-u_{i+2}+4u_{i+1}-u_i}{2T_e}
}\]

Et la dérivée seconde par :

\[\boxed{
u''_i = \frac{\Delta^2 u_i}{T_e^2} = \frac{u_{i+2}-2u_{i+1}+u_i}{T_e^2}
}\]

Résultats n'ayant plus grand chose d'intuitif.

C'est dans le cas général que les choses se compliquent (quand on souhaite interpoler sur plus de points, ou bien calculer des dérivées d'ordre supérieur). On doit alors dériver analytiquement l'expression générale de $L$ en terme de différences finies \textcolor{red}{(à faire)}.

\subsection{Intégration numérique}

\subsubsection{Intégration}

L'intégration numérique fonctionne sur une vision exactement analogue à la dérivation. En supposant le maillage défini et la fonction discrétisée à intégrer connue, on intègre en fait un polynôme de Lagrange par morceau qu'on construit localement (en tous cas c'est la façon de faire qu'on présentera dans ce cours).

On peut se demander pourquoi ne pas utiliser l'expression du polynôme de Lagrange global plutôt que de se limiter à une succession d'approximations locales. La réponse est que les polynômes de degré trop élevés peuvent faire apparaître des fortes variations là où il n'y en avait pas avant discrétisation :

\textcolor{red}{(figure)}

Le polynôme de Lagrange à l'ordre zéro est donné simplement par :

\[
L(x) = \Delta^0 u_i = u_0
\]

Et son intégrale sur un pas $T_e$ (intervalle d'un morceau) :

\[
\int_{T_e i}^{T_e (i+1)} L(x) dx = u_i T_e
\]

Il suffira donc de sommer tous ces produits $u_i T_e$ pour accéder à l'intégrale de $T_e j$ à $T_e k$ :

\[
\int_{T_e j}^{T_e k} L(x) dx = T_e \sum_{i=j}^k u_i 
\]

Le même calcul à l'ordre 1 :

\[
L(x) = \Delta^0 u_i + \Delta^1 u_i \frac{x}{T_e}
\]

\[
\int_{T_e i}^{T_e (i+1)} L(x) dx = u_i T_e + (u_{i+1} - u_i)\frac{\left(T_e (i+1)\right)^2 - (T_e i)^2}{2T_e}
\]

\[
\int_{T_e i}^{T_e (i+1)} L(x) dx = T_e\left(u_i  + (u_{i+1} - u_i)\frac{2i + 1}{2}\right)
\]

\[
\int_{T_e i}^{T_e (i+1)} L(x) dx = T_e\left(u_i  + (u_{i+1} - u_i)\frac{2i + 1}{2}\right)
\]

\textcolor{red}{(c'est quoi ce $i$ qui traîne ?)}

\subsubsection{Equations différentielles ordinaires}

La résolution numérique d'une équation différentielle ordinaire se fait en partant de sa version continue :

\[
\frac{ds}{dt} + \frac{s}{\tau} = \frac{u}{\tau}
\] 

Ici celle du filtre passe-bas, et en remplaçant la fonction et ses dérivées par leurs versions discrétisées, selon une méthode d'interpolation qu'on aura préalablement choisie :

\[
\frac{s_i - s_{i-1}}{T_e} + \frac{s_{i-1}}{\tau} = \frac{u_{i-1} }{\tau}
\]

Il s'agit maintenant d'isoler le terme de plus haut degré - si c'est possible - pour la transformer en relation de récurrence qui s'avèrerait directement implémentable si les conditions aux limites s'écrivent en un même point :

\[
s_i = T_e\left( \frac{u_{i-1} }{\tau} - \frac{s_{i-1}}{\tau}\right) + s_{i-1}
\]

En effet, connaissant $s_0$ et le second membre $u$, il est clair que l'équation est ici résolue.

Les difficultés apparaissent lorsque :

\begin{itemize}
\item Une équation est non-linéaire.
\item Les conditions aux limites sont d'un autre type, ce qui est par exemple le cas d'un problême en régime permanent dans lequel on a deux conditions de Dirichlets.
\item Le second membre comporte des singularités.
\end{itemize}

Je ne tiens pas à m'étendre sur les méthodes de résolutions numériques qui est un sujet extrêmement vaste. Par contre, les équations linéaires ont leur méthode de résolution propre que nous allons voir maintenant.

Toute équation différentielle linéaire peut s'écrire comme une équation du premier ordre sans second membre :

\[
\frac{dS}{dt}-AS=0
\]

Avec $S_\nu = \frac{\partial^\nu s}{\partial x^\nu}$ dans le cas sans second membre ; sinon le second membre vient le compléter \textcolor{red}{(?)}.

Par exemple, le filtre passe-bas en posant $u$ constant :

\[
\frac{d}{dt}\left(\begin{matrix}s \\u\end{matrix}\right)
-\left(\begin{matrix}
-\frac{1}{\tau} & \frac{1}{\tau} \\
0 & 0
\end{matrix}\right)\left(\begin{matrix}s \\u\end{matrix}\right)=0
\]

C'est alors qu'on n'hésitera pas à écrire :

\[
\left(\begin{matrix}s \\u\end{matrix}\right) = e^{At} \left(\begin{matrix}s_0 \\u_0 \end{matrix}\right)
\]

En introduisant l'\emph{exponentielle de matrice}, qu'on calcule numériquement par le développement en série de Taylor qui le défini, lui-même calqué sur celui de l'exponentielle réel :

\[
\exp : A \rightarrow \sum_{k\in\mathbb{N}}\frac{A^k}{k!}
\]

Ainsi, la résolution de l'équation différentielle se ramène au calcul de :

\[
e^{At} = \exp\left(\begin{matrix}
-\frac{t}{\tau} & \frac{t}{\tau} \\
0 & 0
\end{matrix}\right)
\]

Qu'on délèguera à un logiciel de calcul scientifique. Sagemath gérant le calcul formel, il nous en donne même l'expression analytique :

\[
e^{At} = \left(\begin{matrix}
e^{-\frac{t}{\tau}} & 1-e^{-\frac{t}{\tau}} \\
0 & 1
\end{matrix}\right)
\]

Et donc :

\[
\left(\begin{matrix}s \\u\end{matrix}\right) = \left(\begin{matrix}
e^{-\frac{t}{\tau}} & 1-e^{-\frac{t}{\tau}} \\
0 & 1
\end{matrix}\right) \left(\begin{matrix}s_0 \\u_0 \end{matrix}\right)
\]

Qui est bien évidemment équivalent à l'expression qu'on a pû trouver quand nous avons présenté le système \textcolor{red}{(pas fait)}.

Il n'est pas très amusant de voir un ordinateur nous renvoyer une expression analytique que nous avons pû calculer analytiquement. Voyons pour ce qui est d'un système plus complexe.

J'ai choisi de prendre pour un tel système le redresseur triphasé pleine onde servant à la conversion AC-DC et alimentant un moteur à courant continu :

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
  \coordinate (A) at (-2,-5);
  \coordinate (B) at (-3.5,-5);
  \coordinate (C) at (-5,-5);
  \coordinate (D) at (-2,-3.5);
  \coordinate (E) at (-3.5,-2.5);
  \coordinate (F) at (-5,-1.5);
  \coordinate (G) at (-6,-3.5);
  \coordinate (H) at (-6,-2.5);
  \coordinate (I) at (-6,-1.5);
  \coordinate (N) at (-8,-2.5);
  \draw	(-1.5,0) to[open, v^=$U_i$, *-*] (-1.5,-5) -- (A)
  to[Ty] ++(0,2) -- ++(0,1)
  to[Ty] ++(0,2) -- ++(0.5,0);
  \draw	(A) -- ++(-1.5,0) to[Ty] ++(0,2) -- ++(0,1)
  to[Ty] ++(0,2) -- ++(1.5,0);
  \draw	(B) -- ++(-1.5,0) to[Ty] ++(0,2) -- ++(0,1)
  to[Ty] ++(0,2) -- ++(1.5,0);
 \draw (D) -- (G) to[V, v<=$V_3$, *-] ++(-2,0) -- (N);
 \draw (E) -- (H) to[V, v<=$V_2$, *-] ++(-2,0);
 \draw (F) -- (I) to[V, v<=$V_1$, *-] ++(-2,0) -- (N);
 
 \draw (-1.5,0) to[R, l_=$R_i$] ++(2,0)-- ++(0,-0.25)
  to[L, i_=$I_i$, l=$L_i$] ++(0,-2.25) to[V, v<=$E$] ++(0,-2.25) -- ++(0,-0.25) -- (A);
  \draw	(3.5,0) to[R, l=$R_e$] ++(-2,0)
  -- ++(0,-0.25) to[L, i=$I_e$, l=$L_e$, mirror] ++(0,-4.5) -- ++(0,-0.25)
  -- ++(2,0) to[V, v=$U_e$, *-*] ++(0,5);
\end{tikzpicture}
\end{center}
\shorthandon{:!}

Souvenons-nous que nous avions trouvé la tension $U_i$ aux bornes de la charge :

\begin{sagesilent}
x=var('x');
t = var('t');
x_coords = [t for t in srange(0,2*pi,0.01)];
y1_coords = [sin(x-pi/6).n() for x in x_coords];
y2_coords = [sin(x+2*pi/3-pi/6).n() for x in x_coords];
y3_coords = [sin(x+4*pi/3-pi/6).n() for x in x_coords];
Psi = var('Psi');
Psi = 0.4;
xc_coords = [t for t in srange((pi/6).n(),(pi/2).n(),0.01)];
yc_coords = [t for t in srange((pi/6).n(),(2*pi).n(),0.01)];
n = len(xc_coords); 
for t in range(0,n):
    if xc_coords[t]<pi/6+Psi :
        yc_coords[t] = sin(xc_coords[t]+2*pi/3-pi/6).n();
    else:
        yc_coords[t] = (-sin(xc_coords[t]+4*pi/3-pi/6)).n();
xc_coords = [t for t in srange((pi/6).n(),(2*pi).n(),0.01)];
for t in range(0,len(xc_coords)):
    u=t%n;
    yc_coords[t] = yc_coords[u];
output = "";
for i in range(0,len(x_coords)-1):
    output += "\draw[red, thick] ("+str(x_coords[i])+","+str(y1_coords[i])+")--("+str(x_coords[i+1])+","+str(y1_coords[i+1])+");\n";
    output += "\draw[blue, thick] ("+str(x_coords[i])+","+str(y2_coords[i])+")--("+str(x_coords[i+1])+","+str(y2_coords[i+1])+");\n";
    output += "\draw[green, thick] ("+str(x_coords[i])+","+str(y3_coords[i])+")--("+str(x_coords[i+1])+","+str(y3_coords[i+1])+");\n";
for i in range(0,len(xc_coords)-1):
    output += "\draw[black, thick] ("+str(xc_coords[i])+","+str(yc_coords[i])+")--("+str(xc_coords[i+1])+","+str(yc_coords[i+1])+");\n";
\end{sagesilent}

\begin{center}
\begin{tikzpicture}[x=5\textwidth/6/(2*pi)), y=\textwidth/3/2]
	// Axes
	\draw[->] (0,0) -- (2*pi+0.2,0) node[right] {$\alpha \; [rad]$};
	\draw[->] (0,-1.1) -- (0,1.2);
	//Graduation angulaire
	\foreach \x in {1,2,...,12}
     	\draw (pi/6*\x,0.08) -- (pi/6*\x,-0.08)
		node[anchor=north] {$\frac{\x \pi}{6}$};
		
	\draw (0,1) node[left] {$U_0$};
	\draw[dotted] (3.1416/6,0) -- ++(0,0.89);
	\draw[dotted] (3.1416/6+0.4,0) -- ++(0,0.6);
	\draw[dotted] (3.1416/2,0) -- ++(0/2,0.89);
	
	\draw (5*3.1416/6,0.8) node[anchor=north east] {\textcolor{red}{$U_{13}$}};
	\draw (10.5*3.1416/6,0.7) node[anchor=north west] {\textcolor{blue}{$U_{21}$}};
	\draw (9*3.1416/6,0.8) node[anchor=north east] {\textcolor{green}{$U_{32}$}};
	\draw (5*3.1416/6,0.86) node[anchor=south west] {\textcolor{black}{$U_i$}};
	\if\releaseversion1
	\sagestr{output}
	\fi
\end{tikzpicture}
\end{center}

L'équation différentielle à résoudre à l'induit s'écrit :

\[
U_i = R_i I_i + L_i \frac{dI_i}{dt} + E
\]

\[
\frac{dI_i}{dt} + \frac{R_i I_i}{L_i} + \frac{E}{L_i} - \frac{U_i}{L_i}
\]

Nous limiterons notre étude à un sixième de période, comme de coutûme en triphasé alternatif. Pour des raisons pratiques, nous choisirons l'intervalle $[\frac{\pi}{2}+\Psi;\frac{5\pi}{6}+\Psi]$ dans lequel on a simplement $U_i = U_{13} = U_0 \sin(\alpha - \frac{\pi}{6})$.

On supposera toutes les constantes connues et la force électromotrice $E$ constante (ce qui correspond à une vitesse de rotation constante, pratiquement vrai en régime permanent si le sixième de période est très petit devant la constante de temps mécanique. 

Mettons l'équation de l'induit sous forme matricielle. On aurait envie d'écrire quelque chose du genre :

\[
\frac{d}{dt}\left( \begin{matrix}
I_i \\
U_i \\
E
\end{matrix} \right)
- 
\left( \begin{matrix}
-\frac{R_i}{L_i} & \frac{1}{L_i} & -\frac{1}{L_i}\\
? & ? & ?\\
0 & 0 & 0 \\
\end{matrix} \right) \left( \begin{matrix}
I_i \\
U_i \\
E
\end{matrix} \right)= 0
\]

Mais nous ne saurions écrire la deuxième ligne de la matrice $A$, car les équations ne contiennent aucune renseignement sur $\frac{dU_i}{dt}$. Cependant, nous avons calculé un $U_i$ sinusoïdal sur l'intervalle d'étude, ce qui permet de connaître directement sa dérivée :

\[
\frac{dU_i}{dt} = \frac{d}{dt}(U_0 \sin(\omega t - \frac{\pi}{6})) = U_0 \omega \cos(\omega t - \frac{\pi}{6})
\]

Il s'agit donc de faire apparaître le sinus et le cosinus dans le vecteur d'état plutôt qu'$U_i$ :

\[
\frac{d}{dt}\left( \begin{matrix}
I_i \\
\sin(\omega t - \frac{\pi}{6}) \\
\cos(\omega t - \frac{\pi}{6}) \\
E
\end{matrix} \right)
- 
\left( \begin{matrix}
-\frac{R_i}{L_i} & \frac{U_0}{L_i} & 0 & -\frac{1}{L_i}\\
0 & 0 & \omega & 0\\
0 & -\omega & 0 & 0 \\
0 & 0 & 0 & 0\\
\end{matrix} \right) \left( \begin{matrix}
I_i \\
\sin(\omega t - \frac{\pi}{6}) \\
\cos(\omega t - \frac{\pi}{6}) \\
E
\end{matrix} \right)= 0
\]

Qu'on réecrira en terme d'angle $\alpha = \omega t$ :

\[
\frac{d}{d\alpha}\left( \begin{matrix}
I_i \\
\sin(\alpha - \frac{\pi}{6}) \\
\cos(\alpha - \frac{\pi}{6}) \\
E
\end{matrix} \right)
- 
\left( \begin{matrix}
-\frac{R_i}{L_i} & \frac{U_0}{L_i} & 0 & -\frac{1}{L_i}\\
0 & 0 & 1 & 0\\
0 & -1 & 0 & 0 \\
0 & 0 & 0 & 0\\
\end{matrix} \right) \left( \begin{matrix}
I_i \\
\sin(\alpha - \frac{\pi}{6}) \\
\cos(\alpha - \frac{\pi}{6}) \\
E
\end{matrix} \right)= 0
\]

On peut enfin passer à l'exponentielle :

\[
\left( \begin{matrix}
I_i \\
\sin(\alpha - \frac{\pi}{6}) \\
\cos(\alpha - \frac{\pi}{6}) \\
E
\end{matrix} \right)
=
e^{A\left(\alpha-\left(\frac{\pi}{2}+\Psi\right)\right)}
\left( \begin{matrix}
I_0 \\
\sin(\frac{\pi}{3} + \Psi) \\
\cos(\frac{2\pi}{3} + \Psi) \\
E
\end{matrix} \right)
\]

Cette fois-ci, calculer l'exponentielle semble hors de portée du commun des mortels. Sagemath y arrive toujours, mais le résultat est d'une complexité telle qu'il ne tient pas sur la page :

\begin{sagesilent}
R_i = var('R_i');
U_0 = var('U_0');
L_i = var('L_i');
alpha = var('alpha');
Psi = var('Psi');

M = (alpha-(pi/2-Psi))*Matrix([[-R_i/L_i,U_0/L_i,0,-1/L_i],[0,0,1,0],[0,-1,0,0],[0,0,0,0]]);
output = latex(M.exp().apply_map(attrcall('full_simplify')));
\end{sagesilent}

\scriptsize
\[
\if\releaseversion1
\sagestr{output}
\fi
\]
\normalsize

Nous emploierons donc la résolution numérique, affectant une valeur à chacune des constantes, et parcourant les $\alpha$ en vue de tracer $I_i(\alpha)$.

Cependant, avant ça, il faut attribuer une valeur à $I_0$ (courant d'induit en $\alpha = \frac{\pi}{2}+\Psi$). Pour ce faire, on va utiliser la condition aux limites propre au régime permanent, en écrivant que l'état revient à l'identique tous les sixièmes de période. Ainsi :

\[
e^{A\left(\frac{\pi}{3}-\left(\frac{\pi}{2}+\Psi\right)\right)}
\left( \begin{matrix}
I_0 \\
\sin(\frac{\pi}{3} + \Psi) \\
\cos(\frac{2\pi}{3} + \Psi) \\
E
\end{matrix} \right)
= \left( \begin{matrix}
I_0 \\
\sin(\frac{\pi}{3} + \Psi) \\
\cos(\frac{2\pi}{3} + \Psi) \\
E
\end{matrix} \right)
\]

En multipliant par la matrice ligne $\left( \begin{matrix} 1 & 0 & 0 & 0\end{matrix} \right)$, ce qui permet de ne considérer que la première ligne de l'exponentielle :

\[
\left( \begin{matrix} 1 & 0 & 0 & 0\end{matrix} \right) e^{A\left(\frac{\pi}{3}-\left(\frac{\pi}{2}+\Psi\right)\right)}
\left( \begin{matrix}
I_0 \\
\sin(\frac{\pi}{3} + \Psi) \\
\cos(\frac{2\pi}{3} + \Psi) \\
E
\end{matrix} \right)
= I_0
\]

Ou encore, en appellant $M_{\mu\nu}$ la matrice exponentielle :

\[
\left( \begin{matrix} M_{11} & M_{12} & M_{13} & M_{14}\end{matrix} \right) \left( \begin{matrix}
I_0 \\
\sin(\frac{\pi}{3} + \Psi) \\
\cos(\frac{2\pi}{3} + \Psi) \\
E
\end{matrix} \right)
= I_0
\]

\[
\left( \begin{matrix} M_{12} & M_{13} & M_{14}\end{matrix} \right) \left( \begin{matrix}
\sin(\frac{\pi}{3} + \Psi) \\
\cos(\frac{2\pi}{3} + \Psi) \\
E
\end{matrix} \right)
= (1-M_{11}) I_0
\]

\[
I_0 = \frac{1}{1-M_{11}} \left( \begin{matrix} M_{12} & M_{13} & M_{14}\end{matrix} \right) \left( \begin{matrix}
\sin(\frac{\pi}{3} + \Psi) \\
\cos(\frac{2\pi}{3} + \Psi) \\
E
\end{matrix} \right)
\]

$I_0$ étant déterminé par la condition aux limites, il ne reste plus qu'à calculer le produit de l'exponentielle par l'état initial à des $\alpha$ répartis sur l'intervalle d'étude, permettant finalement de tracer $I_i(\alpha)$ :

\textcolor{red}{(tracer $I_i(\alpha)$)}

\subsection{Méthode des volumes finis}

\subsection{Méthode des éléments finis}

\chapter{Traitement du signal}

Ce chapitre fait grand usage des distributions, auxquelles un chapitre est réservé en préambule.

Un signal est une fonction mathématique véhiculée par l'état temporel d'une grandeur physique ou d'un protocole de transmission informatique.

L'idée est que ledit signal n'a pas d'intéret en tant que tel pour l'énergie qu'il transporte (prise la plus faible possible), mais pour l'information qu'il fait transiter.

Parce-que dans toutes les branches de la physique l'énergie est une fonction quadratique de la grandeur et parcequ'un signal $f$ est nécessairement \emph{à énergie finie} :

\[\boxed{
||f||^2 = \braket{f|f} = \int_\mathbb{R} |f(t)|^2 dt < \infty
}\]

L'objet du traitement du signal est l'étude des procédés analogiques et numériques pouvant mener à l'alteration souhaitée ou non d'un signal.

\section{Conversion analogique-numérique}

Les méthodes de contrôle des systèmes étant de plus en plus sophistiquées, l'usage de microcontrôlleur, DSP ou mieux FPGA seul permet leur implémentation. Il n'est plus envisageable de se passer de ces technologies au profit du ``tout analogique''.

Nous commencerons donc nécessairement par une étape de préprocess consistant à numériser un signal - bien souvent une tension électrique (éventuellement produite par le capteur d'une autre grandeur) - avant de passer au calcul proprement dit.

Néanmoins, le traitement du signal a déjà commencé. Ce n'est pas désiré, mais l'information numérique pouvant transiter dans une liaison informatique est limitée et la conversion de l'analogique au numérique ne peut-être anodine :

\begin{itemize}
\item La finitude du débit de transmission nécessite la discrétisation temporelle (échantillonage) du signal.
\item La finitude du nombre de connecteurs rend également finie le nombre de valeurs qu'un signal numérique peut prendre.
\end{itemize}

Le premier point signifie que le convertisseur ne va mesurer la grandeur d'entrée qu'à des instants régulièrement espacés, et laisser le signal numérique dans cet état pendant toute la période d'échantillonage $T_e$.

Formellement, cela revient à multiplier $f$ par un peigne de Dirac $\mathrm{III}_{T_e}$. Le spectre est alors altéré d'une façon très particulière :

\[
\braket{f\mathrm{III}_{T_e}|e^{j\omega t}} = \braket{f|e^{j\omega t}} * \braket{\mathrm{III}_{T_e}|e^{j\omega t}}
\]

\[
\braket{f\mathrm{III}_{T_e}|e^{j\omega t}} = \braket{f|e^{j\omega t}} * \frac{1}{T_e}\mathrm{III}_{\frac{2\pi}{T_e}}
\]

\[
\braket{f\mathrm{III}_{T_e}|e^{j\omega t}} = \frac{1}{T_e} \int_\mathbb{R} \braket{f|e^{j(\omega-a) t}} \sum_{i\in \mathrm{Z}}\delta(a - \frac{2\pi}{T_e} i) \; da
\]

\[
\braket{f\mathrm{III}_{T_e}|e^{j\omega t}} = \frac{1}{T_e} \sum_{i\in \mathrm{Z}} \int_\mathbb{R} \braket{f|e^{j(\omega-a) t}} \delta(a - \frac{2\pi}{T_e} i) \; da
\]

\[
\braket{f\mathrm{III}_{T_e}|e^{j\omega t}} = \frac{1}{T_e} \sum_{i\in \mathrm{Z}} \braket{f|e^{j(\omega-\frac{2\pi}{T_e} i) t}}
\]

Avec une écriture plus concise:

\[\boxed{
\mathcal{F}(\mathrm{III}_{T_e}f, \omega) = \frac{1}{T_e} \sum_{i\in \mathrm{Z}} \mathcal{F}(f, \omega-\frac{2\pi}{T_e} i)
}\]

Qui traduit donc le fait que l'échantillonage additionne des versions translatées de $\frac{2\pi}{T_e}$ du spectre de $f$.

\textcolor{red}{(figure)}

Supposons maintenant que le spectre de $f$ ne soit pas à support compact, ou bien soit à support compact mais dans un intervalle de nombre d'onde maximale supérieur à $\frac{4\pi}{T_e}$. La situation suivante se présenterait :

\textcolor{red}{(figure)}

Dans laquelle les motifs répliqués empiètent les uns sur les autres - et nous ferions donc une mauvaise mesure de $f$. On dirait alors que le \emph{critère de Shannon-Nyquist} :

\[\boxed{
k_e > 2k_{max}
}\]

N'est pas respecté. Pour garantir la satisfaction de ce critère - et donc la validité de l'échantillonage - dans le cas de signaux à large spectre, il conviendra de mettre en place un \emph{filtre anti-repliement} analogique en amont du convertisseur, destiné à sélectionner seulement les fréquences inférieures à $\frac{1}{2 T_e}$.

Une fois l'échantillonage effectué, on peut travailler avec le signal $f\mathrm{III}_{T_e}$ tel que :

\[
f\mathrm{III}_{T_e}(t) = f(x)\sum_{i \in \mathbb{Z}}\delta(t-\frac{2\pi}{T_e} i)
\]

\[
f\mathrm{III}_{T_e}(t) = \sum_{i \in \mathbb{Z}} f(x)\delta(t-\frac{2\pi}{T_e} i)
\]

Toute l'information en $t$ non multiple de $\frac{2\pi}{T_e}$ est perdue, ce qui nous amène à introduire la suite :

\[\boxed{
u : i \rightarrow f(\frac{2\pi}{T_e}i)
}\]

Qui est équivalente à $f\mathrm{III}_{T_e}$.

\section{Traitement des signaux numériques}

Un signal numérique est par nature discrétisé, et prend donc la forme d'une suite $u_n$ (éventuellement, le signal peut-être multi-dimensionnel et donc dépendre de plusieurs indices, c'est le cas d'une vidéo par exemple).

Un signal causal débutera par une suite de $0$.

\subsection{Convolution numérique}

Au coeur du traitement du signal réside la notion de fonction de transfert, qui va venir opérer sur un signal d'entrée $u_n$. Cette fonction de transfert sera la représentation d'une équation différentielle - éventuellement non-linéaire - pouvant-être destinée à toute sortes de chose comme le traitement du son, de l'image ou encore la modélisation et contrôle des systèmes.

Si cet opérateur est causal, invariant par translation et linéaire, alors il peut nécessairement être représenté par une convolution ; auquel cas la connaissance de la réponse impulsionnelle suffit à caractériser entièrement l'opérateur.

Dans le domaine continu on défini la convolution \textcolor{red}{(pas parlé avant)} par :

\[\boxed{
f * g : x \rightarrow \int_\mathbb{R} f(x-t)g(t) dt
}\]

Ne pouvant être aussi précis, on prendra des \emph{pas de temps} suffisament petits pour que cette expression puisse être approchée par sa version discrète :

\[\boxed{
u * v : i \rightarrow \sum_{j \in \mathrm{Z}} u_{i-j}v_j
}\]

\textcolor{red}{(distinction filtre RIF IIF)}

\subsection{Transformée de Fourier discrète}

La transformée de fourier d'une fonction continue $f$ est donnée par :

\[
\mathcal{F}(f) : k \rightarrow \int_\mathbb{R} f(x)e^{-jk x}
\]

Son pendant discret sera naturellement donné par :

\[
\mathcal{F}(u) : k \rightarrow \sum_{i \in \mathrm{Z}} u_i e^{-jk iT_e}
\]

Dont on ne sélectionnera que les valeurs multiples de $\frac{2\pi}{T_e}$ :

\[\boxed{
\mathcal{F}(u) : \alpha \rightarrow \sum_{i \in \mathrm{Z}} u_i e^{-j2\pi \alpha i}
}\]

Cependant, la sommation sur $k \in \mathrm{Z}$ étant impossible en pratique (et présuppose une connaissance parfaite du signal en tout temps, alors que la mesure a bien dû démarrer à un certain moment), on se limitera au calcul de la transformée de Fourier de $f$ convoluée avec celle d'une \emph{fenêtre} à support compact.

Le signal est alors altéré. Si son spectre est nul en deça d'un nombre d'onde $k_{min}$, on choisira dans la mesure du possible une fenêtre de taille :

\[
D > \frac{2\pi}{k_{min}}
\]

De sorte que le spectre de la fenêtre se rapproche de celui d'un Dirac comparativement à celui de $f$.

Les trois exemples élémentaires de fenêtres sont :

\begin{itemize}
\item La fonction porte, nulle partout sauf sur un intervalle de longueur $D$. Sa transformée de Fourier est un sinus cardinal \textcolor{red}{(pas parlé avant)} (cela se vérifie facilement) dont les lobes secondaires empiètent sur le spectre du signal initial, ce qui engendre un \emph{phénomène de Gibbs} indésiré ; c'est-à-dire et une altération majeure du spectre à haute fréquence.
\item Un sinus cardinal correctement paramétré, de façon à que sa transformée de Fourier soit une porte s'arrétant en $\frac{2\pi}{D}$. Cette fenêtre est très bonne, mais irréalisable en pratique car de longueure infinie.
\item La \emph{fenêtre de Hamming} qui est un compromis réalisable des deux méthodes précédentes, se définissant par :

\[
h(t) = 0.54-0.46\cos(\frac{2\pi}{D} x)
\]

Sur l'intervalle $[0,D]$ et nulle ailleurs. Ce mode de fenêtrage est souvent utilisé en pratique.
\end{itemize}

Un fenêtrage adapté permet donc de limiter le phénomène de Gibbs tout en limitant la quantité de calcul nécessaire à la transformation de Fourier numérique.

En outre, les algorithmes de transformée de Fourier comme FFT \textcolor{red}{(pas parlé, à faire)} nécessitent un nombre de points $n$ qui est une puissance de $2$. En pratique, pour effectuer un échantillonage suivi d'une FFT on appliquera la méthode suivante :

\begin{itemize}
\item Premièrement, on impose une taille de fenêtre $D = \frac{2\pi}{k_{min}}$ et on utilise le critère de Shannon pour trouver une borne minimum à $k_e$. On met en place un filtre anti-repliement analogique pour éviter tout risque de repliement. 
\item Ensuite, on en déduit le nombre de points minimum à traiter $n_{min} = \frac{k_e D}{2\pi}$ et on prend pour $n$ la puissance de $2$ supérieure.
\item Enfin, on recalcule $k_e = \frac{2\pi n}{D}$, qui respectera nécessairement le critère de Shannon (avec un peu de zèle) ou l'on augmentera la taille de la fenêtre selon la situation.
\end{itemize}

On notera que les algorithmes FFT conservent le nombre de points, ce qui permet de définir une résolution spectrale $r=\frac{k_e}{2\pi n}=\frac{1}{D}$.

\subsection{Transformée en Z}

La transformée de Laplace n'est pas adaptée au calcul numérique, pour deux raisons :

\begin{itemize}
\item L'intervention d'exponentielles, difficiles à calculer.
\item Elle est définie en tout point sur une intégrale, qui correspond à une somme dans le monde discret ; laquelle est également coûteuse en temps de calcul.
\end{itemize}

Pourtant, la transformée de Laplace réduisant la convolution à une simple multiplication terme à terme, on aimerait disposer d'un tel outil. Le moyen qu'ont trouvé les ingénieurs pour lever ces deux problêmes est assez remarquable, et a permis l'explosion des techniques de traitement numérique du signal.

Reprenons la définition de la transformée de Laplace :

\[
\mathcal{L}(f) : p \rightarrow \int_\mathbb{R} f(x)e^{-p x} dx
\]

Sous une version discrète, on aurait envie d'écrire :

\[
\mathcal{L}(f)(p) \sim \sum_{i \in \mathrm{Z}} u_i e^{-pT_e i}
\]

Notre but va maintenant être d'écarter le problême de l'exponentielle en la linéarisant. Ce point est fondamental, et comme nous allons le voir revient à choisir une méthode d'intégration (Euler, trapèzes...), nous pourrons donc nous référer à la discussion sur le sujet dans le chapitre d'analyse numérique (\textcolor{red}{(pas fait)}. Par exemple, pour les trapèzes, nous ferions :

\[
\mathcal{L}(f)(p) \sim \sum_{i \in \mathrm{Z}} u_i \left( \frac{e^{p\frac{T_e}{2}}}{e^{-p\frac{T_e}{2}}}\right)^{-i}
\]

\[
\mathcal{L}(f)(p) \sim \sum_{i \in \mathrm{Z}} u_i \left( \frac{1+p\frac{T_e}{2}}{1-p\frac{T_e}{2}}\right)^{-i}
\]

Et nous poserions $z = \frac{1+p\frac{T_e}{2}}{1-p\frac{T_e}{2}}$, définissant la \emph{transformée en Z} comme l'approximation discrète de la transformée de Laplace :

\[\boxed{
\mathcal{Z}(u) : z \rightarrow \sum_{i \in \mathrm{Z}} u_i z^{-i}
}\]

Et on aurait donc $p = \frac{2}{T} \frac{1 - z^{-1} }{1 + z^{-1} }$

Cependant, à des fins pédagogiques, je vais me restreindre à utiliser seulement la linéarisation par la méthode d'Euler :

\[
\mathcal{L}(f)(p) \sim \sum_{i \in \mathrm{Z}} u_i (1+pT_e)^{-i}
\]

Pour donc poser $z = 1+pT_e$.

Tout comme la transformée de Laplace, $\mathcal{Z}(u)$ est bien souvent définie sur un domaine de $\mathbb{C}$ ; qui cette fois sera un disque dont on cherchera le rayon avant toute chose.

Comme tout cela doit vous sembler excessivement abstrait, nous allons maintenant passer à un exemple. Imaginions que nous souhaitions retranscrire le comportement du filtre passe-bas le plus simple dans un calculateur. 

Je vous renvois à la partie sur les filtres électriques, où nous avions trouvé que l'équation différentielle régissant ce système s'écrivait :

\[
\frac{ds}{dt} + \frac{s}{\tau} = \frac{u}{\tau}
\]

Appliquer la transformée de Laplace sur cette équation nous permet de connaître la \emph{fonction de transfert} de ce filtre :

\[
\braket{\frac{ds}{dt}|e^{-p^* t}} + \frac{1}{\tau}\braket{s|e^{-p^* t}} = \frac{1}{\tau}\braket{u|e^{-p^* t}}
\]

L'équation devient algébrique :

\[
(p+\frac{1}{\tau})\braket{s|e^{-p^* t}} = \frac{1}{\tau}\braket{u|e^{-p^* t}}
\]

\[
\frac{\braket{s|e^{-p^* t}}}{\braket{u|e^{-p^* t}}} = \frac{1}{1 + \tau p}
\]

On défini finalement l'opérateur $h$ représentatif de ce filtre par sa fonction de transfert :

\[
\mathcal{L}(h) : p \rightarrow \frac{1}{1 + \tau p}
\]

En vue de discrétiser le problême, commençons par calculer la transformée en Z associée ; il s'agit de remplacer $p$ par $\frac{z-1}{T_e}$ :

\[
\mathcal{Z}(h) : z \rightarrow \frac{1}{1 + \frac{\tau}{T_e} (z-1)}
\]

\[
\mathcal{Z}(h)(z) = \frac{z^{-1}}{\frac{\tau}{T_e} + (1-\frac{\tau}{T_e})z^{-1}}
\]

Nous désirons maintenant appliquer cet opérateur $h$ à un signal $u$, c'est-à-dire en faire la convolution $s=h*u$. Evidemment, nous allons passer par les transformées en Z :

\[
\mathcal{Z}(s) = \mathcal{Z}(h)\mathcal{Z}(u)
\]

\[
\sum_{i \in \mathrm{Z}} s_i z^{-i} = \frac{z^{-1}}{\frac{\tau}{T_e} + (1-\frac{\tau}{T_e})z^{-1}}\sum_{i \in \mathrm{Z}} u_i z^{-i}
\]

\[
\left(\frac{\tau}{T_e} + (1-\frac{\tau}{T_e})z^{-1}\right)\sum_{i \in \mathrm{Z}}s_i z^{-i} = z^{-1}\sum_{i \in \mathrm{Z}} u_i z^{-i}
\]

\[
\frac{\tau}{T_e}\sum_{i \in \mathrm{Z}}s_i z^{-i}
+ (1-\frac{\tau}{T_e})\sum_{i \in \mathrm{Z}}s_i z^{-(i+1)} = \sum_{i \in \mathrm{Z}} u_i z^{-(i+1)}
\]

Par des changements de variable :

\[
\frac{\tau}{T_e}\sum_{i \in \mathrm{Z}}s_i z^{-i}
+ (1-\frac{\tau}{T_e})\sum_{i \in \mathrm{Z}}s_{i-1} z^{-i} = \sum_{i \in \mathrm{Z}} u_{i-1} z^{-i}
\]

\[
\sum_{i \in \mathrm{Z}}\left(\frac{\tau}{T_e}s_i
+ (1-\frac{\tau}{T_e})s_{i-1} \right)z^{-i} = \sum_{i \in \mathrm{Z}} u_{i-1} z^{-i}
\]

Et c'est là qu'on voit apparaître une égalité entre deux transformées en Z - qu'on inverse donc immédiatement sans souci :

\[
\frac{\tau}{T_e}s_i + (1-\frac{\tau}{T_e})s_{i-1} = u_{i-1}
\]

Qui nous donne - presque miraculeusement la première fois qu'on voit ce calcul - le même algorithme récurrent que celui que nous avions obtenu par la méthode d'Euler :

\[
s_i = (1-\frac{T_e}{\tau})s_{i-1} + \frac{T_e}{\tau}u_{i-1}
\]

Parce-qu'en effet, en le mettant sous la forme suivante :

\[
\frac{s_i - s_{i-1}}{T_e} + \frac{s_{i-1}}{\tau} = \frac{u_{i-1} }{\tau}
\]

On retrouve bien une forme approchée de l'équation différentielle dont nous sommes partis :

\[
\frac{ds}{dt} + \frac{s}{\tau} = \frac{u}{\tau}
\]

Evidemment, je laisse à votre imagination le soin de perçevoir l'intéret de cet outil quand il s'agira de régler d'implémenter des filtres plus complexes. Il suffira alors de connaître la transformée de Laplace de ce filtre (ce qui est plus naturel que de connaître son équation différentielle - en général - car quand on dimensionne un filtre on travaille directement sur le domaine fréquentiel) puis de reproduire ce qui vient d'être fait pour trouver la relation de récurrence à programmer.

Il est également tout à fait possible de concevoir par cette méthode des filtres non-linéaires ; la seule complication étant que la fonction de transfert n'est alors plus une fonction rationnelle et que le calcul de sa transformée inverse peut-être difficile voir impossible analytiquement.

Enfin, la présence de $z$ à la puissance positive est le signe d'un filtre acausal, ce qui peut avoir du sens dans le traitement d'un signal préconçu comme un enregistrement audio.

\chapter{Contrôle des systèmes}

Le contrôle des systèmes est une discipline souvent désignée sous l'appellation d'\emph{automatique}. Un grand nombre des dispositifs ayant étés vus dans ce cours nécessitent d'être commandés.

En premier lieu, nous allons voir un exemple basique de régulation. Ensuite seulement, nous rentrerons dans le détail de la théorie.

\section{Régulation PID}

Reprenons l'exemple de l'alimentation de la machine à courant continu par redressement sur pont de Graetz, qui nous accompagne depuis de nombreuses pages.

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
  \coordinate (A) at (-2,-5);
  \coordinate (B) at (-3.5,-5);
  \coordinate (C) at (-5,-5);
  \coordinate (D) at (-2,-3.5);
  \coordinate (E) at (-3.5,-2.5);
  \coordinate (F) at (-5,-1.5);
  \coordinate (G) at (-6,-3.5);
  \coordinate (H) at (-6,-2.5);
  \coordinate (I) at (-6,-1.5);
  \coordinate (N) at (-8,-2.5);
  \draw	(-1.5,0) to[open, v^=$U_i$, *-*] (-1.5,-5) -- (A)
  to[Ty] ++(0,2) -- ++(0,1)
  to[Ty] ++(0,2) -- ++(0.5,0);
  \draw	(A) -- ++(-1.5,0) to[Ty] ++(0,2) -- ++(0,1)
  to[Ty] ++(0,2) -- ++(1.5,0);
  \draw	(B) -- ++(-1.5,0) to[Ty] ++(0,2) -- ++(0,1)
  to[Ty] ++(0,2) -- ++(1.5,0);
 \draw (D) -- (G) to[V, v<=$V_3$, *-] ++(-2,0) -- (N);
 \draw (E) -- (H) to[V, v<=$V_2$, *-] ++(-2,0);
 \draw (F) -- (I) to[V, v<=$V_1$, *-] ++(-2,0) -- (N);
 
 \draw (-1.5,0) to[R, l_=$R_i$] ++(2,0)-- ++(0,-0.25)
  to[L, i_=$I_i$, l=$L_i$] ++(0,-2.25) to[V, v<=$E$] ++(0,-2.25) -- ++(0,-0.25) -- (A);
  \draw	(3.5,0) to[R, l=$R_e$] ++(-2,0)
  -- ++(0,-0.25) to[L, i=$I_e$, l=$L_e$, mirror] ++(0,-4.5) -- ++(0,-0.25)
  -- ++(2,0) to[V, v=$U_e$, *-*] ++(0,5);
\end{tikzpicture}
\end{center}
\shorthandon{:!}

Afin de bien cerner la situation, mettons qu'on désire mettre en oeuvre ce montage pour actionner l'un des moteurs de propulsion d'un train à caténaires triphasés.

Mettons que le train soit à \emph{consigne causale}, c'est à dire qu'un conducteur humain (ou une centrale ferroviaire) donne à chaque instant la consigne en vitesse sans qu'on ait accès aux commandes futures. Le couple sera déterminé par le modèle mécanique du train (prenant en compte notamment le poids et les frottements).

On se placera dans l'hypothèse confortable (et on cherchera à y rester) où la constante de temps électrique du système est très grande devant la période $\frac{2\pi}{6\omega}$ des ondulations coté continu, ce qui implique qu'on puisse mener les calculs avec :

\[
U_i = \frac{3U_0}{\pi}\cos(\Psi)
\]

En outre, on supposera le système linéaire tant du point de vue électrique que mécanique. Si ce n'est pas le cas, il faudra se limiter au voisinage d'un point de fonctionnement et linéariser, ou bien faire varier les paramètres de régulation (que nous allons calculer) explicitement avec le temps ; ou encore se passer de conducteur humain et mettre en place une commande optimale telle que nous la verrons dans le chapite d'optimisation \textcolor{red}{(pas fait)}, mais c'est d'un tout autre niveau de complexité.

L'idée de cette section est donc de mettre en oeuvre une stratégie de régulation pour trouver une valeur pertinente à donner à $\Psi$ à tout instant connaissant la vitesse qu'on souhaite que le système ait.

Les hypothèses sus-cités menent à l'outil archaïque qu'est la \emph{régulation PID} (pour \emph{proportional-integral-derivative}, mais qui convient très souvent en pratique, au moins pour les systèmes de la vie de tous les jours.

Nous allons procéder en deux étapes. D'abord, nous mettrons en place la régulation du couple sur la base du modèle électrique, puis nous adjoindrons à celle-ci une régulation en vitesse.

\subsection{Régulation en couple}

Au niveau élémentaire, les grandeurs du système que nous contrôlons vraiment sont les tensions de gâchettes des thyristors. Cependant, une commande par évènements discrets \textcolor{red}{(pas parlé avant, ne sais même pas où le mettre)} permet de se ramener à la commande directe de l'angle de retard à l'amorçage $\Psi$. Enfin, une hypothèse précédente permettra d'écrire :

\[
\Psi = \arccos{\frac{\pi U_i}{3U_0}}
\]

Sous conditions qu'on soit capable de calculer numériquement un arcosinus suffisament rapidement, on pourra simplement se ramener à une commande de $U_i$, masquant tout à fait les spécificités du redresseur. L'entièreté de notre discussion sera donc valable pour n'importe quelle technologie d'alimentation délivrant une tension qu'on pourra se permettre de supposer non-ondulante.

La grandeur d'entrée de notre système sera donc $U_i$. Le première idée pourrait-être de choisir directement le bon $U_i$ qui donnerait à court terme le bon couple - et on parlerait alors de \emph{commande en boucle ouverte} :

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
\node[left] at (0,0) {$\Gamma_c$};
\node[rectangle,draw,fill=white] (H) at (1,0) {$C(p)$};
\node[rectangle,draw,fill=white] (U_i) at (4,0) {$MCC(p)$};
\node[right] at (5.5,0) {$\Omega$};

\draw[->] (0,0) -- (H);
\draw[->] (H) -- (U_i) node[midway, above]{$U_i$};
\draw[->] (U_i) -- (5.5,0);
\end{tikzpicture}
\end{center}
\shorthandon{:!}

L'usage des transformées de Laplace est presque inévitable en automatique, car elles sont beaucoup plus simples à visualiser et manipuler par l'esprit que les équations différentielles ; et si vous ne me croyez pas, alors patientez un peu.

Commencer à chercher $C$ - la fonction de transfert du \emph{correcteur} - à partir de ceci reviendrait néanmoins à oublier une chose importante, qui est que la fonction de transfert de la machine à courant continu ne dépend pas que de la machine elle-même, mais également de son environnement à travers un couple $\Gamma_{ch}$. Le détail des équations du moteur à courant continu est donné ci-dessous (que je vous laisse vérifier à partir de la section dédiée dans le chapitre d'électromagnétisme appliqué) :

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
\node[left] at (0,0) {$\Gamma_c$};
\node[rectangle,draw,fill=white] (H) at (1,0) {$C(p)$};
\node[circle,draw,fill=white] (U_i) at (2.5,0) {$-$};
\node[rectangle,draw,fill=white] (Ui-E) at (4,0) {$\frac{1}{R_i+L_ip}$};
\node[rectangle,draw,fill=white] (I_i) at (6,0) {$\braket{\frac{\partial M}{\partial \theta}}I_e$};
\node[circle,draw,fill=white] (gamma_em) at (8,0) {$-$};
\node[rectangle,draw,fill=white] (gamma) at (9.5,0) {$\frac{1}{Jp}$};
\node[right] at (10,0) {$\Omega$};

\node[above] at (8,0.5) {$\Gamma_{ch}$};
\node[rectangle,draw,fill=white] (MIe) at (5,-1.5) {$\braket{\frac{\partial M}{\partial \theta}}I_e$};

\draw[->] (0,0) -- (H);
\draw[->] (H) -- (U_i) node[midway, above]{$U_i$};
\draw[->] (U_i) -- (Ui-E);
\draw[->] (Ui-E) -- (I_i) node[midway, above]{$I_i$};
\draw[->] (I_i) -- (gamma_em) node[midway, above]{$\Gamma_{em}$};
\draw[->] (gamma_em) -- (gamma) node[midway, above]{$\Gamma$};
\draw[->] (gamma) -- (10,0);

\draw[->] (8,0.5) -- (gamma_em);

\draw[->] (9,0) -- ++(-0.25,0) |- (MIe);
\draw[->] (MIe) -| (U_i) node[midway, above left]{$E$};

\end{tikzpicture}
\end{center}
\shorthandon{:!}

Ce diagramme - dit \emph{graphe d'asservissement} - représente une seule équation qui contiendrait une inconnue de trop. Il n'existe donc pas de $C$ permettant de commander correctement de cette façon.

On peut imaginer avoir à notre disposition un modèle mécanique permettant d'approximer $\Gamma_{ch}$, mais réguler sur la base de ce modèle peut-être risqué car l'intégration pure $\frac{1}{Jp}$ suivie du passe-bas $\frac{1}{R_i+L_ip}$ fait dériver toute erreur de modélisation.

Il est donc nécessaire pour pouvoir proposer un contrôle décent que le moteur soit asservi à partir d'au moins une autre des grandeurs en aval de $U_i$. Les mesures de deux d'entre elles sont particulièrement accessibles :

\begin{itemize}
\item Le courant $I_i$, qui nécessiterait un ampèremêtre interfacé ou un capteur à effet Hall.
\item La vitesse angulaire qui nécessiterait un tachymètre, codeur optique ou résolveur.
\end{itemize}

Pour une commande en couple, on va vouloir chercher à réguler $\Gamma_{em}$ qui est proportionnel à $I_i$, lequel semble donc tout indiqué pour servir de grandeur d'asservissement. \textcolor{red}{(mais quand même, quid d'un asservissement sur $\Omega$ ?)}

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
\node[left] at (-1.5,0) {$\Gamma_c$};
\node[circle,draw,fill=white] (epsgamma) at (-0.5,0) {$-$};
\node[rectangle,draw,fill=white] (H) at (1,0) {$C_\Gamma(p)$};
\node[circle,draw,fill=white] (U_i) at (2.5,0) {$-$};
\node[rectangle,draw,fill=white] (Ui-E) at (4,0) {$\frac{1}{R_i+L_ip}$};
\node[rectangle,draw,fill=white] (I_i) at (6,0) {$\braket{\frac{\partial M}{\partial \theta}}I_e$};
\node[circle,draw,fill=white] (gamma_em) at (8,0) {$-$};
\node[rectangle,draw,fill=white] (gamma) at (9.5,0) {$\frac{1}{Jp}$};
\node[right] at (10,0) {$\Omega$};

\node[above] at (8,0.5) {$\Gamma_{ch}$};
\node[rectangle,draw,fill=white] (MIe) at (5,-1.5) {$\braket{\frac{\partial M}{\partial \theta}}I_e$};
\node[rectangle,draw,fill=white] (MesureI_i) at (1,-0.75) {$\braket{\frac{\partial M}{\partial \theta}}I_e$};

\draw[->] (epsgamma) -- (H) node[midway, above]{$\varepsilon$};
\draw[->] (H) -- (U_i) node[midway, above]{$U_i$};
\draw[->] (U_i) -- (Ui-E);
\draw[->] (Ui-E) -- (I_i) node[midway, above]{$I_i$};
\draw[->] (I_i) -- (gamma_em) node[midway, above]{$\Gamma_{em}$};
\draw[->] (gamma_em) -- (gamma) node[midway, above]{$\Gamma$};
\draw[->] (gamma) -- (10,0);

\draw[->] (8,0.5) -- (gamma_em);

\draw[->] (9,0) -- ++(-0.25,0) |- (MIe);
\draw[->] (MIe) -| (U_i) node[midway, above left]{$E$};

\draw[->] (-1.5,0) -- (epsgamma);
\draw[->] (I_i) -- ++(-1,0) |- (MesureI_i);
\draw[->] (MesureI_i) -| (epsgamma)node[midway, above left]{$\widetilde{\Gamma}_{em}$};

\end{tikzpicture}
\end{center}
\shorthandon{:!}

Cette fois-ci, c'est une bonne architecture de commande. Dans les calculs, nous noterons $M = \braket{\frac{\partial M}{\partial \theta}}$ pour alléger les notations (notation consacrée mais particulièrement mal choisie, vous en conviendrez).

On dira que la boucle ouverte ($I_i$ fonction de $\varepsilon$) s'écrit :

\[
I_i(p) = \frac{C_\Gamma(p)\varepsilon(p)-E(p)}{R_i + L_i p}
\]

\[
I_i(p) = \frac{\frac{1}{R_i}(C_\Gamma(p)\varepsilon(p)-E(p))}{1 + \frac{L_i}{R_i} p}
\]

Et la boucle fermée ($I_i$ fonction de $\Gamma_c$) :

\[
\varepsilon(p) = \Gamma_c(p) - MI_eI_i(p)
\]

\[
I_i(p) = \frac{\frac{1}{R_i}\left(C_\Gamma(p)(\Gamma_c(p) - MI_eI_i(p))-E(p)\right)}{1 + \frac{L_i}{R_i} p}
\]

\[
\left(1 + \frac{L_i}{R_i} p + \frac{C_\Gamma(p)MI_e}{R_i}\right)I_i(p) = \frac{1}{R_i}\left(C_\Gamma(p)\Gamma_c(p)-E(p)\right)
\]

\[
I_i(p) = \frac{\frac{1}{R_i}\left(C_\Gamma(p)\Gamma_c(p)-E(p)\right)}{1 + \frac{L_i}{R_i} p + \frac{C_\Gamma(p)MI_e}{R_i}}
\]

Passons à la suite : l'enjeu est maintenant de choisir $C_\Gamma$ de façon pertinente.

Mais que veut dire ``façon pertinente'' ? Il y a principalement trois critères :

\begin{itemize}
\item La \emph{précision} qui indique l'erreur statique entre la valeur obtenue $\Gamma_{em}$ et la valeur désirée $\Gamma_c$.
\item La \emph{rapidité} qui est le reflet du temps qu'il faut à la sortie pour rejoindre une nouvelle commande.
\item La \emph{stabilité}, concept sur lequel nous reviendrons dans la prochaine section et qui sous-tend l'idée selon laquelle le moteur pourrait s'emballer si l'on régule n'importe comment.
\end{itemize}

Il faut également faire attention au grandeurs maximales admissibles par le système. Notamment, les documentations des machines à courant continu donnent un $\frac{dI_i}{dt}$ à ne pas dépasser sous peine de s'exposer à la rupture des isolations entre conducteurs \textcolor{red}{(vérifier ça)}.

Nous allons donc devoir imaginer $C_\Gamma$ de sorte à qu'il satisfasse au mieux ces trois critères et les contraintes de dimensionnement. Pour ce faire, nous allons mettre en place la méthode de la \emph{compensation de pôle}. L'idée est que la transformée de Laplace de la machine à courant continue est sous-optimale dans la valeur de ses grandeurs intrinsèques, et qu'on souhaiterait donc la modifier à notre guise sans en altérer la forme.

Pour ce faire, on cherchera donc à supprimer l'ancien pôle pour en introduire numériquement un nouveau, plus adaptée au mode de fonctionnement désiré de la machine, de façon à acroître ses performance par rapport à son comportement naturel : 

\[
I_i(p) = \frac{\frac{1}{R_i}\left(\frac{C_\Gamma(p)}{1 + \frac{L_i}{R_i} p}\Gamma_c(p)-\frac{E(p)}{1 + \frac{L_i}{R_i} p}\right)}{1 + \frac{MI_e}{R_i}\frac{C_\Gamma(p)}{1 +\frac{L_i}{R_i} p}}
\]

On choisi donc $C_\Gamma(p)$ proportionnel à $\frac{1 + \frac{L_i}{R_i} p}{p}$, selon un facteur $\frac{K_p}{\tau_i}$, de façon qu'on ai $C_\Gamma(p) = K_p + \frac{1}{\tau_ip}$ avec $K_p\tau_i = \frac{L_i}{R_i}$. La boucle fermée devient :

\[
I_i(p) = \frac{\frac{1}{R_i}\left(\frac{K_p}{\tau_ip}\Gamma_c(p)-\frac{E(p)}{1 + \frac{L_i}{R_i} p}\right)}{1 + \frac{MI_e}{R_i}\frac{K_p}{\tau_ip}}
\]

\[
I_i(p) = \frac{\frac{1}{MI_e}\left(\Gamma_c(p)-\frac{\tau_i}{K_p}\frac{E(p)}{1 + \frac{L_i}{R_i} p}\right)}{1 + \frac{R_i\tau_i}{MI_eK_p}p}
\]

Sans perturbation de $E$, la boucle fermée est devenue un filtre du premier ordre, comme l'était le système d'équation originel de la machine ! Le pôle est maintenant déplacé en $r = \frac{MI_eK_p}{R_i\tau_i}$, et donc réglable numériquement par le biais de $K_p$ et $\tau_i$ (il faut deux degrés de liberté pour conserver un produit $K_p\tau_i = \frac{L_i}{R_i}$ nécessaire à la compensation de l'ancien pôle).

La boucle ouverte s'écrit :

\[
I_i(p) = \frac{\frac{1}{R_i}\left(\left(K_p + \frac{1}{\tau_ip}\right)\varepsilon(p)-E(p)\right)}{1 + \frac{L_i}{R_i} p}
\]

\[
I_i(p) = \frac{\frac{1}{R_i\tau_ip}\left(1 + K_p\tau_ip\right)\varepsilon(p)}{1 + \frac{L_i}{R_i} p} 
- \frac{\frac{1}{R_i}E(p)}{1 + \frac{L_i}{R_i} p}
\]

\[
I_i(p) = \frac{\varepsilon(p)}{R_i\tau_ip} 
- \frac{\frac{1}{R_i}E(p)}{1 + \frac{L_i}{R_i} p}
\]

La compensation de pôle a fait disparaître le premier ordre en boucle ouverte au profit d'une intégration pure (on ne s'intéresse toujours pas à $E$ qui est donnée à titre indicatif mais supposée non-perturbée à nos échelles de temps, hypothèse émanante du fait que $E$ est proportionnelle à la vitesse $\Omega$ qui varie très lentement devant la constant de temps électrique $\frac{L_i}{R_i}$).

Les paramètres $K_p$ et $\tau_i$ permettent de régler la pente de la rampe d'intégration pour une erreur constante, c'est-à-dire la vitesse selon laquelle on veut que la machine réagisse. C'est alors qu'on se souvient qu'un des critères pour déterminer ces paramètres était la rapidité, et on comprend que le temps $\tau_i$ représente cette rapidité.

L'idée est donc d'avoir une pente de $I_i$ proportionnelle à l'erreur, plutôt que suivant une relation exponentielle comme le ferait naturellement la machine à courant continu sans correcteur.

On doit alors se rappeller qu'il y a une contrainte sur $\frac{dI_i}{dt}$, qui ne doit pas dépasser un certain maximum. L'erreur maximale déterminera la pente maximale, et ainsi on choisira :

\[
\tau_i = \frac{\varepsilon_{max}}{R_i\left(\frac{dI_i}{dt}\right)_{max}}
\]

Cependant, ce n'est pas la seule chose à laquelle il faut faire attention. Il reste la question de la précision et de la stabilité que nous n'avons pas encore évoqué.

La précision n'intervient pas dans ce problême, car la réponse en boucle fermée à un échelon de Heaviside :

\[
I_i(p) = \frac{\frac{\Gamma_c}{MI_e}}{p\left(1 + \frac{R_i\tau_i}{MI_eK_p}p\right)}
\]

\[
I_i(p) = \frac{A_1}{p} + \frac{A_2}{p+\frac{MI_eK_p}{R_i\tau_i}}
\]

\[
I_i(p) = \frac{\frac{\Gamma_c}{MI_e}}{p} - \frac{\frac{\Gamma_c}{MI_e}\frac{MI_eK_p}{R_i\tau_i}}{p+\frac{MI_eK_p}{R_i\tau_i}}
\]

Est une exponentielle décroissante sommée à un échelon, comme le met en évidence cette décomposition en éléments simples, et que cet échelon vaut $\frac{\Gamma_c}{MI_e}$ qui est bien le courant désiré dans l'induit.

La stabilité n'intervient pas non plus, car la boucle fermée est un filtre d'ordre $1$ à racine réelle négative que nous connaissons sous forme de montage électronique passif lequel ne peut apporter l'énergie nécessaire à l'existence d'une instabilité.

Notons que la fonction de transfert du correcteur étant l'addition d'une intégration et d'une amplification, il existe un filtre - éventuellement actif - qui permet son implémentation analogique. 

On doit également prendre en compte une contrainte sur la valeur maximale de $I_i$, à cause du point de fusion des isolants recouvrants les connecteurs. Cela nous amène à ajouter un saturateur derrière le correcteur.

\subsection{Régulation en vitesse}

\section{Systèmes dynamiques}

Cette section vient compléter celle dédiée aux équations différentielles non-linéaires qui figure dans le chapitre sur les équations différentielles. 

\subsection{Stabilité de Lyapunov}

\textcolor{red}{(clarifier cette section)}

Stabilité :

\[\boxed{
\forall \varepsilon > 0, \exists \delta > 0 : |X(0)|<\delta \Rightarrow \forall t>0, |X(t)|<\varepsilon
}\]

Stabilité asymptotique :

\[\boxed{
\exists \delta' : |X(0)|\leq \delta \Rightarrow \lim_{t\rightarrow \infty}|X(t)|=0
}\]


Théorème de Lyapunov :

Soit $\mathbb{D} \in \mathbb{R}^n : 0 \in \mathbb{D}$. Soit $V\in C^1(\mathbb{R}^n,\mathbb{R})$. Si :

\[\boxed{
\left\{\begin{matrix}
\forall x \in \mathbb{D} \setminus\{0\} \\
V(0)=0 \\
\forall X\in \mathbb{D}, (\nabla V f)(X) \leq 0
\end{matrix}\right.
}\]

Alors $X=0$ est stable pour le système et $V$ est une fonction de Lyapunov faible.

Si de plus $\forall x\in \mathbb{D}, (\nabla V f)(X) < 0$, $X=0$ est localement asymptotiquement stable.

Si en plus :

\[
\lim_{||x|| \rightarrow \infty} V(x) = \infty
\]

Alors $x=0$ est globalement asymptotiquement stable.


Exemples 1 :

\[
\frac{\partial X}{\partial x} = -x^3
\]

Soit $V : x\rightarrow x^2$, $(\nabla V f)(X) = -2x^4 <0 \forall x \neq 0$.

De plus $V$ est trivialement radialement non-bornée, et donc $x=0$ est globalement asymptotiquement stable.

Exemple du pendule :

\[
\frac{\partial}{\partial x}\left(\begin{matrix}
X_1 \\ X_2
\end{matrix}\right) = \left(\begin{matrix}
X_2 \\ -a \sin X_1
\end{matrix}\right)
\]

Soit $V : X\rightarrow a(1-\cos(X_1)) + \frac{1}{2}X_2^2$. Cette fonction s'annule tous les $X_1=2k\pi$.

\[
(\nabla V f)\left(\begin{matrix}
X_1 \\ X_2
\end{matrix}\right) = \left(\begin{matrix}
\frac{\partial V}{\partial X_1} & \frac{\partial V}{\partial X_2}
\end{matrix}\right) \left(\begin{matrix}
\frac{\partial X_1}{\partial x} \\ \frac{\partial X_2}{\partial x}
\end{matrix}\right)
\]

\[
(\nabla V f)\left(\begin{matrix}
X_1 \\ X_2
\end{matrix}\right) = \left(\begin{matrix}
-a \sin(X_1) & X_2
\end{matrix}\right) \left(\begin{matrix}
X_2 \\ -a \sin X_1
\end{matrix}\right)
\]

\[
(\nabla V f)\left(\begin{matrix}
X_1 \\ X_2
\end{matrix}\right) = -2a X_2 \sin X_1
\]

Exemple du pendule avec frottement :

\[
\frac{\partial}{\partial x}\left(\begin{matrix}
X_1 \\ X_2
\end{matrix}\right) = \left(\begin{matrix}
X_2 \\ -a \sin X_1 - b X_2
\end{matrix}\right)
\]

Soit :

\[
V : X\rightarrow a(1-\cos(X_1)) + \frac{1}{2}X^\top \left(\begin{matrix}
\frac{b^2}{2} & \frac{b}{2} \\
\frac{b}{2} & 1
\end{matrix}\right)X
\]

\[
(\nabla V f)\left(\begin{matrix}
X_1 \\ X_2
\end{matrix}\right) = -\frac{b}{2}X_2^2 - \frac{ab}{2}X_1 \sin X_1
\]

Exemple :

\[
\frac{\partial}{\partial x}\left(\begin{matrix}
X_1 \\ X_2
\end{matrix}\right) = \left(\begin{matrix}
-X_1+X_2^2 \\ -X_2^2
\end{matrix}\right)
\]

\[
V : X\rightarrow \frac{1}{2}(X_1^2+X_2^2)
\]

Dans le cas linéaire, le théorème de Lyapunov devient :

\textcolor{red}{(compléter)}

Matrice définie positive : $X^\top P X >0 \forall X$.

Exemple pendule avec frottement :

\[
\nabla f = \left(\begin{matrix}
0 & 1 & \\ a\cos X_1 & -b 
\end{matrix}\right)
\]

Principe de LaSalle :

\[
\forall x \in \mathbb{D},  (\nabla V f)\left(\begin{matrix}
X_1 \\ X_2
\end{matrix}\right)\leq \alpha(x)\leq 0, \alpha \in \mathcal{C}^1, E = \{x\in \mathbb{D} : \alpha(x) = 0\}
\]

Alors quelque soit le condition initiale l'état tend vers $M$ est le plus grand enseble invariant dans $E$.

\chapter{Théorie des graphes}

L'objet de ce chapitre est un sujet fascinant, à l'intersection des mathématiques profondes et de informatique théorique, mais trouvant de très naturelles applications en ingénierie. 

Il est le premier représentant historique de la \emph{recherche opérationnelle}, qui est une sélection de disciplines scientifiques ayant pour ambitieuse vocation de permettre à chacun de faire les meilleurs choix possibles ; tant que la science s'autorise à nous dire qu'un tel choix existe. Elle s'étend au delà de l'ingénierie, car où elle peut aboutir au perfectionnement de modes d'organisation sociétaux ou intervenir en gestion de projet. 

Le prochain chapitre forme l'autre versant principal de la recherche opérationnelle, mais vous allez-voir qu'il sont assez remarquablement imbriqués l'un dans l'autre.

\section{Graphe}

\subsection{Formulation générale}

\subsubsection{Définition}

Un \emph{graphe} est une structure mathématique comportant simplement un ensemble de \emph{sommets} $X$ et une fonction successeur $\Gamma$ allant de $X$ dans $\mathcal{P}(X)$.

Il est dit \emph{non-orienté} si :

\[\boxed{
\forall (x,y)\in X^2, \; y\in\Gamma(x) \Leftrightarrow x\in\Gamma(y)
}\]

Et \emph{complet} si : 

\[\boxed{
\forall (x,y)\in X^2, \; y\in\Gamma(x) \vee x\in\Gamma(y)
}\]

L'ensemble constitué des paires :

\[\boxed{
(x,y)\in X^2 : y\in\Gamma(x)
}\]

Sont les \emph{arêtes} du graphe.

\subsubsection{Représentation matricielle}

On défini la \emph{matrice d'adjacence} par :

\[\boxed{
\forall(x_i, x_j)\in X^2, \; \left\{\begin{matrix}
M_{ij} = 1 \Leftrightarrow \; x_j\in \Gamma(x_i)\\
M_{ij} = 0 \Leftrightarrow \; x_j\notin \Gamma(x_i)
\end{matrix}\right.
}\]

Qui est la façon habituelle de coder un graph au travers des arêtes qu'il comprend.

Ces matrices ne correspondent pas tout à fait à la structure d'algèbre matricielle habituelle, car l'addition naturelle y est remplacée par la multiplication terme-à-terme (qui se ramène à un ET logique, ou encore à l'intersection des graphes). C'est à dire que :

\[\boxed{
(M\cap N)_i^j = (M)_i^j \times (N)_i^j
}\]

\textcolor{red}{(théorie spectrale)}

\subsection{Chaîne, chemin, cycle et circuit}

\subsubsection{Définition \& existence}

Une \emph{chaîne} $\mu$ est une suite de sommets reliés par des arêtes. Un \emph{chemin} est une chaîne où toutes les arêtes vont dans le bon sens, c'est à dire que $\forall i>0, \; \mu_{i+1} \in \Gamma(\mu_i)$.

Un \emph{cycle} est une chaîne reliant un point à lui-même, et on le qualifiera de \emph{circuit} s'il est de plus un chemin.

\textcolor{red}{(figure)}

On peut alors introduire la relation binaire $\mathcal{R}$ qui nous dit si deux sommets sont reliés par une chaîne (ou un chemin suivant le problême auquel on s'intéresse) ou non.

Cette relation est une relation d'équivalence, au sens qu'on lui a donné dans le premier chapitre \textcolor{red}{(pas fait)}, c'est à dire qu'elle est transitive, réflexive et symétrique.

Revenons à nos matrices. La structure que nous avons construite en remplaçant l'addition par l'intersection est toujours une algèbre, mais nous allons voir que la multiplication matricielle y prend un sens très intéressant :

\[
(M\times N)_i^j = M_i^k N_k^j
\]

Et notamment :

\[
({M^2})_i^j = M_i^k M_k^j
\]

Qui donne l'existence des chemins de longueur $2$ entre les sommets d'incide $i$ et $j$, et ainsi de suite pour toute puissance de $M$. Il suffit d'intersecter toutes les puissances de $M$ pour obtenir la matrice $C$ d'existence des chemins :

\[\boxed{
C = \bigcap_n M^n
}\]

On pourra arrêter $n$ au nombre de sommets que le graphe comprend. De plus, on pourra s'arrêter dès lors que deux matrices successives seront identiques.

\subsubsection{Connexité}

Un graph est dit \emph{connexe} si :

\[\boxed{
\forall (x,y), \; \exists \mu : x\in \mu \wedge y\in \mu
}\]

C'est-à-dire que le graph est en un seul morceau. Il est dit \emph{fortement connexe} si $\mu$ peut-être toujours choisie de sorte qu'elle soit un chemin.

Pour tester la connexité, il suffit de calculer la matrice d'existence des chemins $C$ et de vérifier qu'elle est partout unitaire.

\subsubsection{Parcours eulérien et hamiltonien}

Dans un graphe, on dit qu'une chaîne ou un cycle est \emph{eulérien} s'il passe une unique fois par toutes les arêtes du graphe, et \emph{hamiltonien} s'il passe une unique fois par tous ses sommets.

Un graphe eulérien ou hamiltonien est un graphe contenant au moins un cycle (ou circuit) eulérien ou hamiltonien.

On peut montrer \textcolor{red}{(à faire, par l'absurde je crois)} qu'un graphe est eulérien au sens des cycles si tous ses sommets ont un nombre pair d'arêtes, et au sens des circuits s'il y a autant d'arrêtes entrantes que sortantes.

\textcolor{red}{(discussion de la recherche de cycle hamitonien, quoi qu'apparement on n'en connaisse pas dixit Maquin)}

\section{Réseaux}

La théorie des graphes telle qu'elle vient d'être présentée n'est pas la partie la plus utile aux ingénieurs, politiques et gestionnaires. C'est la notion de \emph{réseau} - au sens de la théorie des graphes - qui va ouvrir un champ d'application qu'on pourrait qualifier de démentiel de par son ampleur.

On attribue à chaque arête un \emph{poids} $l$ (ou une longueur suivant le vocabulaire qu'on préfère). Cette variable peut-être n'importe quelle grandeur physique de flux caractéristique d'une connexion entre deux points : une débit de fluide ou d'information maximal admissible, une puissance active ou réactive, etc.

Tout un tas de problêmes peuvent se poser lors de la conception ou le contrôle d'un réseau, et nous allons donner dans cette section des solutions à un échantillon de ces problêmes. Nous nous limiterons aux réseaux acycliques pour nous contenter d'exemples élémentaires.

\subsection{Principe de Bellman}

Faire le meilleur choix - l'objet de la recherche opérationnelle - consiste bien souvent à \emph{optimiser} des grandeurs. Par exemple, on va chercher à réduire un coût, un temps, une consommation, bref à économiser des ressources.

Le cadre formel général de la théorie de l'optimisation sera présentée dans le prochain chapitre, mais nous commençons à en faire dès à présent. L'idée est qu'un grand nombre de systèmes physiques ou des situations peuvent-être modélisés par le biais de réseaux, et qu'inventer des méthodes d'optimisation propres à ceux-ci permet de couvrir un grand nombre de problêmes auxquels les hommes peuvent-être confrontés. On comprend donc qu'une branche spécifique de la recherche opérationnelle lui soit dédiée.

Le principe de Bellman est central aussi bien en optimisation générale qu'en théorie des graphes. Il s'énonce comme suit :

\begin{it}
Si un premier choix $u_1^*$ est optimal pour un critère $J$ en vue de la réalisation d'un projet $X_f$, alors il suffira de continuer avec un choix optimal $u_2^*$ permettant d'atteindre $X_f$ en optimisant $J_2$ pour qu'on puisse dire que les meilleures décisions aient été prises ($J^* = J^*_1 + J^*_2$ sera la valeur optimale de $J$).
\end{it}

Autrement dit, la réalisation parfaite n'est qu'une succession de choix individuels optimaux. Pour savoir quel est le meilleur prochain coup, il suffit donc d'explorer toutes les voies possibles à plus long terme, en prenant à chaqué étape pour acquise une certaine \emph{carte} des optimalités passées.

Ce principe est presque une doctrine de la vie, qui nous dit que pour faire les bons choix la première chose est d'éviter les situations qui ne vous permettent pas d'atteindre vos plus grandes ambitions. Quand vous voulez quelque chose de tout votre coeur, n'envisagez pas toutes les possibilités ou même seulement celles qui vous semblent les plus profitables à court terme mais commencez par réfléchir à quelles sont les voies qui ne mennent pas à votre but ultime ; et n'y mettez pas les pieds.

En informatique, il va permettre de réduire considérablement le temps de calcul de certains problêmes par rapport à une exploration exhaustive de toutes les possibilités, en excluant à chaque itération celles qui ne mènent manifestement pas à l'objectif.

\subsection{Recherche du plus court chemin}

Le problême du plus court chemin dans un réseau s'écrit entre un sommet $a$ et un sommet $b$ :

\[
\left\{\begin{matrix}
\min J = \min \sum_{i\in\mu} l(i) \\
\mu_0 = a \wedge \mu_f = b
\end{matrix}\right.
\]

Avec $\mu_f$ une notation pour désigner son dernier élément $\mu_{(\# \mu-1)}$.

L'algorithme qui va être décrit - dit de \emph{Moore-Dijkstra} - pourra aisément être adapté au calcul de la \emph{matrice des plus courts chemins}. C'est un algorithme de ce genre qui calcule pour nous la route à suivre dans un GPS.

Connaissant un ensemble $\mathbb{M}$ de chaînes permettant pour chacune d'entre elle d'atteindre un sommet différent $X$. On explore toutes les arêtes sortantes de chacun des sommets atteint.

Pour appliquer le principe de Bellman, on considère les sommets ayant plusieurs chemins incidents et on se permet d'oublier les voies les moins profitables. On procède par itération jusqu'à remplir tout le graphe.

Le principe de Bellman consiste donc à faire le ménage dans $\mathbb{M}$ en permanence. A chaque itération, $\mathbb{M}$ est remplacée par :

\[
\mathbb{M}' = \Gamma(\mathbb{M})\setminus \{\mu : \; \exists \nu : \; \mu_f \in \nu \wedge J(\mu) > J(\nu)\}
\]

\textcolor{red}{(vérifier ça c'est peut-être pas tout à fait exact mais l'idée est là)}

Avec $\Gamma(\mathbb{M})$ les chaînes pouvant être atteintes en faisant un pas de plus. Une fois le graphe complété, le sommet $b$ est alors nécessairement atteint par un unique chemin qui est le plus court chemin.

\subsection{Ordonnancement}

Le plus long chemin entre un sommet $a$ et un sommet $b$ d'un réseau a une signification bien précise en gestion de projet : sa longueur correspond au temps qu'il faut pour avoir atteint tous les jalons préalables à la réalisation de l'étape $b$, puis d'avoir complété celle-ci (les longueurs des arêtes étant les durées de chaque tâche).

Une étape (un sommet) peut-être associée à une \emph{date au plus tôt}, qui est le plus long chemin allant du sommet initiale à lui (impossible d'arriver à cette étape avant cette date), et d'une \emph{date au plus tard} à laquelle l'étape doit-être réalisée pour ne pas retarder le projet global, et qui est la durée minimale de ce projet soustraite de la longueur du plus long chemin le séparant de la dernière étape.

Evidemment, le principe de Bellman s'écrit comme le fait que ces deux dates coïncident sur le plus long chemin global (appellé \emph{chemin critique}).

\textcolor{red}{(compléter, éclaircir et vérifier)}

\subsection{Problême du postier chinois \& du voyageur de commerce}

\textcolor{red}{(compléter page 17 cours de Maquin)}

\subsection{Arbre couvrant de poids minimum}

Un arbre est un réseau sans cycle.

L'objet de ce chapitre est, pour un réseau connexe donné, de trouver l'arbre couvrant $A$ - c'est-à-dire le sous-réseau sans cycle qui passerait par tous les sommets du réseau - qui minimiserait la somme des longueurs de chaque arête qu'il fait intervenir :

\[
\left\{\begin{matrix}
\min J = \min \sum_{i\in A} l(i) \\
A \; couvrant
\end{matrix}\right.
\]

La nature de l'objet qu'on cherche est différente de ce qu'on a fait jusqu'à présent : ce n'est plus un chemin mais un arbre (sous-réseau du réseau donné initialement) qui sera solution de ce problême.

De fait, nous aurons recours à un algorithme dit \emph{glouton} qui partira d'un sommet $X_0$ pour trouver une solution correspondante à un minimum local de $J$.

Le problême avec cet algorithme est qu'il ne garanti pas que la solution trouvée soit réellement la solution du problême, mais seulement celle restreinte à un ensemble de possibilités semblables les unes aux autres - celles que l'algorithme a exploré en partant du sommet $X_0$.

La solution n'est donc pas la meilleure qui soit, mais on a des chances pour qu'elle ne soit pas complètement mauvaise non plus.

A partir d'un arbre $A$, on construit un autre arbre incluant les sommets atteignables, mais en choissant toujours les voies les plus économiques pour atteindre les nouveaux sommets, comme le préconise la programmation dynamique.

\textcolor{red}{(compléter, il me semble que cet algorithme est meilleur que celui de Prim)}

\subsection{Recherche du flot maximal}

Soit un réseau, on cherche un autre réseau sur lequel s'appliquerait la loi de Kirchoff, c'est-à-dire qu'un sommet est en fait un \emph{noeud} et que la somme orienté des longueurs de ses arêtes est nulle (loi de Kirchoff). En outre, on impose qu'aucune arête de ce nouveau réseau ne soit affectée d'une longueur supérieure à celle de l'ancien.

Bien sûr, ce ne sont plus des longueurs qui interviennent dans ce nouveau réseau mais plutôt des débits ou puissances, et l'ensemble des valeurs attribuées à ses arêtes est un \emph{flot}. L'ancien réseau contient en fait l'information sur les grandeures maximales admissibles par les canalisations et câbles.

L'objet de ce problême est - pour un réseau donné et dimensionné - d'optimiser le transport d'un sommet $A$ à un sommet $B$, on parlera alors de flot maximal.

L'algorithme de \emph{Ford-Fulkerson} que nous allons voir s'appuie naturellement sur la programmation dynamique \textcolor{red}{(à vérifier)}.

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
\node[circle,draw,fill=white] (E) at (0,0) {$E$};
\node[circle,draw,fill=white] (a) at (2,2) {$a$};
\node[circle,draw,fill=white] (b) at (2,0) {$b$};
\node[circle,draw,fill=white] (c) at (4,2) {$c$};
\node[circle,draw,fill=white] (d) at (4,0) {$d$};
\node[circle,draw,fill=white] (e) at (2,-2) {$e$};
\node[circle,draw,fill=white] (f) at (4,-2) {$f$};
\node[circle,draw,fill=white] (g) at (5,1) {$g$};
\node[circle,draw,fill=white] (S) at (6,0) {$S$};


\draw[->,blue] (E) -- (a) node[midway, above left]{$\textcolor{blue}{5}\textcolor{black}{,0}$};
\draw[->,blue] (a) -- (c) node[midway, above]{$\textcolor{blue}{7}\textcolor{black}{,0}$};
\draw[->,blue] (c) -- (g) node[midway, above right]{$\textcolor{blue}{7}\textcolor{black}{,0}$};
\draw[->,blue] (g) -- (S) node[midway, above right]{$\textcolor{blue}{10}\textcolor{black}{,0}$};
\draw[->,blue] (E) -- (b) node[midway, above]{$\textcolor{blue}{13}\textcolor{black}{,0}$};
\draw[->,blue] (b) -- (c) node[midway, below left]{$\textcolor{blue}{8}\textcolor{black}{,0}$};
\draw[->,blue] (a) -- (d) node[midway, above left]{$\textcolor{blue}{10}\textcolor{black}{,0}$};
\draw[->,blue] (b) -- (d) node[midway, below]{$\textcolor{blue}{2}\textcolor{black}{,0}$};
\draw[->,blue] (d) -- (g) node[midway, above left]{$\textcolor{blue}{4}\textcolor{black}{,0}$};
\draw[->,blue] (d) -- (S) node[midway, above]{$\textcolor{blue}{6}\textcolor{black}{,0}$};
\draw[->,blue] (E) -- (e) node[midway, below left]{$\textcolor{blue}{8}\textcolor{black}{,0}$};
\draw[->,blue] (b) -- (e) node[midway, left]{$\textcolor{blue}{1}\textcolor{black}{,0}$};
\draw[->,blue] (e) -- (d) node[midway]{$\textcolor{blue}{2}\textcolor{black}{,0}$};
\draw[->,blue] (e) -- (f) node[midway, above]{$\textcolor{blue}{4}\textcolor{black}{,0}$};
\draw[->,blue] (d) -- (f) node[midway, right]{$\textcolor{blue}{2}\textcolor{black}{,0}$};
\draw[->,blue] (f) -- (S) node[midway, below right]{$\textcolor{blue}{6}\textcolor{black}{,0}$};

\end{tikzpicture}
\end{center}
\shorthandon{:!}

Les deux graphes sont ici co-représentés : les grandeurs maximales admissibles sont en bleu, et celui que nous allons construire est initialisé à $0$.

Le but étant de trouver le flot maximisant le transport entre $E$ et $S$.

Avant d'appliquer Ford-Fulkerson proprement dit, on commence par exécuter un algorithme glouton permettant d'établir un flot \emph{complet}, c'est-à-dire pour lequel aucune chaîne pouvant débiter plus n'existe.

Prenons la chemin situé tout en haut. On peut au maximum faire passer $5$ unités, l'arc limitant étant $E-a$.

On peut alors enlever des unités au premier graphe pour les donner au second. La branche est maintenant saturée.

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
\node[circle,draw,fill=white] (E) at (0,0) {$E$};
\node[circle,draw,fill=white] (a) at (2,2) {$a$};
\node[circle,draw,fill=white] (b) at (2,0) {$b$};
\node[circle,draw,fill=white] (c) at (4,2) {$c$};
\node[circle,draw,fill=white] (d) at (4,0) {$d$};
\node[circle,draw,fill=white] (e) at (2,-2) {$e$};
\node[circle,draw,fill=white] (f) at (4,-2) {$f$};
\node[circle,draw,fill=white] (g) at (5,1) {$g$};
\node[circle,draw,fill=white] (S) at (6,0) {$S$};


\draw[->,red] (E) -- (a) node[midway, above left]{$\textcolor{black}{0,}\textcolor{red}{5}$};
\draw[->] (a) -- (c) node[midway, above]{$\textcolor{black}{2}\textcolor{black}{,5}$};
\draw[->] (c) -- (g) node[midway, above right]{$\textcolor{black}{2}\textcolor{black}{,5}$};
\draw[->] (g) -- (S) node[midway, above right]{$\textcolor{black}{5}\textcolor{black}{,5}$};
\draw[->,blue] (E) -- (b) node[midway, above]{$\textcolor{blue}{13}\textcolor{black}{,0}$};
\draw[->,blue] (b) -- (c) node[midway, below left]{$\textcolor{blue}{8}\textcolor{black}{,0}$};
\draw[->,blue] (a) -- (d) node[midway, above left]{$\textcolor{blue}{10}\textcolor{black}{,0}$};
\draw[->,blue] (b) -- (d) node[midway, below]{$\textcolor{blue}{2}\textcolor{black}{,0}$};
\draw[->,blue] (d) -- (g) node[midway, above left]{$\textcolor{blue}{4}\textcolor{black}{,0}$};
\draw[->,blue] (d) -- (S) node[midway, above]{$\textcolor{blue}{6}\textcolor{black}{,0}$};
\draw[->,blue] (E) -- (e) node[midway, below left]{$\textcolor{blue}{8}\textcolor{black}{,0}$};
\draw[->,blue] (b) -- (e) node[midway, left]{$\textcolor{blue}{1}\textcolor{black}{,0}$};
\draw[->,blue] (e) -- (d) node[midway]{$\textcolor{blue}{2}\textcolor{black}{,0}$};
\draw[->,blue] (e) -- (f) node[midway, above]{$\textcolor{blue}{4}\textcolor{black}{,0}$};
\draw[->,blue] (d) -- (f) node[midway, right]{$\textcolor{blue}{2}\textcolor{black}{,0}$};
\draw[->,blue] (f) -- (S) node[midway, below right]{$\textcolor{blue}{6}\textcolor{black}{,0}$};
\end{tikzpicture}
\end{center}
\shorthandon{:!}

On procède ainsi succéssivement en testant la saturation de tous les chemins possibles allant de $A$ vers $B$. L'algorithme est glouton car dépend de l'ordre de traitement des chemins, et rien ne laisse présager que le flot obtenu soit optimal mais il est effectivement complet : 

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
\node[circle,draw,fill=white] (E) at (0,0) {$E$};
\node[circle,draw,fill=white] (a) at (2,2) {$a$};
\node[circle,draw,fill=white] (b) at (2,0) {$b$};
\node[circle,draw,fill=white] (c) at (4,2) {$c$};
\node[circle,draw,fill=white] (d) at (4,0) {$d$};
\node[circle,draw,fill=white] (e) at (2,-2) {$e$};
\node[circle,draw,fill=white] (f) at (4,-2) {$f$};
\node[circle,draw,fill=white] (g) at (5,1) {$g$};
\node[circle,draw,fill=white] (S) at (6,0) {$S$};


\draw[->,red] (E) -- (a) node[midway, above left]{$\textcolor{black}{0,}\textcolor{red}{5}$};
\draw[->] (a) -- (c) node[midway, above]{$\textcolor{black}{2},\textcolor{black}{5}$};
\draw[->,red] (c) -- (g) node[midway, above right]{$\textcolor{black}{0,}\textcolor{red}{7}$};
\draw[->,red] (g) -- (S) node[midway, above right]{$\textcolor{black}{0,}\textcolor{red}{10}$};
\draw[->] (E) -- (b) node[midway, above]{$\textcolor{black}{8},\textcolor{black}{5}$};
\draw[->] (b) -- (c) node[midway, below left]{$\textcolor{black}{6},\textcolor{black}{2}$};
\draw[->,blue] (a) -- (d) node[midway, above left]{$\textcolor{blue}{10}\textcolor{black}{,0}$};
\draw[->,red] (b) -- (d) node[midway, below]{$\textcolor{black}{0,}\textcolor{red}{2}$};
\draw[->] (d) -- (g) node[midway, above left]{$\textcolor{black}{1},\textcolor{black}{3}$};
\draw[->] (d) -- (S) node[midway, above]{$\textcolor{black}{5},\textcolor{black}{1}$};
\draw[->] (E) -- (e) node[midway, below left]{$\textcolor{black}{3},\textcolor{black}{5}$};
\draw[->,red] (b) -- (e) node[midway, left]{$\textcolor{black}{0,}\textcolor{red}{1}$};
\draw[->,red] (e) -- (d) node[midway]{$\textcolor{black}{0,}\textcolor{red}{2}$};
\draw[->,red] (e) -- (f) node[midway, above]{$\textcolor{black}{0,}\textcolor{red}{4}$};
\draw[->,blue] (d) -- (f) node[midway, right]{$\textcolor{blue}{2}\textcolor{black}{,0}$};
\draw[->] (f) -- (S) node[midway, below right]{$\textcolor{black}{2},\textcolor{black}{4}$};

\end{tikzpicture}
\end{center}
\shorthandon{:!}

Le critère à optimiser vaut ici $J=15$.

Le flot n'est pas maximal, car il existe un chaîne passant uniquement par des arcs non-saturés comptés positivement, et non-vides comptés négativement :

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
\node[circle,draw,fill=white] (E) at (0,0) {$E$};
\node[circle,draw,fill=white] (a) at (2,2) {$a$};
\node[circle,draw,fill=white] (b) at (2,0) {$b$};
\node[circle,draw,fill=white] (c) at (4,2) {$c$};
\node[circle,draw,fill=white] (d) at (4,0) {$d$};
\node[circle,draw,fill=white] (e) at (2,-2) {$e$};
\node[circle,draw,fill=white] (f) at (4,-2) {$f$};
\node[circle,draw,fill=white] (g) at (5,1) {$g$};
\node[circle,draw,fill=white] (S) at (6,0) {$S$};


\draw[->,red] (E) -- (a) node[midway, above left]{$\textcolor{black}{0,}\textcolor{red}{5}$};
\draw[->,very thick] (a) -- (c) node[midway, above]{$\textcolor{black}{2},\textcolor{black}{5}$};
\draw[->,red] (c) -- (g) node[midway, above right]{$\textcolor{black}{0,}\textcolor{red}{7}$};
\draw[->,red] (g) -- (S) node[midway, above right]{$\textcolor{black}{0,}\textcolor{red}{10}$};
\draw[->,very thick] (E) -- (b) node[midway, above]{$\textcolor{black}{8},\textcolor{black}{5}$};
\draw[->,very thick] (b) -- (c) node[midway, below left]{$\textcolor{black}{6},\textcolor{black}{2}$};
\draw[->,blue,very thick] (a) -- (d) node[midway, above left]{$\textcolor{blue}{10}\textcolor{black}{,0}$};
\draw[->,red] (b) -- (d) node[midway, below]{$\textcolor{black}{0,}\textcolor{red}{2}$};
\draw[->] (d) -- (g) node[midway, above left]{$\textcolor{black}{1},\textcolor{black}{3}$};
\draw[->,very thick] (d) -- (S) node[midway, above]{$\textcolor{black}{5},\textcolor{black}{1}$};
\draw[->] (E) -- (e) node[midway, below left]{$\textcolor{black}{3},\textcolor{black}{5}$};
\draw[->,red] (b) -- (e) node[midway, left]{$\textcolor{black}{0,}\textcolor{red}{1}$};
\draw[->,red] (e) -- (d) node[midway]{$\textcolor{black}{0,}\textcolor{red}{2}$};
\draw[->,red] (e) -- (f) node[midway, above]{$\textcolor{black}{0,}\textcolor{red}{4}$};
\draw[->,blue] (d) -- (f) node[midway, right]{$\textcolor{blue}{2}\textcolor{black}{,0}$};
\draw[->] (f) -- (S) node[midway, below right]{$\textcolor{black}{2},\textcolor{black}{4}$};

\end{tikzpicture}
\end{center}
\shorthandon{:!}

Dans cette situation, on voit que l'arête $a-c$ peut être délestée au profit de la chaîne $a-d-S$. Cela est rendu possible du fait qu'on puisse augmenter le flot dans $E-b-c$ pour compenser le défaut pris dans $a-c$ :

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
\node[circle,draw,fill=white] (E) at (0,0) {$E$};
\node[circle,draw,fill=white] (a) at (2,2) {$a$};
\node[circle,draw,fill=white] (b) at (2,0) {$b$};
\node[circle,draw,fill=white] (c) at (4,2) {$c$};
\node[circle,draw,fill=white] (d) at (4,0) {$d$};
\node[circle,draw,fill=white] (e) at (2,-2) {$e$};
\node[circle,draw,fill=white] (f) at (4,-2) {$f$};
\node[circle,draw,fill=white] (g) at (5,1) {$g$};
\node[circle,draw,fill=white] (S) at (6,0) {$S$};


\draw[->,red] (E) -- (a) node[midway, above left]{$\textcolor{black}{0,}\textcolor{red}{5}$};
\draw[->,blue] (a) -- (c) node[midway, above]{$\textcolor{blue}{7}\textcolor{black}{,0}$};
\draw[->,red] (c) -- (g) node[midway, above right]{$\textcolor{black}{0,}\textcolor{red}{7}$};
\draw[->,red] (g) -- (S) node[midway, above right]{$\textcolor{black}{0,}\textcolor{red}{10}$};
\draw[->] (E) -- (b) node[midway, above]{$\textcolor{black}{3},\textcolor{black}{10}$};
\draw[->] (b) -- (c) node[midway, below left]{$\textcolor{black}{1},\textcolor{black}{7}$};
\draw[->] (a) -- (d) node[midway, above left]{$\textcolor{black}{5},\textcolor{black}{5}$};
\draw[->,red] (b) -- (d) node[midway, below]{$\textcolor{black}{0,}\textcolor{red}{2}$};
\draw[->] (d) -- (g) node[midway, above left]{$\textcolor{black}{1},\textcolor{black}{3}$};
\draw[->,red] (d) -- (S) node[midway, above]{$\textcolor{black}{0,}\textcolor{red}{6}$};
\draw[->] (E) -- (e) node[midway, below left]{$\textcolor{black}{3},\textcolor{black}{5}$};
\draw[->,red] (b) -- (e) node[midway, left]{$\textcolor{black}{0,}\textcolor{red}{1}$};
\draw[->,red] (e) -- (d) node[midway]{$\textcolor{black}{0,}\textcolor{red}{2}$};
\draw[->,red] (e) -- (f) node[midway, above]{$\textcolor{black}{0,}\textcolor{red}{4}$};
\draw[->,blue] (d) -- (f) node[midway, right]{$\textcolor{blue}{2}\textcolor{black}{,0}$};
\draw[->] (f) -- (S) node[midway, below right]{$\textcolor{black}{2},\textcolor{black}{4}$};

\end{tikzpicture}
\end{center}
\shorthandon{:!}

Ce flot est maximal, pour un critère $J=20$, car il n'existe aucune chaîne respectant la propriété précédente. L'algorithme de Ford-Fulkerson prend fin après une seule itération.

Néanmoins, la chaîne suivante vous aura peut-être tapé dans l'oeil :

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
\node[circle,draw,fill=white] (E) at (0,0) {$E$};
\node[circle,draw,fill=white] (a) at (2,2) {$a$};
\node[circle,draw,fill=white] (b) at (2,0) {$b$};
\node[circle,draw,fill=white] (c) at (4,2) {$c$};
\node[circle,draw,fill=white] (d) at (4,0) {$d$};
\node[circle,draw,fill=white] (e) at (2,-2) {$e$};
\node[circle,draw,fill=white] (f) at (4,-2) {$f$};
\node[circle,draw,fill=white] (g) at (5,1) {$g$};
\node[circle,draw,fill=white] (S) at (6,0) {$S$};


\draw[->,red] (E) -- (a) node[midway, above left]{$\textcolor{black}{0,}\textcolor{red}{5}$};
\draw[->,blue,very thick] (a) -- (c) node[midway, above]{$\textcolor{blue}{7}\textcolor{black}{,0}$};
\draw[->,red] (c) -- (g) node[midway, above right]{$\textcolor{black}{0,}\textcolor{red}{7}$};
\draw[->,red] (g) -- (S) node[midway, above right]{$\textcolor{black}{0,}\textcolor{red}{10}$};
\draw[->,very thick] (E) -- (b) node[midway, above]{$\textcolor{black}{3},\textcolor{black}{10}$};
\draw[->,very thick] (b) -- (c) node[midway, below left]{$\textcolor{black}{1},\textcolor{black}{7}$};
\draw[->,very thick] (a) -- (d) node[midway, above left]{$\textcolor{black}{5},\textcolor{black}{5}$};
\draw[->,red] (b) -- (d) node[midway, below]{$\textcolor{black}{0,}\textcolor{red}{2}$};
\draw[->] (d) -- (g) node[midway, above left]{$\textcolor{black}{1},\textcolor{black}{3}$};
\draw[->,red] (d) -- (S) node[midway, above]{$\textcolor{black}{0,}\textcolor{red}{6}$};
\draw[->] (E) -- (e) node[midway, below left]{$\textcolor{black}{3},\textcolor{black}{5}$};
\draw[->,red] (b) -- (e) node[midway, left]{$\textcolor{black}{0,}\textcolor{red}{1}$};
\draw[->,red] (e) -- (d) node[midway]{$\textcolor{black}{0,}\textcolor{red}{2}$};
\draw[->,red] (e) -- (f) node[midway, above]{$\textcolor{black}{0,}\textcolor{red}{4}$};
\draw[->,blue,very thick] (d) -- (f) node[midway, right]{$\textcolor{blue}{2}\textcolor{black}{,0}$};
\draw[->,very thick] (f) -- (S) node[midway, below right]{$\textcolor{black}{2},\textcolor{black}{4}$};

\end{tikzpicture}
\end{center}
\shorthandon{:!}

L'arête qui pose problême est $a-c$, qu'on ne peut pas vider plus qu'elle ne l'est déjà. Mais suivant le système physique qu'on considère, il peut-être envisagé de mettre en place une liaison $a-c$ bidirectionnelle, ou simplement d'échanger l'appareillage qui se trouve en $a$ avec celui en $c$ :

\shorthandoff{:!}
\begin{center}
\begin{tikzpicture}
\node[circle,draw,fill=white] (E) at (0,0) {$E$};
\node[circle,draw,fill=white] (a) at (2,2) {$a$};
\node[circle,draw,fill=white] (b) at (2,0) {$b$};
\node[circle,draw,fill=white] (c) at (4,2) {$c$};
\node[circle,draw,fill=white] (d) at (4,0) {$d$};
\node[circle,draw,fill=white] (e) at (2,-2) {$e$};
\node[circle,draw,fill=white] (f) at (4,-2) {$f$};
\node[circle,draw,fill=white] (g) at (5,1) {$g$};
\node[circle,draw,fill=white] (S) at (6,0) {$S$};


\draw[->,red] (E) -- (a) node[midway, above left]{$\textcolor{black}{0,}\textcolor{red}{5}$};
\draw[->] (c) -- (a) node[midway, above]{$\textcolor{black}{6},\textcolor{black}{1}$};
\draw[->,red] (c) -- (g) node[midway, above right]{$\textcolor{black}{0,}\textcolor{red}{7}$};
\draw[->,red] (g) -- (S) node[midway, above right]{$\textcolor{black}{0,}\textcolor{red}{10}$};
\draw[->] (E) -- (b) node[midway, above]{$\textcolor{black}{2},\textcolor{black}{11}$};
\draw[->,red] (b) -- (c) node[midway, below left]{$\textcolor{black}{0,}\textcolor{red}{8}$};
\draw[->] (a) -- (d) node[midway, above left]{$\textcolor{black}{4},\textcolor{black}{6}$};
\draw[->,red] (b) -- (d) node[midway, below]{$\textcolor{black}{0,}\textcolor{red}{2}$};
\draw[->] (d) -- (g) node[midway, above left]{$\textcolor{black}{1},\textcolor{black}{3}$};
\draw[->,red] (d) -- (S) node[midway, above]{$\textcolor{black}{0,}\textcolor{red}{6}$};
\draw[->] (E) -- (e) node[midway, below left]{$\textcolor{black}{3},\textcolor{black}{5}$};
\draw[->,red] (b) -- (e) node[midway, left]{$\textcolor{black}{0,}\textcolor{red}{1}$};
\draw[->,red] (e) -- (d) node[midway]{$\textcolor{black}{0,}\textcolor{red}{2}$};
\draw[->,red] (e) -- (f) node[midway, above]{$\textcolor{black}{0,}\textcolor{red}{4}$};
\draw[->] (d) -- (f) node[midway, right]{$\textcolor{black}{1},\textcolor{black}{1}$};
\draw[->] (f) -- (S) node[midway, below right]{$\textcolor{black}{1},\textcolor{black}{5}$};

\end{tikzpicture}
\end{center}
\shorthandon{:!}

Avec un critère $J=21$. Faire ceci pour toutes les arêtes d'un graphe revient à le considérer acyclique, et dans ce cas on verra apparaître à la fin de la procédure une \emph{section maximale} représentée en pointilée \textcolor{red}{(pas fait)} qui est la partie limitante du flot.

\chapter{Optimisation}

\section{Méthodes de descente}

\section{Optimisation sous contrainte}

\subsection{Formulation générale}

\subsubsection{Multiplieurs de Lagrange}

\subsubsection{Conditions d'ordre 1}

\subsubsection{Conditions d'ordre 2}

\subsection{Programmation linéaire}

\subsubsection{Problême linéaire}

La méthode de réduction de Gauss-Jordan \textcolor{red}{(pas fait)} permet de résoudre les systèmes linéaires de la forme :

\[
AX=B
\]

Où $A$ est une matrice carré inversible. L'objet de se chapitre est de s'intéresser à une extension de la théorie au cas où $A$ n'est plus carré, et où il y a moins d'équations que d'inconnues.

La solution n'est alors plus unique, et parmi l'ensemble des solutions admissibles on cherchera celle qui optimisera un critère $J$ dépendant linéairement de $X$.

Un problême de programmation linéaire s'écrit sous forme standard :

\[\boxed{
\left\{\begin{matrix}
\max \; C_i X^i \\
AX=B \\
X\geq 0
\end{matrix}\right.
}\]

En posant $J=C_i X^i$. La notation $X\geq 0$ est standard en programmation linéaire et désigne l'inégalité terme à terme $\forall i, \; X^i>0$.

On peut se demander dans quel cas pratique est-ce qu'on peut se retrouver dans une situation où $A$ n'est pas carrée. En fait, les variables excédentaires traduisent des inéquations, au sens où pour une matrice $A$ carré :

\[
A_{ij}X^j\geq B_i \Rightarrow \exists A'_i, \; X'_i :\; A'_{ij}X'^j = B_i
\]

\textcolor{red}{(notations ?)}

La matrice $A'$ se construit en ajoutant à $X$ des variables $\varepsilon_i>0$ choisies de sorte qu'on ai toujours : 
\[
A'_{ij}X'^j = A_{ij}X^j + \varepsilon_i = B_i
\]

Le nombre de contraintes (les lignes) étant inchangé, les colonnes supplémentaires rendent la nouvelle matrice $A'$ rectangulaire.

Formellement, on cherchera à ramener un problême linéaire sous forme \emph{canonique} :

\[
\left\{\begin{matrix}
\max \; C_i X^i \\
AX\leq B \\
X\geq 0
\end{matrix}\right.
\]

A sa forme standard. Prenons un exemple \textcolor{red}{(trouver contexte, si possible à base de Smart Grid)} :

\[
\left\{\begin{matrix}
\max x + 0.45y \\
\left(\begin{matrix}
\frac{1}{2} & 1 \\
1 & \frac{1}{2}
\end{matrix}\right)
\left(\begin{matrix}
x \\
y
\end{matrix}\right)
\leq
\left(\begin{matrix}
1 \\
1
\end{matrix}\right) \\
x,y\geq 0
\end{matrix}\right.
\]

Ce problême trouve une interprétation graphique dans le plan :

\textcolor{red}{(figure)}

L'ensemble des solutions admissibles (compatibles avec les contraintes) est un polyèdre de $\mathbb{R}^2$. On doit trouver dans cet ensemble la valeur de $X$ maximisant $J$. On commencera par se convaincre que ce ne peut être le cas que sur le bord du polyèdre (ou alors c'est que $J$ est constant auquel cas le problême est sans grand intéret). 

Pour ce faire, il faut avoir à l'esprit que la linéarité de $J$ implique sa monotonie selon une direction quelconque. Si on prend un $X$ à l'intérieur du domaine, on peut ainsi trouver une direction qui permette d'augmenter $J$ en suivant celle-ci.

\textcolor{red}{(figure)}

Une fois arrivé au contact d'une arête, on peut s'intéresser à l'évolution de $J$ suivant ladite arête (de façon à rester dans le domaine admissible). Si $J$ n'est pas constante sur cette direction, on peut parcourir l'arête dans l'un ou l'autre sens pour l'augmenter (toujours par monotonie de $J$) jusqu'à rejoindre un sommet.

\textcolor{red}{(figure)}

A moins que la situation ne soit particulière, l'optimum $X^*$ se trouvera donc en un sommet, celui où les deux degrés de liberté que sont $x$ et $y$ sont contraints suivant le sens qui devrait permettre d'augmenter $J$.

Il suffit donc de tester la valeur de $J$ sur tous les sommets et de choisir celui pour lequel cette valeur est la plus grande. Cette méthode s'implémente très facilement pour notre exemple quadrilatéral, mais la programmation linéaire peut faire intervenir un grand nombre d'égalités et d'inégalités dans des espaces de grande dimension $\# X$, où l'exploration complète des sommets peut-être inefficace.

Cependant, en plus de la monotonie de la variation du critère, le problême linéaire apporte une autre simplification par rapport à un problême d'optimisation moins spécifique : la linéarité des contraintes implique la convexité de l'ensemble des solutions admissibles \textcolor{red}{(arguments de démonstration ?)}.

C'est-à-dire que pour toutes solutions admissibles $X_1$ et $X_2$, le segment reliant ces deux solutions est constitué de solutions admissibles.

\textcolor{red}{(figure)}

Cette propriété est un grand atout car elle permet de conclure que la solution optimale $X^*$ est atteignable quelque soit la solution $X_0$ dont on part. C'est-à-dire qu'une fois n'est pas coutûme, la recherche gloutonne atteindra nécessairement l'optimum global puisqu'il est le seul optimum du problême.

Encore une autre façon de le dire : il n'est jamais nécessaire de diminuer $J$ pour se rendre dans la partie du graphe voisine de la solution optimale globale, puisque tout le graphe y mène.

Cette propriété a permis le développement d'une méthode de résolution adaptée aux problêmes linéaires, particulièrement rapide et menant toujours à l'optimum global ; qui n'a pas d'équivalent en optimisation non-linéaire.

\subsubsection{Simplexe}

L'\emph{algorithme du simplexe} est la méthode préconisée pour la résolution des problêmes linéaires classiques. Nous allons l'appliquer à notre exemple :

\[
\left\{\begin{matrix}
\max x + 0.45y \\
\left(\begin{matrix}
\frac{1}{2} & 1 \\
1 & \frac{1}{2}
\end{matrix}\right)
\left(\begin{matrix}
x \\
y
\end{matrix}\right)
\leq
\left(\begin{matrix}
1 \\
1
\end{matrix}\right) \\
x,y\geq 0
\end{matrix}\right.
\]


Qu'on réecrit sous forme standard :

\[
\left(\begin{matrix}
\frac{1}{2} & 1 & 1 & 0 \\
1 & \frac{1}{2} & 0 & 1 \\
1 & 0.45 & 0 & 0
\end{matrix}\right)
\left(\begin{matrix}
x \\
y \\
\varepsilon_1 \\
\varepsilon_2
\end{matrix}\right)
=
\left(\begin{matrix}
1 \\
1 \\
J
\end{matrix}\right)
\]

Avec toutes les variables positives. En prévision des manipulations à venir, on a intégré au système des contraintes une pseudo-équation (ce n'en est pas vraiment une puisque $J$ est variable) contenant l'information sur le critère.

Initialisons l'état en $X = \left(\begin{matrix}0 \\ 0 \end{matrix}\right)$. Les variables de base sont choisies de façon à compenser le défaut pour que la solution soit admissible :

\[
\left(\begin{matrix}
\frac{1}{2} & 1 & 1 & 0 \\
1 & \frac{1}{2} & 0 & 1 \\
1 & 0.45 & 0 & 0
\end{matrix}\right)
\left(\begin{matrix}
0 \\
0 \\
1 \\
1
\end{matrix}\right)
=
\left(\begin{matrix}
1 \\
1 \\
0
\end{matrix}\right)
\]

Le critère est donc initialement pris nul, et nous allons effectuer une recherche gloutonne pour l'augmenter succesivement jusqu'à temps que ce ne soit pas possible.

Pour ce faire, on commence par augmenter $x$ qui est la variable de plus haute \emph{sensibilité}, c'est-à-dire pour laquelle le coefficient dans le critère est le plus élevé ($1$ contre $0.45$). On choisira $x=1$ de façon à satisfaire la seconde contrainte (la première également, mais c'est la seconde qui est \emph{limitante}), et on recalculera les autres variables  :

\[
\left(\begin{matrix}
\frac{1}{2} & 1 & 1 & 0 \\
1 & \frac{1}{2} & 0 & 1 \\
1 & 0.45 & 0 & 0
\end{matrix}\right)
\left(\begin{matrix}
1 \\
0 \\
\frac{1}{2} \\
0
\end{matrix}\right)
=
\left(\begin{matrix}
1 \\
1 \\
1
\end{matrix}\right)
\]

Le critère a augmenté d'une unité.

On effectue maintenant un changement de base, c'est-à-dire qu'on redéfini les vecteurs de base de façon à rendre explicite la suppression d'un degré de liberté ($x$ n'est plus une inconnue intéressante du problême car associée à une unique équation ; la seconde ligne) :

\[
\left(\begin{matrix}
0 & \frac{3}{4} & 1 & -\frac{1}{2} \\
1 & \frac{1}{2} & 0 & 1 \\
0 & -0.05 & 0 & -1
\end{matrix}\right)
\left(\begin{matrix}
1 \\
0 \\
\frac{1}{2} \\
0
\end{matrix}\right)
=
\left(\begin{matrix}
\frac{1}{2} \\
1 \\
0
\end{matrix}\right)
\]

La première itération de l'algorithme est effectuée. La dernière ligne ne correspond plus au critère proprement dit, mais contient l'information sur l'évolution de celui-ci dans la nouvelle base.

L'idée est maintenant de recommencer en augmentant $y$. Néanmoins, le coefficient sur $y$ dans la dernière ligne est devenu négatif, ce qui implique qu'augmenter $x_2$ induirait une diminution du critère. Le critère ne peut donc plus être augmenté, et la solution $X = \left(\begin{matrix}1 \\ 0\end{matrix}\right)$ est optimale pour un critère $J=1$ ; nous retrouvons bien la solution graphique \textcolor{red}{(pas fait)}.

Néanmoins, on peut proposer une variante de notre problême où une deuxième itération de l'algorithme sera nécessaire :

\[
\left\{\begin{matrix}
\max x + 0.55y \\
\left(\begin{matrix}
\frac{1}{2} & 1 \\
1 & \frac{1}{2}
\end{matrix}\right)
\left(\begin{matrix}
x \\
y
\end{matrix}\right)
\leq
\left(\begin{matrix}
1 \\
1
\end{matrix}\right) \\
x,y\geq 0
\end{matrix}\right.
\]

En reprenant exactement ce qui vient d'être fait, nous arrivons à :

\[
\left(\begin{matrix}
0 & \frac{3}{4} & 1 & -\frac{1}{2} \\
1 & \frac{1}{2} & 0 & 1 \\
0 & 0.05 & 0 & -1
\end{matrix}\right)
\left(\begin{matrix}
1 \\
0 \\
\frac{1}{2} \\
0
\end{matrix}\right)
=
\left(\begin{matrix}
\frac{1}{2} \\
1 \\
0
\end{matrix}\right)
\]

le critère varie maintenant dans le même sens que $y$. On peut donc effectuer une seconde itération du simplexe en augmentant $y$, limitée par la première contrainte :

\[
\left(\begin{matrix}
0 & \frac{3}{4} & 1 & -\frac{1}{2} \\
1 & \frac{1}{2} & 0 & 1 \\
0 & 0.05 & 0 & -1
\end{matrix}\right)
\left(\begin{matrix}
\frac{2}{3} \\
\frac{2}{3} \\
0 \\
0
\end{matrix}\right)
=
\left(\begin{matrix}
\frac{1}{2} \\
1 \\
\frac{0.1}{3}
\end{matrix}\right)
\]

Et par un changement de base :

\[
\left(\begin{matrix}
0 & 1 & \frac{4}{3} & -\frac{2}{3} \\
1 & 0 & -\frac{2}{3} & \frac{4}{3} \\
0 & 0 & -\frac{0.2}{3} & -\frac{0.9}{3}
\end{matrix}\right)
\left(\begin{matrix}
\frac{2}{3} \\
\frac{2}{3} \\
0 \\
0
\end{matrix}\right)
=
\left(\begin{matrix}
\frac{2}{3} \\
\frac{2}{3}\\
0
\end{matrix}\right)
\]

Les poids de $J$ sur $x$ et $y$ sont nuls dans cette nouvelle base, et le critère ne peut donc plus être augmenté. On a atteint l'optimum en $X = \left(\begin{matrix}\frac{2}{3} \\ \frac{2}{3}\end{matrix}\right)$ pour un critère $J=1+\frac{0.1}{3}=\frac{3.1}{3}$.

\textcolor{red}{(figure)}

Enfin, par symétrie du problême, si le critère s'écrivait $J = 0.45x+y$, alors la solution serait naturellement en $X = \left(\begin{matrix}0 \\ 1\end{matrix}\right)$. 

\subsection{Optimisation statique}

Supposons qu'on veuille chercher le minimum de la fonction suivante :

\[
\mathcal{L}: (x,y) \rightarrow x^2+y^2
\]

Ce minimum se trouve bien évidemment en zéro, mais nous allons voir comment développer une théorie de l'optimisation à partir d'objets élémentaires.

Le gradient de cette fonction s'évalue comme :

\[
\nabla \mathcal{L}(x,y) = 2\left(\begin{matrix}
x \\ y
\end{matrix}\right)
\]

Et la condition d'annulation du gradient à l'optimum :

\[\boxed{
\nabla \mathcal{L}(x^*,y^*) = 0
}\]

\[
\left(\begin{matrix}
x^* \\ y^*
\end{matrix}\right)=0
\]

Trivial. Ajoutons à ce problême une contrainte, la solution est priée d'appartenir à la droite :

\[
\mathcal{C} = \{ x+y-1=0 \}
\]

Nous allons maintenant utiliser une ruse. Cette contrainte étant linéaire nous allons définir comme :

\[
c: (x,y) \rightarrow x+y-1
\]

La fonction à évaluer pour aboutir à un gradient, définie de façon à ce que $\mathcal{C}=c^{-1}(0)$. On a alors :

\[
\nabla c(x,y) = \left(\begin{matrix}
1 \\ 1
\end{matrix}\right)
\]

On observe maintenant qu'au minimum, les deux gradients ne peuvent qu'êtres colinéaires. On a alors \textcolor{red}{(passer par le théorème du multiplicateur de Lagrange)} :

\[\boxed{
\exists \lambda : \nabla \mathcal{L}(x^*,y^*) = \lambda \nabla c(x^*,y^*)
}\]

Ici :

\[
\exists \lambda : \left(\begin{matrix}
x^* \\ y^*
\end{matrix}\right) = \frac{\lambda}{2} \left(\begin{matrix}
1 \\ 1
\end{matrix}\right)
\]

Le minimum se trouverait donc sur l'espace engendré par $\left(\begin{matrix}
1 \\ 1
\end{matrix}\right)$, et c'est effectivement le cas ! Pour trouver $\lambda$, il suffit d'écrire la contrainte :

\[
\frac{\lambda}{2}+\frac{\lambda}{2}-1=0
\]

\[
\lambda=1
\]

Le minimum se situe donc en :

\[
\left(\begin{matrix}
x^* \\ y^*
\end{matrix}\right) = \left(\begin{matrix}
\frac{1}{2} \\ \frac{1}{2}
\end{matrix}\right)
\]

Excellent. On peut finalement définir un nouvel outil propice à l'optimisation sous contrainte :

\[\boxed{
\mathcal{H} : (x,y,\lambda) = \lambda c(x,y) - \mathcal{L}(x,y,\lambda)
}\]

Pour lequel le \emph{principe du maximum} s'écrira :

\[\boxed{
\nabla \mathcal{H}(x^*,y^*) = 0
}\]

\subsection{Calcul des variations}

On s'intéresse au \emph{problême du Brachistochrone} :

\textcolor{red}{(figure)}

Celui-ci va nous permettre de développer une théorie de l'optimisation des fonctionnelles, c'est-à-dire répondre à des problêmes de la forme :

\[\boxed{
\min \int \mathcal{L}(h(x),\dot{h}(x)) \; dx
}\]

Où ce qui doit-être déterminé n'est pas la variable $x$, mais bien (à cause de l'intégrale) la fonction $h$. Le calcul des variation permet ainsi de trouver des fonctions optimisantes, et est un outil absolument redoutable pour le physicien d'un niveau avancé.

Nous nous limitons à un \emph{Lagrangien} $\mathcal{L}$ qui ne dépend pas explicitement du temps.

Vous allez pouvoir aperçevoir la connexion très profonde qu'il y a entre l'optimisation et la mécanique. Un théorème important est l'\emph{équation d'Euler-Lagrange} \textcolor{red}{(à démontrer)} qui s'écrit :

\[\boxed{
\frac{\partial \mathcal{L}}{\partial h}(h^*,\dot{h}) - \frac{d}{dx}\frac{\partial \mathcal{L}}{\partial \dot{h}}(h^*,\dot{h})=0
}\]

En toute rigueur, $\mathcal{L}$ et $\dot{h}$ devraient également s'écrire étoilées, mais je souligne ici que c'est bien $h^*$ qu'on recherche dans ce problême et qui impose le reste.

\textcolor{red}{(brachistochrone)}

Changeons de problêmes. On ajoute maintenant une contrainte sur la longueur de route.

\textcolor{red}{(compléter)}

\[
l(x_0)=l_0
\]

Avec :

\[
l : x \rightarrow \int \sqrt{1+\dot{h}^2(x')} \; dx'
\]

C'est donc tout naturellement qu'on définira, par analogie avec la section précédente :

\[
c : x \rightarrow \sqrt{1+\dot{h}^2(x)}
\]

Et pour quoi faire ? Pour s'empresser d'en calculer le gradient bien évidemment :

\textcolor{red}{(compléter)}

De sorte qu'on puisse définir un \emph{Hamiltonien} :

\[\boxed{
\mathcal{H} : (x,h,\dot{h}) = \lambda(x) c(x,h,\dot{h}) - \mathcal{L}(x,h,\dot{h})
}\]

Qui vérifie le principe du maximum :

\[
\frac{\partial \mathcal{H}}{\partial h}(h^*,\dot{h}) - \frac{d}{dx}\frac{\partial \mathcal{H}}{\partial \dot{h}}(h^*,\dot{h})=0
\]

\subsection{Filtrage optimal}

\section{Programmation dynamique}

\section{Commande optimale}

\part{Outillage pour l'ingénierie}
\setcounter{chapter}{0}

\bibliographystyle{biblio}
\bibliography{biblio}

\end{document}
